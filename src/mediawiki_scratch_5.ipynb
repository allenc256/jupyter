{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import gzip\n",
    "import json\n",
    "import datetime\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf():\n",
    "    global sess\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    # maximum number of symbols in an input sequence\n",
    "    max_sequence_length = 40\n",
    "\n",
    "    # number of symbols in vocabulary\n",
    "    # (symbols are expected to be in range(vocab_size))\n",
    "    vocab_size = 10000\n",
    "\n",
    "    # number of dimensions in input embeddings\n",
    "    embedding_size = 128\n",
    "    \n",
    "    # number of sequences per batch\n",
    "    batch_size = 256\n",
    "    \n",
    "    # number of target classes\n",
    "    num_target_classes = 2\n",
    "    \n",
    "    # number of combined (attention + feed forward) layers\n",
    "    num_layers = 1\n",
    "    \n",
    "    # number of parsing threads in data pipeline\n",
    "    dataset_pipeline_parallel_calls = 4\n",
    "    \n",
    "    # size of prefetch in data pipeline\n",
    "    dataset_pipeline_prefetch = batch_size * 16\n",
    "    \n",
    "    # shuffle buffer size\n",
    "    dataset_pipeline_shuffle_buffer_size = 10000\n",
    "    \n",
    "    # dropout rate\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example_proto, max_sequence_length=hp.max_sequence_length):\n",
    "    features = {\n",
    "        'inputs': tf.VarLenFeature(tf.int64),\n",
    "        'word_endings': tf.VarLenFeature(tf.int64),\n",
    "        'targets': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    \n",
    "    parsed = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    def convert_and_pad(sparse_tensor):\n",
    "        result = tf.sparse_tensor_to_dense(sparse_tensor)\n",
    "        # TODO: properly ignore elements which are too large (right now we just clip)\n",
    "        result = result[:max_sequence_length]\n",
    "        result = tf.pad(result, [[0, max_sequence_length - tf.shape(result)[0]]])\n",
    "        return result\n",
    "    \n",
    "    return (convert_and_pad(parsed['inputs']),\n",
    "            tf.shape(parsed['inputs'])[0],\n",
    "            convert_and_pad(parsed['word_endings']),\n",
    "            convert_and_pad(parsed['targets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "# Data pipeline\n",
    "# -------------\n",
    "\n",
    "dataset_filenames = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(dataset_filenames)\n",
    "dataset = dataset.map(parse_example, \n",
    "                      num_parallel_calls = hp.dataset_pipeline_parallel_calls)\n",
    "dataset = dataset.shuffle(hp.dataset_pipeline_shuffle_buffer_size)\n",
    "dataset = dataset.prefetch(hp.dataset_pipeline_prefetch)\n",
    "dataset = dataset.batch(hp.batch_size)\n",
    "\n",
    "dataset_iterator = dataset.make_initializable_iterator()\n",
    "(input_sequences_it,\n",
    " input_lengths_it,\n",
    " input_word_endings_it,\n",
    " target_sequences_it) = dataset_iterator.get_next()\n",
    "\n",
    "# Placeholders\n",
    "# ------------\n",
    "\n",
    "input_sequences = tf.placeholder_with_default(input_sequences_it,\n",
    "                                              shape = [None, hp.max_sequence_length],\n",
    "                                              name = 'input_sequences')\n",
    "input_lengths = tf.placeholder_with_default(input_lengths_it,\n",
    "                                            shape = [None],\n",
    "                                            name = 'input_lengths')\n",
    "input_word_endings = tf.placeholder_with_default(input_word_endings_it,\n",
    "                                                 shape = [None, hp.max_sequence_length],\n",
    "                                                 name = 'input_word_endings')\n",
    "target_sequences = tf.placeholder_with_default(target_sequences_it,\n",
    "                                               shape = [None, hp.max_sequence_length],\n",
    "                                               name = 'target_sequences')\n",
    "\n",
    "# sequences of input positions (not a placeholder)\n",
    "input_positions = tf.range(hp.max_sequence_length, dtype=tf.int32)\n",
    "input_positions = tf.tile(input_positions, [tf.shape(input_sequences)[0]])\n",
    "input_positions = tf.reshape(input_positions, \n",
    "                             (tf.shape(input_sequences)[0], hp.max_sequence_length), \n",
    "                             name = 'input_positions')\n",
    "\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# Embeddings\n",
    "# ----------\n",
    "\n",
    "# sequences of input embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_sequence_embeddings = tf.get_variable('input_sequence_embeddings', \n",
    "                                            (hp.vocab_size, hp.embedding_size))\n",
    "input_sequences_embedded = tf.nn.embedding_lookup(input_sequence_embeddings, \n",
    "                                                  input_sequences,\n",
    "                                                  name = 'input_sequences_embedded')\n",
    "\n",
    "# sequences of input position embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_position_embeddings = tf.get_variable('input_position_embeddings', \n",
    "                                            (hp.max_sequence_length, hp.embedding_size))\n",
    "input_positions_embedded = tf.nn.embedding_lookup(input_position_embeddings, input_positions)\n",
    "\n",
    "# sequences of word ending embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_word_ending_embeddings = tf.get_variable('input_word_ending_embeddings',\n",
    "                                               (2, hp.embedding_size))\n",
    "input_word_endings_embedded = tf.nn.embedding_lookup(input_word_ending_embeddings, \n",
    "                                                     input_word_endings,\n",
    "                                                     name = 'input_word_endings_embedded')\n",
    "\n",
    "# Sequence mask\n",
    "# -------------\n",
    "\n",
    "sequence_mask = tf.sequence_mask(input_lengths,\n",
    "                                 hp.max_sequence_length,\n",
    "                                 dtype = tf.float32)\n",
    "# expand dimensions to support broadcasting\n",
    "expanded_sequence_mask = tf.expand_dims(sequence_mask, \n",
    "                                        2, \n",
    "                                        name = 'sequence')\n",
    "\n",
    "input_combined_embedded = tf.add_n([input_sequences_embedded, \n",
    "                                    input_positions_embedded, \n",
    "                                    input_word_endings_embedded])\n",
    "# TODO: is this necessary?\n",
    "input_combined_embedded = tf.multiply(input_combined_embedded,\n",
    "                                      expanded_sequence_mask)\n",
    "input_combined_embedded = tf.layers.dropout(input_combined_embedded,\n",
    "                                            rate = hp.dropout_rate,\n",
    "                                            training = is_training)\n",
    "\n",
    "# Layer normalization\n",
    "# -------------------\n",
    "\n",
    "def layer_norm(x, num_units, scope, reuse=None, epsilon=1e-6):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        scale = tf.get_variable(\n",
    "            \"layer_norm_scale\", [num_units], initializer=tf.ones_initializer())\n",
    "        bias = tf.get_variable(\n",
    "            \"layer_norm_bias\", [num_units], initializer=tf.zeros_initializer())\n",
    "        result = layer_norm_compute(x, epsilon, scale, bias)\n",
    "        return result\n",
    "\n",
    "def layer_norm_compute(x, epsilon, scale, bias):\n",
    "    # TODO: incorporate length into layer normalization?\n",
    "    mean = tf.reduce_mean(x, axis=[-1], keep_dims=True)\n",
    "    variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keep_dims=True)\n",
    "    norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n",
    "    return norm_x * scale + bias\n",
    "    \n",
    "# Attention\n",
    "# ---------\n",
    "\n",
    "def attention_layer(A, dropout_rate, is_training):\n",
    "    A_T = tf.transpose(A, perm=[0, 2, 1])\n",
    "    scaled_logits = tf.matmul(A, A_T) / tf.sqrt(tf.cast(tf.shape(A)[-1], tf.float32))\n",
    "    result = tf.matmul(tf.nn.softmax(scaled_logits), A)\n",
    "    result = tf.layers.dropout(result,\n",
    "                               rate = dropout_rate,\n",
    "                               training = is_training)\n",
    "    return result\n",
    "\n",
    "# Feed-forward\n",
    "# ------------\n",
    "\n",
    "def feed_forward_layer(A, num_units, dropout_rate, is_training, scope, reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        A = tf.layers.dense(A, num_units, activation=tf.nn.relu, name='fc1')\n",
    "        A = tf.layers.dense(A, num_units, name='fc2')\n",
    "        A = tf.layers.dropout(A, rate = dropout_rate, training = is_training)\n",
    "        return A\n",
    "\n",
    "# Layers\n",
    "# ------\n",
    "\n",
    "def combined_layer(A, num_units, mask, dropout_rate, is_training, scope, reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        A = layer_norm(A + attention_layer(A, dropout_rate, is_training), num_units, scope='attention_norm')\n",
    "        A = layer_norm(A + feed_forward_layer(A, num_units, dropout_rate, is_training, 'ff'), num_units, scope='ff_norm')\n",
    "        A *= mask\n",
    "        return A\n",
    "    \n",
    "layer = input_combined_embedded\n",
    "for i in range(hp.num_layers):\n",
    "    layer = combined_layer(layer, \n",
    "                           hp.embedding_size, \n",
    "                           expanded_sequence_mask, \n",
    "                           hp.dropout_rate,\n",
    "                           is_training,\n",
    "                           'layer_%d' % i)\n",
    "\n",
    "# Softmax\n",
    "# -------\n",
    "\n",
    "output_logits = tf.layers.dense(layer, hp.num_target_classes, name='softmax')\n",
    "\n",
    "# TODO: is the softmax here really necessary?\n",
    "output_sequences = tf.nn.softmax(output_logits)\n",
    "output_sequences = tf.argmax(output_sequences, axis=-1)\n",
    "output_sequences *= tf.cast(sequence_mask, tf.int64)\n",
    "\n",
    "# Loss\n",
    "# ----\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_sequences,\n",
    "                                                        logits=output_logits)\n",
    "losses *= sequence_mask\n",
    "\n",
    "total_loss = tf.reduce_sum(losses)\n",
    "total_input_length = tf.reduce_sum(input_lengths)\n",
    "mean_loss  = total_loss / tf.cast(total_input_length, tf.float32)\n",
    "\n",
    "true_positives = tf.reduce_sum(output_sequences * target_sequences)\n",
    "false_positives = tf.reduce_sum(tf.maximum(output_sequences - target_sequences, 0))\n",
    "false_negatives = tf.reduce_sum(tf.maximum(target_sequences - output_sequences, 0))\n",
    "\n",
    "# Training\n",
    "# --------\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "train_op = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "\n",
    "# Summaries\n",
    "# ---------\n",
    "\n",
    "tf.summary.scalar('mean_loss', mean_loss)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"input_sequence_embeddings:0\": 1280000\n",
      "parameters for \"input_position_embeddings:0\": 5120\n",
      "parameters for \"input_word_ending_embeddings:0\": 256\n",
      "parameters for \"layer_0/attention_norm/layer_norm_scale:0\": 128\n",
      "parameters for \"layer_0/attention_norm/layer_norm_bias:0\": 128\n",
      "parameters for \"layer_0/ff/fc1/kernel:0\": 16384\n",
      "parameters for \"layer_0/ff/fc1/bias:0\": 128\n",
      "parameters for \"layer_0/ff/fc2/kernel:0\": 16384\n",
      "parameters for \"layer_0/ff/fc2/bias:0\": 128\n",
      "parameters for \"layer_0/ff_norm/layer_norm_scale:0\": 128\n",
      "parameters for \"layer_0/ff_norm/layer_norm_bias:0\": 128\n",
      "parameters for \"softmax/kernel:0\": 256\n",
      "parameters for \"softmax/bias:0\": 2\n",
      "total parameters: 1319170\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print('parameters for \"%s\": %d' % (variable.name, variable_parameters))\n",
    "    total_parameters += variable_parameters\n",
    "print('total parameters: %d' % total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(filename, header='results', train=False, show_progress=True):\n",
    "    cum_loss = 0\n",
    "    cum_input_length = 0\n",
    "    \n",
    "    cum_true_positives = 0\n",
    "    cum_false_positives = 0\n",
    "    cum_false_negatives = 0\n",
    "    \n",
    "    sess.run(dataset_iterator.initializer, feed_dict={\n",
    "        dataset_filenames: [filename]\n",
    "    })\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    if show_progress:\n",
    "        progress = tqdm()\n",
    "        \n",
    "    while True:\n",
    "        try:\n",
    "            (_,\n",
    "             curr_loss, \n",
    "             curr_input_length, \n",
    "             curr_true_positives,\n",
    "             curr_false_positives,\n",
    "             curr_false_negatives) = sess.run((train_op if train else [],\n",
    "                                               total_loss,\n",
    "                                               total_input_length,\n",
    "                                               true_positives,\n",
    "                                               false_positives,\n",
    "                                               false_negatives),\n",
    "                                              feed_dict = { is_training: train })\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "        if show_progress:\n",
    "            progress.update(curr_input_length)\n",
    "\n",
    "        cum_loss += curr_loss\n",
    "        cum_input_length += curr_input_length\n",
    "        cum_true_positives += curr_true_positives\n",
    "        cum_false_positives += curr_false_positives\n",
    "        cum_false_negatives += curr_false_negatives\n",
    "    \n",
    "    if show_progress:\n",
    "        progress.close()\n",
    "\n",
    "    finish = datetime.datetime.now()\n",
    "    elapsed = (finish - start).total_seconds()\n",
    "    \n",
    "    precision = cum_true_positives / (cum_true_positives + cum_false_positives)\n",
    "    recall = cum_true_positives / (cum_true_positives + cum_false_negatives)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print('%s: loss=%g, elapsed=%gs, precision=%g, recall=%g, F1=%g' % (header,\n",
    "                                                                        cum_loss/cum_input_length, \n",
    "                                                                        elapsed, \n",
    "                                                                        precision, \n",
    "                                                                        recall, \n",
    "                                                                        F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1856792it [00:05, 395129.18it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-640fb123e91e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                               \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                               show_progress=True)\n\u001b[0m\u001b[1;32m     11\u001b[0m     result = evaluate_dataset('../data/simplewiki/simplewiki-20171103.entity_recognition.dev.tfrecords',\n\u001b[1;32m     12\u001b[0m                               \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dev %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0cc443b9ac7b>\u001b[0m in \u001b[0;36mevaluate_dataset\u001b[0;34m(filename, header, train, show_progress)\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                \u001b[0mfalse_positives\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                false_negatives),\n\u001b[0;32m---> 31\u001b[0;31m                                               feed_dict = { is_training: train })\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# builder = tf.saved_model.builder.SavedModelBuilder('../models/simplewiki/attention_1_layer')\n",
    "# builder.add_meta_graph_and_variables(sess, ['training'])\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    result = evaluate_dataset('../data/simplewiki/simplewiki-20171103.entity_recognition.train.tfrecords',\n",
    "                              header='train %d' % epoch,\n",
    "                              train=True,\n",
    "                              show_progress=True)\n",
    "    result = evaluate_dataset('../data/simplewiki/simplewiki-20171103.entity_recognition.dev.tfrecords',\n",
    "                              header='dev %d' % epoch,\n",
    "                              train=False,\n",
    "                              show_progress=False)\n",
    "    #builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
