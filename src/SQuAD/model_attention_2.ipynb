{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achang/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_tf(sess = None, log_device_placement = False):\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    return tf.InteractiveSession(config = tf.ConfigProto(log_device_placement = log_device_placement))\n",
    "\n",
    "def dump_statistics():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        print('parameters for \"%s\": %d' % (variable.name, variable_parameters))\n",
    "        total_parameters += variable_parameters\n",
    "    print('total parameters: %d' % total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    dropout_rate = 0.0\n",
    "    \n",
    "    context_size = 850\n",
    "    question_size = 60\n",
    "    answers_size = 6\n",
    "    \n",
    "    d_hidden = 200\n",
    "    \n",
    "    num_attn_layers_contexts = 3\n",
    "    num_attn_layers_questions = 3\n",
    "    num_attn_layers_joint = 3\n",
    "\n",
    "    dataset_batch_size = 48\n",
    "    dataset_num_parallel_calls = 4\n",
    "    dataset_prefetch_size = 128\n",
    "    dataset_shuffle_size = 1000\n",
    "    \n",
    "    max_distance_bias = 15\n",
    "    \n",
    "    gradient_clip_norm = 5\n",
    "    \n",
    "    loss_pos_weight = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel:\n",
    "    def __init__(self, session, word_embeddings, hparams):\n",
    "        self._session = session\n",
    "        self._word_embeddings = word_embeddings\n",
    "        self._hparams = hparams\n",
    "        \n",
    "    def _parse_example(self, example_proto):\n",
    "        # parse proto\n",
    "        parsed = tf.parse_single_example(example_proto, features = {\n",
    "            'context': tf.VarLenFeature(tf.int64),\n",
    "            'question': tf.VarLenFeature(tf.int64),\n",
    "            'answer_starts': tf.VarLenFeature(tf.int64),\n",
    "            'answer_ends': tf.VarLenFeature(tf.int64), })\n",
    "        \n",
    "        # convert to dense tensors\n",
    "        context = tf.sparse_tensor_to_dense(parsed['context'])\n",
    "        question = tf.sparse_tensor_to_dense(parsed['question'])\n",
    "        answer_starts = tf.sparse_tensor_to_dense(parsed['answer_starts'])\n",
    "        answer_ends = tf.sparse_tensor_to_dense(parsed['answer_ends'])\n",
    "        \n",
    "        # pad tensors\n",
    "        context_len = tf.shape(context)[0]\n",
    "        question_len = tf.shape(question)[0]\n",
    "        answers_len = tf.shape(answer_starts)[0]\n",
    "        zero_vector = self._word_embeddings.shape[0] - 1\n",
    "        context = tf.pad(\n",
    "            context,\n",
    "            [[0, self._hparams.context_size - context_len]],\n",
    "            constant_values = 0)\n",
    "        question = tf.pad(\n",
    "            question,\n",
    "            [[0, self._hparams.question_size - question_len]],\n",
    "            constant_values = 0)\n",
    "        answer_starts = tf.pad(\n",
    "            answer_starts,\n",
    "            [[0, self._hparams.answers_size - answers_len]],\n",
    "            constant_values = -1)\n",
    "        answer_ends = tf.pad(\n",
    "            answer_ends,\n",
    "            [[0, self._hparams.answers_size - answers_len]],\n",
    "            constant_values = -1)\n",
    "        \n",
    "        return (context, question, answer_starts, answer_ends)\n",
    "    \n",
    "    def _build_dataset_pipeline(self):\n",
    "        with tf.variable_scope('dataset'):\n",
    "            # placeholders\n",
    "            self._dataset_filenames = tf.placeholder(\n",
    "                tf.string,\n",
    "                shape = [None],\n",
    "                name = 'dataset_filenames')\n",
    "            self._dataset_limit = tf.placeholder_with_default(\n",
    "                tf.constant(-1, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_limit')\n",
    "            self._dataset_shuffle_size = tf.placeholder_with_default(\n",
    "                tf.constant(self._hparams.dataset_batch_size, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_shuffle_size')\n",
    "            self._dataset_batch_size = tf.placeholder_with_default(\n",
    "                tf.constant(self._hparams.dataset_batch_size, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_batch_size')\n",
    "            self._dataset_prefetch_size = tf.placeholder_with_default(\n",
    "                tf.constant(self._hparams.dataset_prefetch_size, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_prefetch_size')\n",
    "\n",
    "            # build dataset\n",
    "            dataset = tf.data.TFRecordDataset(\n",
    "                tf.random_shuffle(self._dataset_filenames),\n",
    "                compression_type='GZIP')\n",
    "            dataset = dataset.take(self._dataset_limit)\n",
    "            dataset = dataset.map(\n",
    "                self._parse_example,\n",
    "                num_parallel_calls = self._hparams.dataset_num_parallel_calls)\n",
    "            dataset = dataset.shuffle(self._dataset_shuffle_size)\n",
    "            dataset = dataset.prefetch(self._dataset_prefetch_size)\n",
    "            dataset = dataset.batch(self._dataset_batch_size)\n",
    "\n",
    "            # build iterator\n",
    "            self._dataset_iterator = dataset.make_initializable_iterator()\n",
    "            (contexts, questions, answer_starts, answer_ends) = self._dataset_iterator.get_next()\n",
    "            \n",
    "            # give key tensors names\n",
    "            self._contexts = tf.identity(contexts, 'contexts')\n",
    "            self._questions = tf.identity(questions, 'questions')\n",
    "            self._answer_starts = tf.identity(answer_starts, 'answer_starts')\n",
    "            self._answer_ends = tf.identity(answer_ends, 'answer_ends')\n",
    "\n",
    "            # hint static shapes\n",
    "            self._contexts.set_shape([None, self._hparams.context_size])\n",
    "            self._questions.set_shape([None, self._hparams.question_size])\n",
    "            self._answer_starts.set_shape([None, self._hparams.answers_size])\n",
    "            self._answer_ends.set_shape([None, self._hparams.answers_size])\n",
    "\n",
    "            # minibatch size\n",
    "            self._minibatch_size = tf.shape(self._contexts)[0]\n",
    "            self._minibatch_size = tf.identity(self._minibatch_size, 'minibatch_size')\n",
    "            \n",
    "            # context positions\n",
    "            p = tf.range(self._hparams.context_size, dtype = tf.int64)\n",
    "            p = tf.tile(p, [self._minibatch_size])\n",
    "            p = tf.reshape(\n",
    "                p,\n",
    "                [self._minibatch_size, self._hparams.context_size],\n",
    "                name = 'context_positions')\n",
    "            self._context_positions = p\n",
    "\n",
    "            # question positions\n",
    "            p = tf.range(self._hparams.question_size, dtype = tf.int64)\n",
    "            p = tf.tile(p, [self._minibatch_size])\n",
    "            p = tf.reshape(\n",
    "                p,\n",
    "                [self._minibatch_size, self._hparams.question_size],\n",
    "                name = 'question_positions')\n",
    "            self._question_positions = p\n",
    "            \n",
    "    def _attention_layer(self,\n",
    "                         keys,\n",
    "                         queries,\n",
    "                         values,\n",
    "                         size = None,\n",
    "                         distance_bias = False,\n",
    "                         mask_type = None):\n",
    "        with tf.variable_scope('attention'):\n",
    "            # default size\n",
    "            if size is None:\n",
    "                size = keys.shape[-1].value\n",
    "            \n",
    "            # variables\n",
    "            key_projection = tf.get_variable(\n",
    "                'key_projection',\n",
    "                [keys.shape[-1].value, size])\n",
    "            query_projection = tf.get_variable(\n",
    "                'query_projection',\n",
    "                [queries.shape[-1].value, size])\n",
    "            \n",
    "            # extract # queries/keys (must be statically known)\n",
    "            num_queries = queries.shape[-2].value\n",
    "            num_keys = keys.shape[-2].value\n",
    "            \n",
    "            # compute weights\n",
    "            q = tf.tensordot(queries, query_projection, axes = 1) # [batch_size, num_queries, size]\n",
    "            q.set_shape([None, queries.shape[-2].value, size])\n",
    "            k = tf.tensordot(keys, key_projection, axes = 1)      # [batch_size, num_keys, size]\n",
    "            k.set_shape([None, keys.shape[-2].value, size])\n",
    "            k = tf.transpose(k, perm = [0, 2, 1])                 # [batch_size, size, num_keys]\n",
    "            w = tf.matmul(q, k)                                   # [batch_size, num_queries, num_keys]\n",
    "            w /= np.sqrt(size)\n",
    "            \n",
    "            # apply distance bias\n",
    "            if distance_bias:\n",
    "                bias = tf.constant(\n",
    "                    [[-max(float(np.abs(i - j)), self._hparams.max_distance_bias)\n",
    "                        for j in range(num_keys)]\n",
    "                        for i in range(num_queries)])\n",
    "                bias = tf.expand_dims(bias, axis = 0)             # [1, num_queries, num_keys]\n",
    "                bias *= self._distance_scaling_factor\n",
    "                w += bias\n",
    "            \n",
    "            # apply mask\n",
    "            if mask_type is not None:\n",
    "                infinity= 1e25\n",
    "                if mask_type == 'f':\n",
    "                    mask = [[-infinity if i <= j else infinity\n",
    "                        for j in range(num_keys)]\n",
    "                        for i in range(num_queries)]\n",
    "                    mask[0][0] = infinity\n",
    "                    mask = tf.constant(mask)\n",
    "                elif mask_type == 'b':\n",
    "                    mask = [[-infinity if i >= j else infinity\n",
    "                        for j in range(num_keys)]\n",
    "                        for i in range(num_queries)]\n",
    "                    mask[-1][-1] = infinity\n",
    "                    mask = tf.constant(mask)\n",
    "                elif mask_type == 's':\n",
    "                    mask = [[-infinity if i == j else infinity\n",
    "                        for j in range(num_keys)]\n",
    "                        for i in range(num_queries)]\n",
    "                    mask = tf.constant(mask)\n",
    "                mask = tf.expand_dims(mask, axis = 0)             # [1, num_queries, num_keys]\n",
    "                w = tf.minimum(w, mask)\n",
    "\n",
    "            # softmax\n",
    "            w = tf.nn.softmax(w, name = 'weights')\n",
    "            \n",
    "            # apply weights\n",
    "            return tf.matmul(w, values)\n",
    "        \n",
    "#     def _attention_layer_self(self, layer):\n",
    "#         # grab layer size\n",
    "#         size = layer.shape[-1].value\n",
    "#        \n",
    "#         # self-attention\n",
    "#         attn = self._attention_layer(\n",
    "#             layer,\n",
    "#             layer,\n",
    "#             layer,\n",
    "#             distance_bias = True,\n",
    "#             mask_type = 's')\n",
    "#        \n",
    "#         return self._fusion_layer([layer, attn], size)\n",
    "#    \n",
    "#     def _fusion_layer(self, layers, size):\n",
    "#         with tf.variable_scope('fusion'):\n",
    "#             # fuse self-attention layer\n",
    "#             layer = tf.concat(layers, axis = -1)\n",
    "#\n",
    "#             # feed-forward hidden layer\n",
    "#             layer = tf.layers.dense(\n",
    "#                 layer,\n",
    "#                 size * 2,\n",
    "#                 activation = tf.nn.relu,\n",
    "#                 name = 'ff_hidden')\n",
    "#\n",
    "#             # feed-forward output layer\n",
    "#             layer = tf.layers.dense(\n",
    "#                 layer,\n",
    "#                 size,\n",
    "#                 name = 'ff_output')\n",
    "#\n",
    "#             # batch norm\n",
    "#             layer = tf.layers.batch_normalization(\n",
    "#                 layer,\n",
    "#                 training = self._training)\n",
    "#\n",
    "#             # dropout\n",
    "#             layer = tf.layers.dropout(\n",
    "#                 layer,\n",
    "#                 rate = self._hparams.dropout_rate,\n",
    "#                 training = self._training)\n",
    "#\n",
    "#             return layer\n",
    "#\n",
    "#     def _layer_norm(self, layer, epsilon = 1e-6):\n",
    "#         with tf.variable_scope('layer_norm'):\n",
    "#             size = layer.shape[-1].value\n",
    "#             scale = tf.get_variable(\n",
    "#                 'layer_norm_scale',\n",
    "#                 [size],\n",
    "#                 initializer = tf.ones_initializer())\n",
    "#             bias = tf.get_variable(\n",
    "#                 'layer_norm_bias',\n",
    "#                 [size],\n",
    "#                 initializer = tf.zeros_initializer())\n",
    "#             mean = tf.reduce_mean(\n",
    "#                 layer,\n",
    "#                 axis = -1,\n",
    "#                 keep_dims = True)\n",
    "#             variance = tf.reduce_mean(\n",
    "#                 tf.square(layer - mean),\n",
    "#                 axis = -1,\n",
    "#                 keep_dims = True)\n",
    "#             norm_layer = (layer - mean) * tf.rsqrt(variance + epsilon)\n",
    "#             return norm_layer * scale + bias\n",
    "\n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('model'):\n",
    "            # placeholders\n",
    "            self._training = tf.placeholder(tf.bool, name = 'training')\n",
    "\n",
    "            # attention distance scale\n",
    "            self._distance_scaling_factor = tf.get_variable(\n",
    "                'distance_scaling_factor',\n",
    "                shape = [],\n",
    "                initializer = tf.constant_initializer([0.3]))\n",
    "            \n",
    "            # word embeddings\n",
    "            word_embeddings = tf.get_variable(\n",
    "                name = \"word_embeddings\",\n",
    "                shape = self._word_embeddings.shape,\n",
    "                initializer = tf.constant_initializer(self._word_embeddings),\n",
    "                trainable = False)\n",
    "            contexts_embedded = tf.nn.embedding_lookup(\n",
    "                word_embeddings,\n",
    "                self._contexts)\n",
    "            questions_embedded = tf.nn.embedding_lookup(\n",
    "                word_embeddings,\n",
    "                self._questions)\n",
    "            \n",
    "            # position embeddings\n",
    "            position_embeddings = tf.get_variable(\n",
    "                'position_embeddings',\n",
    "                [self._hparams.context_size, self._hparams.d_hidden],\n",
    "                dtype = tf.float32)\n",
    "            context_positions_embedded = tf.nn.embedding_lookup(\n",
    "                position_embeddings,\n",
    "                self._context_positions)\n",
    "            question_positions_embedded = tf.nn.embedding_lookup(\n",
    "                position_embeddings,\n",
    "                self._question_positions)\n",
    "            \n",
    "            # transform contexts\n",
    "            with tf.variable_scope('contexts'):\n",
    "                contexts_layer = tf.layers.dense(\n",
    "                    contexts_embedded,\n",
    "                    self._hparams.d_hidden,\n",
    "                    activation = tf.nn.relu)\n",
    "                contexts_layer += context_positions_embedded\n",
    "                contexts_layer = tf.layers.batch_normalization(\n",
    "                    contexts_layer,\n",
    "                    training = self._training)\n",
    "            \n",
    "            # embed questions w/ positions\n",
    "            with tf.variable_scope('questions'):\n",
    "                questions_layer = tf.layers.dense(\n",
    "                    questions_embedded,\n",
    "                    self._hparams.d_hidden,\n",
    "                    activation = tf.nn.relu)\n",
    "                questions_layer += question_positions_embedded\n",
    "                questions_layer = tf.layers.batch_normalization(\n",
    "                    questions_layer,\n",
    "                    training = self._training)\n",
    "            \n",
    "            # context self-attention layers\n",
    "            for i in range(self._hparams.num_attn_layers_contexts):\n",
    "                with tf.variable_scope('contexts_self_%d' % i):\n",
    "                    attn = self._attention_layer(\n",
    "                        contexts_layer,\n",
    "                        contexts_layer,\n",
    "                        contexts_layer,\n",
    "                        distance_bias = True,\n",
    "                        mask_type = 's')\n",
    "                    contexts_layer += attn\n",
    "                    contexts_layer = tf.layers.batch_normalization(\n",
    "                        contexts_layer,\n",
    "                        training = self._training)\n",
    "\n",
    "            # question self-attention layers\n",
    "            for i in range(self._hparams.num_attn_layers_questions):\n",
    "                with tf.variable_scope('questions_self_%d' % i):\n",
    "                    attn = self._attention_layer(\n",
    "                        questions_layer,\n",
    "                        questions_layer,\n",
    "                        questions_layer,\n",
    "                        distance_bias = True,\n",
    "                        mask_type = 's')\n",
    "                    questions_layer += attn\n",
    "                    questions_layer = tf.layers.batch_normalization(\n",
    "                        questions_layer,\n",
    "                        training = self._training)\n",
    "\n",
    "            # joint attention layer\n",
    "            with tf.variable_scope('joint'):\n",
    "                attn = self._attention_layer(\n",
    "                    queries = contexts_layer,\n",
    "                    keys = questions_layer,\n",
    "                    values = questions_layer)\n",
    "                joint_layer = contexts_layer + attn\n",
    "                joint_layer = tf.layers.batch_normalization(\n",
    "                    joint_layer,\n",
    "                    training = self._training)\n",
    "\n",
    "            # joint self-attention layers\n",
    "            for i in range(self._hparams.num_attn_layers_joint):\n",
    "                with tf.variable_scope('joint_self_%d' % i):\n",
    "                    attn = self._attention_layer(\n",
    "                        joint_layer,\n",
    "                        joint_layer,\n",
    "                        joint_layer,\n",
    "                        distance_bias = True,\n",
    "                        mask_type = 's')\n",
    "                    joint_layer += attn\n",
    "                    joint_layer = tf.layers.batch_normalization(\n",
    "                        joint_layer,\n",
    "                        training = self._training)\n",
    "\n",
    "            # output: answer logits\n",
    "            self._answer_logits = tf.layers.dense(\n",
    "                joint_layer,\n",
    "                1,\n",
    "                name = 'answer_logits')\n",
    "            self._answer_logits = tf.squeeze(          # [batch_size, context_size]\n",
    "                self._answer_logits,\n",
    "                axis = -1,\n",
    "                name = 'answer_logits')\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        with tf.variable_scope('optimize'):\n",
    "            # answer mask\n",
    "            a0 = tf.sequence_mask(\n",
    "                self._answer_starts[:, 0],\n",
    "                self._hparams.context_size,\n",
    "                dtype = tf.int32)\n",
    "            a1 = tf.sequence_mask(\n",
    "                self._answer_ends[:, 0] + 1,\n",
    "                self._hparams.context_size,\n",
    "                dtype = tf.int32)\n",
    "            self._answers = tf.identity(a1 - a0, 'answers')\n",
    "\n",
    "            # individual losses\n",
    "            losses = tf.nn.weighted_cross_entropy_with_logits(\n",
    "                targets = tf.cast(self._answers, tf.float32),\n",
    "                logits = self._answer_logits,\n",
    "                pos_weight = self._hparams.loss_pos_weight)\n",
    "\n",
    "            # total loss\n",
    "            self._total_loss = tf.reduce_sum(losses) / tf.cast(self._hparams.context_size, tf.float32)\n",
    "            self._total_loss = tf.identity(self._total_loss, 'total_loss')\n",
    "            \n",
    "            # mean loss\n",
    "            self._mean_loss = self._total_loss / tf.cast(self._minibatch_size, tf.float32)\n",
    "            self._mean_loss = tf.identity(self._mean_loss, 'mean_loss')\n",
    "            \n",
    "            # estimated answers\n",
    "            self._answer_probs = tf.sigmoid(\n",
    "                self._answer_logits,\n",
    "                name = 'answer_probs')\n",
    "            self._answer_estimates = tf.cast(\n",
    "                self._answer_probs > 0.5,\n",
    "                tf.int32,\n",
    "                name = 'answer_estimates')\n",
    "\n",
    "            # F1\n",
    "            self._total_true_positives = tf.reduce_sum(\n",
    "                self._answers * self._answer_estimates,\n",
    "                name = 'total_true_positives')\n",
    "            self._total_false_positives = tf.reduce_sum(\n",
    "                (1 - self._answers) * self._answer_estimates,\n",
    "                name = 'total_false_positives')\n",
    "            self._total_false_negatives = tf.reduce_sum(\n",
    "                self._answers * (1 - self._answer_estimates),\n",
    "                name = 'total_false_negatives')\n",
    "            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self._global_step = tf.Variable(0, name = 'global_step', trainable = False)\n",
    "                self._optimizer = tf.train.AdamOptimizer(learning_rate = self._hparams.learning_rate)\n",
    "                \n",
    "                # gradient clipping\n",
    "                gradients, variables = zip(*self._optimizer.compute_gradients(self._mean_loss))\n",
    "                gradients, _ = tf.clip_by_global_norm(\n",
    "                    gradients, \n",
    "                    self._hparams.gradient_clip_norm)\n",
    "                \n",
    "                self._train_op = self._optimizer.apply_gradients(\n",
    "                    zip(gradients, variables),\n",
    "                    global_step = self._global_step)\n",
    "                \n",
    "    def process(self,\n",
    "                dataset_filenames,\n",
    "                dataset_limit = -1,\n",
    "                header = 'results',\n",
    "                train = False,\n",
    "                log_file = None):\n",
    "        # initialize dataset to files\n",
    "        self._session.run(self._dataset_iterator.initializer, feed_dict={\n",
    "            self._dataset_filenames: dataset_filenames,\n",
    "            self._dataset_limit: dataset_limit })\n",
    "\n",
    "        cum_loss = 0\n",
    "        cum_num_examples = 0\n",
    "        cum_tps = 0\n",
    "        cum_fps = 0\n",
    "        cum_fns = 0\n",
    "        \n",
    "        # start progress\n",
    "        start = datetime.datetime.now()\n",
    "        progress = tqdm_notebook(leave = False, desc = header)\n",
    "\n",
    "        while True:\n",
    "            # process a minibatch\n",
    "            try:\n",
    "                (_,\n",
    "                 curr_total_loss,\n",
    "                 curr_tps,\n",
    "                 curr_fps,\n",
    "                 curr_fns,\n",
    "                 curr_minibatch_size) = self._session.run(\n",
    "                    (self._train_op if train else (),\n",
    "                     self._total_loss,\n",
    "                     self._total_true_positives,\n",
    "                     self._total_false_positives,\n",
    "                     self._total_false_negatives,\n",
    "                     self._minibatch_size),\n",
    "                    feed_dict = { self._training: train })\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "            # update loss stats\n",
    "            cum_loss += curr_total_loss\n",
    "            cum_tps += curr_tps\n",
    "            cum_fps += curr_fps\n",
    "            cum_fns += curr_fns\n",
    "            cum_num_examples += curr_minibatch_size\n",
    "            \n",
    "            # update progress\n",
    "            progress.update(curr_minibatch_size)\n",
    "            progress.set_postfix(loss = cum_loss / cum_num_examples)\n",
    "\n",
    "        # end progress\n",
    "        progress.close()\n",
    "        finish = datetime.datetime.now()\n",
    "        \n",
    "        # precision\n",
    "        precision = 0\n",
    "        if cum_tps + cum_fps > 0:\n",
    "            precision = cum_tps / (cum_tps + cum_fps)\n",
    "            \n",
    "        # recall\n",
    "        recall = 0\n",
    "        if cum_tps + cum_fns > 0:\n",
    "            recall = cum_tps / (cum_tps + cum_fns)\n",
    "            \n",
    "        # F1\n",
    "        F1 = 0\n",
    "        if precision + recall > 0:\n",
    "            F1 = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "        # print/log output\n",
    "        message = '%s: time=%s, step=%d, loss=%g, precision=%g, recall=%g, F1=%g' % (\n",
    "            header,\n",
    "            finish - start,\n",
    "            tf.train.global_step(sess, self._global_step),\n",
    "            cum_loss / cum_num_examples,\n",
    "            precision,\n",
    "            recall,\n",
    "            F1)\n",
    "        print(message)\n",
    "        if log_file:\n",
    "            print(message, file=log_file)\n",
    "            log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../data/SQuAD/data_1.vocab.embeddings.npy.gz', 'rb') as f:\n",
    "    word_embeddings = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(path):\n",
    "    return sorted([os.path.join(path, file) for file in os.listdir(path)])\n",
    "\n",
    "train_set = list_files('../../data/SQuAD/data_1.train')\n",
    "dev_set = list_files('../../data/SQuAD/data_1.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"model/distance_scaling_factor:0\": 1\n",
      "parameters for \"model/position_embeddings:0\": 170000\n",
      "parameters for \"model/contexts/dense/kernel:0\": 60000\n",
      "parameters for \"model/contexts/dense/bias:0\": 200\n",
      "parameters for \"model/contexts/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/contexts/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/questions/dense/kernel:0\": 60000\n",
      "parameters for \"model/questions/dense/bias:0\": 200\n",
      "parameters for \"model/questions/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/questions/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/contexts_self_0/attention/key_projection:0\": 40000\n",
      "parameters for \"model/contexts_self_0/attention/query_projection:0\": 40000\n",
      "parameters for \"model/contexts_self_0/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/contexts_self_0/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/contexts_self_1/attention/key_projection:0\": 40000\n",
      "parameters for \"model/contexts_self_1/attention/query_projection:0\": 40000\n",
      "parameters for \"model/contexts_self_1/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/contexts_self_1/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/contexts_self_2/attention/key_projection:0\": 40000\n",
      "parameters for \"model/contexts_self_2/attention/query_projection:0\": 40000\n",
      "parameters for \"model/contexts_self_2/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/contexts_self_2/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/questions_self_0/attention/key_projection:0\": 40000\n",
      "parameters for \"model/questions_self_0/attention/query_projection:0\": 40000\n",
      "parameters for \"model/questions_self_0/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/questions_self_0/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/questions_self_1/attention/key_projection:0\": 40000\n",
      "parameters for \"model/questions_self_1/attention/query_projection:0\": 40000\n",
      "parameters for \"model/questions_self_1/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/questions_self_1/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/questions_self_2/attention/key_projection:0\": 40000\n",
      "parameters for \"model/questions_self_2/attention/query_projection:0\": 40000\n",
      "parameters for \"model/questions_self_2/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/questions_self_2/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/joint/attention/key_projection:0\": 40000\n",
      "parameters for \"model/joint/attention/query_projection:0\": 40000\n",
      "parameters for \"model/joint/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/joint/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/joint_self_0/attention/key_projection:0\": 40000\n",
      "parameters for \"model/joint_self_0/attention/query_projection:0\": 40000\n",
      "parameters for \"model/joint_self_0/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/joint_self_0/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/joint_self_1/attention/key_projection:0\": 40000\n",
      "parameters for \"model/joint_self_1/attention/query_projection:0\": 40000\n",
      "parameters for \"model/joint_self_1/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/joint_self_1/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/joint_self_2/attention/key_projection:0\": 40000\n",
      "parameters for \"model/joint_self_2/attention/query_projection:0\": 40000\n",
      "parameters for \"model/joint_self_2/batch_normalization/gamma:0\": 200\n",
      "parameters for \"model/joint_self_2/batch_normalization/beta:0\": 200\n",
      "parameters for \"model/answer_logits/kernel:0\": 200\n",
      "parameters for \"model/answer_logits/bias:0\": 1\n",
      "total parameters: 1095402\n"
     ]
    }
   ],
   "source": [
    "sess = reset_tf(sess)\n",
    "\n",
    "model = AttentionModel(sess, word_embeddings, HyperParameters())\n",
    "model._build_dataset_pipeline()\n",
    "model._build_model()\n",
    "model._build_optimizer()\n",
    "dump_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77f9af3dd344a65a1ff3319d8d1c36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train_0', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('../../logs/SQuAD/model_attention_2.1.log', 'wt') as f:\n",
    "    for i in range(5):\n",
    "        model.process(\n",
    "            train_set,\n",
    "            header = 'train_%d' % i,\n",
    "            train = True,\n",
    "            log_file = f)\n",
    "        model.process(\n",
    "            dev_set,\n",
    "            header = 'dev_%d' % i,\n",
    "            train = False,\n",
    "            log_file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(\n",
    "    model._dataset_iterator.initializer,\n",
    "    feed_dict = {\n",
    "        model._dataset_filenames: train_set[:1],\n",
    "        model._dataset_limit: 100 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts, questions, answers, answer_estimates = sess.run(\n",
    "    [model._contexts,\n",
    "     model._questions,\n",
    "     model._answers,\n",
    "     model._answer_estimates],\n",
    "    feed_dict = { model._training: False })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_estimates[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
