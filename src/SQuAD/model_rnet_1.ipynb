{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achang/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import collections\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from rnet_func import cudnn_gru, native_gru, dot_attention, summ, dropout, ptr_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_tf(sess = None, log_device_placement = False):\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    return tf.InteractiveSession(config = tf.ConfigProto(log_device_placement = log_device_placement))\n",
    "\n",
    "def dump_statistics():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        if shape.is_fully_defined():\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            print('parameters for \"%s\": %d' % (variable.name, variable_parameters))\n",
    "            total_parameters += variable_parameters\n",
    "        else:\n",
    "            print('parameters for \"%s\": ?' % (variable.name))\n",
    "    print('total parameters: %d' % total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    dropout_rate = 0.2\n",
    "    \n",
    "    max_context_len = 850\n",
    "    max_question_len = 60\n",
    "    max_word_len = 16\n",
    "    max_answer_len = 6\n",
    "    \n",
    "    hidden_dim = 75\n",
    "\n",
    "    data_batch_size = 64\n",
    "    data_num_parallel_calls = 2\n",
    "    data_prefetch_size = 256\n",
    "    data_shuffle_size = 512\n",
    "    \n",
    "    grad_clip_norm = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example_proto, hp):\n",
    "    # parse proto\n",
    "    parsed = tf.parse_single_example(example_proto, features={\n",
    "        'context_wids': tf.VarLenFeature(tf.int64),\n",
    "        'context_cids': tf.VarLenFeature(tf.int64),\n",
    "        'question_wids': tf.VarLenFeature(tf.int64),\n",
    "        'question_cids': tf.VarLenFeature(tf.int64),\n",
    "        'answer_starts': tf.VarLenFeature(tf.int64),\n",
    "        'answer_ends': tf.VarLenFeature(tf.int64), })\n",
    "\n",
    "    # convert to dense tensors\n",
    "    c_wids = tf.sparse_tensor_to_dense(parsed['context_wids'])\n",
    "    c_cids = tf.sparse_tensor_to_dense(parsed['context_cids'])\n",
    "    q_wids = tf.sparse_tensor_to_dense(parsed['question_wids'])\n",
    "    q_cids = tf.sparse_tensor_to_dense(parsed['question_cids'])\n",
    "    a0 = tf.sparse_tensor_to_dense(parsed['answer_starts'])\n",
    "    a1 = tf.sparse_tensor_to_dense(parsed['answer_ends'])\n",
    "\n",
    "    # determine word lengths\n",
    "    c_wlen = tf.shape(c_wids)[0]\n",
    "    q_wlen = tf.shape(q_wids)[0]\n",
    "    a_len = tf.shape(a0)[0]\n",
    "\n",
    "    # reshape char arrays\n",
    "    c_cids = tf.reshape(c_cids, [c_wlen, hp.max_word_len])\n",
    "    q_cids = tf.reshape(q_cids, [q_wlen, hp.max_word_len])\n",
    "\n",
    "    # pad to maximum length (necessary for batching tensors)\n",
    "    c_wids = tf.pad(c_wids, [[0, hp.max_context_len - c_wlen]])\n",
    "    c_cids = tf.pad(c_cids, [[0, hp.max_context_len - c_wlen], [0, 0]])\n",
    "    q_wids = tf.pad(q_wids, [[0, hp.max_question_len - q_wlen]])\n",
    "    q_cids = tf.pad(q_cids, [[0, hp.max_question_len - q_wlen], [0, 0]])\n",
    "    a0 = tf.pad(a0, [[0, hp.max_answer_len - a_len]])\n",
    "    a1 = tf.pad(a1, [[0, hp.max_answer_len - a_len]])\n",
    "    \n",
    "    # determine char lengths\n",
    "    c_clens = tf.reduce_sum(tf.cast(c_cids > 0, tf.int64), axis=-1)\n",
    "    q_clens = tf.reduce_sum(tf.cast(q_cids > 0, tf.int64), axis=-1)\n",
    "\n",
    "    return (c_wids, c_wlen, c_cids, c_clens, q_wids, q_wlen, q_cids, q_clens, a0, a1)\n",
    "\n",
    "def get_dataset(file, hp, limit=None, repeat=True):\n",
    "    def _parse(ex):\n",
    "        return parse_example(ex, hp)\n",
    "    d = tf.data.TFRecordDataset(file, compression_type = 'GZIP')\n",
    "    if limit:\n",
    "        d = d.take(limit)\n",
    "    d = d.map(_parse, num_parallel_calls=hp.data_num_parallel_calls)\n",
    "    d = d.shuffle(hp.data_shuffle_size)\n",
    "    if repeat:\n",
    "        d = d.repeat()\n",
    "    d = d.batch(hp.data_batch_size)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_dropout(input_data, rate, training, recurrent=True):\n",
    "    # sizes\n",
    "    batch_size = tf.shape(input_data)[0]\n",
    "    size = input_data.shape[-1].value\n",
    "\n",
    "    # noise mask\n",
    "    ns = None\n",
    "    if recurrent:\n",
    "        ns = [batch_size, 1, size]\n",
    "\n",
    "    # apply dropout\n",
    "    return tf.layers.dropout(input_data, rate=rate, training=training, noise_shape=ns)\n",
    "\n",
    "def rnn_unidir(input_data, \n",
    "               size, \n",
    "               dropout_rate, \n",
    "               training,\n",
    "               name='rnn_uni',\n",
    "               reuse=None):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        # sizes\n",
    "        batch_size = tf.shape(input_data)[0]\n",
    "\n",
    "        # GRU\n",
    "        gru = tf.contrib.cudnn_rnn.CudnnGRU(\n",
    "            num_layers=1, num_units=size, input_size=input_data.shape[-1].value)\n",
    "\n",
    "        # variables\n",
    "        gru_params = tf.get_variable(\n",
    "            'gru_params', [gru.params_size().eval()])\n",
    "        gru_input_h = tf.get_variable(\n",
    "            'gru_input_h', [1, 1, size])\n",
    "\n",
    "        # dropout\n",
    "        d_in = rnn_dropout(input_data, dropout_rate, training)\n",
    "\n",
    "        # tranpose to time-major\n",
    "        d_in = tf.transpose(d_in, perm=[1, 0, 2])\n",
    "\n",
    "        # tile input states\n",
    "        h_in = tf.tile(gru_input_h, [1, batch_size, 1])\n",
    "\n",
    "        # compute GRU\n",
    "        d_out, h_out = gru(d_in, h_in, gru_params)\n",
    "\n",
    "        # untranspose from time-major\n",
    "        d_out = tf.transpose(d_out, perm=[1, 0, 2])\n",
    "\n",
    "        return d_out\n",
    "\n",
    "def rnn_bidir(input_data,\n",
    "              input_lens,\n",
    "              size,\n",
    "              dropout_rate,\n",
    "              training,\n",
    "              name='rnn_bidir',\n",
    "              reuse=None):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        # reverse input\n",
    "        d_in_fw = input_data\n",
    "        d_in_bk = tf.reverse_sequence(input_data, input_lens, 1)\n",
    "\n",
    "        # RNN\n",
    "        d_out_fw = rnn_unidir(d_in_fw, size, dropout_rate, training, 'fw', reuse)\n",
    "        d_out_bk = rnn_unidir(d_in_bk, size, dropout_rate, training, 'bk', reuse)\n",
    "\n",
    "        # reverse output\n",
    "        d_out_bk = tf.reverse_sequence(d_out_bk, input_lens, 1)\n",
    "\n",
    "        # concat\n",
    "        return tf.concat([d_out_fw, d_out_bk], axis=-1)\n",
    "\n",
    "def rnn_bidir_multi(input_data,\n",
    "                    input_lens,\n",
    "                    size,\n",
    "                    num_layers,\n",
    "                    dropout_rate,\n",
    "                    training,\n",
    "                    name='rnn',\n",
    "                    reuse=None):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        d = input_data\n",
    "        d_out = []\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            d = rnn_bidir(d, input_lens, size, dropout_rate, training, 'layer_%d' % i, reuse)\n",
    "            d_out.append(d)\n",
    "\n",
    "        # concat outputs from all layers\n",
    "        return tf.concat(d_out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(inputs, memory, size, dropout_rate=0.0, training=False, name='attn', reuse=None):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        # dropout\n",
    "        i = rnn_dropout(inputs, dropout_rate, training)\n",
    "        m = rnn_dropout(memory, dropout_rate, training)\n",
    "\n",
    "        # project\n",
    "        i = tf.layers.dense(\n",
    "            i, size, use_bias=False, activation=tf.nn.relu, name='proj_i')\n",
    "        m = tf.layers.dense(\n",
    "            m, size, use_bias=False, activation=tf.nn.relu, name='proj_m')\n",
    "\n",
    "        # compute weights\n",
    "        m_T = tf.transpose(m, [0, 2, 1])\n",
    "        w = tf.matmul(i, m_T)\n",
    "        w = tf.nn.softmax(w)\n",
    "\n",
    "        # apply weights\n",
    "        outputs = tf.matmul(w, memory)\n",
    "        outputs = tf.concat([inputs, outputs], axis=-1)\n",
    "\n",
    "        # TODO: how important is this?\n",
    "        # compute gating weights\n",
    "        o = rnn_dropout(outputs, dropout_rate, training)\n",
    "        g = tf.nn.sigmoid(tf.layers.dense(o, outputs.shape[-1].value, use_bias=False, name='gate'))\n",
    "\n",
    "        # apply gating weights\n",
    "        return outputs * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(memory, size, dropout_rate=0.0, training=False, name='summ', reuse=None):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        # dropout\n",
    "        m = rnn_dropout(memory, dropout_rate, training)\n",
    "        \n",
    "        # compute weights\n",
    "        w = tf.layers.dense(m, size, activation=tf.nn.tanh, name='w0', reuse=reuse)\n",
    "        w = tf.layers.dense(w, 1, use_bias=False, name='w1', reuse=reuse)\n",
    "        w = tf.nn.softmax(w, 1)\n",
    "\n",
    "        # weights sum\n",
    "        return tf.reduce_sum(memory * w, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(c, id_to_word, a0, a1):\n",
    "    # normalize\n",
    "    a1 = max(a0, a1)\n",
    "    \n",
    "    # get tokens\n",
    "    tokens = [id_to_word[c[i]] for i in range(a0, a1+1)]\n",
    "    \n",
    "    # remove articles\n",
    "    tokens = [t for t in tokens if not re.match(r'^(a|an|the)$', t)]\n",
    "    \n",
    "    # remove punctuation\n",
    "    exclude = set(string.punctuation)\n",
    "    tokens = [''.join([ch for ch in t if ch not in exclude]) for t in tokens]\n",
    "    \n",
    "    # remove empty tokens\n",
    "    tokens = [t for t in tokens if len(t)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def f1_score(ans, est):\n",
    "    common = collections.Counter(ans) & collections.Counter(est)\n",
    "    tp = sum(common.values())\n",
    "    if tp == 0:\n",
    "        return 0\n",
    "    pre = tp / len(est)\n",
    "    rec = tp / len(ans)\n",
    "    f1 = 2 * pre * rec / (pre + rec)\n",
    "    return f1\n",
    "\n",
    "def score_answers(c, id_to_word, a0, a1, a0_est, a1_est):\n",
    "    N, M = a0.shape\n",
    "    ems = []\n",
    "    f1s = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        est = extract_answer(c[i], id_to_word, a0_est[i], a1_est[i])\n",
    "        em = 0\n",
    "        f1 = 0\n",
    "        \n",
    "        for j in range(M):\n",
    "            if a0[i, j] <= 0 and a1[i, j] <= 0:\n",
    "                break\n",
    "            ans = extract_answer(c[i], id_to_word, a0[i, j], a1[i, j])\n",
    "            em = max(em, 1 if ans == est else 0)\n",
    "            f1 = max(f1, f1_score(ans, est))\n",
    "        \n",
    "        ems.append(em)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    return np.mean(ems), np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnModel:\n",
    "    def __init__(self, hp, word_emb, data_it, handle):\n",
    "        # handle\n",
    "        self.handle = handle\n",
    "        \n",
    "        # training\n",
    "        self.training = tf.placeholder(tf.bool, name='training')\n",
    "\n",
    "        # read data (for speed)\n",
    "        (c_wid, c_wlen, c_cid, c_clen, \n",
    "         q_wid, q_wlen, q_cid, q_clen, \n",
    "         self.a0, self.a1) = data_it.get_next()\n",
    "\n",
    "        # trim data\n",
    "        c_max_wlen = tf.reduce_max(c_wlen)\n",
    "        q_max_wlen = tf.reduce_max(q_wlen)\n",
    "        c_wid = c_wid[:, :c_max_wlen]\n",
    "        c_cid = c_cid[:, :c_max_wlen, :]\n",
    "        q_wid = q_wid[:, :q_max_wlen]\n",
    "        q_cid = q_cid[:, :q_max_wlen, :]\n",
    "        \n",
    "        # save contexts for evaluation\n",
    "        self.c_wid = c_wid\n",
    "\n",
    "        # masks\n",
    "        c_wmask = tf.sequence_mask(c_wlen, c_max_wlen, dtype = tf.float32)\n",
    "        q_wmask = tf.sequence_mask(q_wlen, q_max_wlen, dtype = tf.float32)\n",
    "        \n",
    "        # embed\n",
    "        with tf.variable_scope('embed'):\n",
    "            word_emb = tf.get_variable(\n",
    "                'word', word_emb.shape,\n",
    "                initializer = tf.constant_initializer(word_emb),\n",
    "                trainable = False)\n",
    "            c_emb = tf.nn.embedding_lookup(word_emb, c_wid)\n",
    "            q_emb = tf.nn.embedding_lookup(word_emb, q_wid)\n",
    "\n",
    "        with tf.variable_scope(\"encoding\"):\n",
    "            rnn = cudnn_gru(\n",
    "                num_layers=3, num_units=hp.hidden_dim, batch_size=hp.data_batch_size,\n",
    "                input_size=c_emb.shape[-1].value, keep_prob=1-hp.dropout_rate,\n",
    "                is_train=self.training)\n",
    "            c = rnn(c_emb, seq_len=c_wlen)\n",
    "            q = rnn(q_emb, seq_len=q_wlen)\n",
    "\n",
    "        with tf.variable_scope(\"attention\"):\n",
    "            qc_att = dot_attention(\n",
    "                c, q, mask=q_wmask, hidden=hp.hidden_dim, keep_prob=1-hp.dropout_rate,\n",
    "                is_train=self.training)\n",
    "            rnn = cudnn_gru(\n",
    "                num_layers=1, num_units=hp.hidden_dim, batch_size=hp.data_batch_size,\n",
    "                input_size=qc_att.shape[-1].value, keep_prob=1-hp.dropout_rate,\n",
    "                is_train=self.training)\n",
    "            att = rnn(qc_att, seq_len=c_wlen)\n",
    "\n",
    "        with tf.variable_scope(\"match\"):\n",
    "            self_att = dot_attention(\n",
    "                att, att, mask=c_wmask, hidden=hp.hidden_dim, keep_prob=1-hp.dropout_rate,\n",
    "                is_train=self.training)\n",
    "            rnn = cudnn_gru(\n",
    "                num_layers=1, num_units=hp.hidden_dim, batch_size=hp.data_batch_size,\n",
    "                input_size=self_att.shape[-1].value, keep_prob=1-hp.dropout_rate,\n",
    "                is_train=self.training)\n",
    "            match = rnn(self_att, seq_len=c_wlen)\n",
    "\n",
    "        with tf.variable_scope(\"pointer\"):\n",
    "            init = summ(\n",
    "                q[:, :, -2 * hp.hidden_dim:], hp.hidden_dim, mask=q_wmask,\n",
    "                keep_prob=1-hp.dropout_rate, is_train=self.training)\n",
    "            pointer = ptr_net(\n",
    "                batch=hp.data_batch_size, hidden=init.shape[-1].value,\n",
    "                keep_prob=1-hp.dropout_rate, is_train=self.training)\n",
    "            l0, l1 = pointer(init, match, hp.hidden_dim, c_wmask)\n",
    "\n",
    "        with tf.variable_scope(\"predict\"):\n",
    "            outer = tf.matmul(\n",
    "                tf.expand_dims(tf.nn.softmax(l0), axis=2),\n",
    "                tf.expand_dims(tf.nn.softmax(l1), axis=1))\n",
    "            outer = tf.matrix_band_part(outer, 0, 15)\n",
    "            self.a0_est = tf.argmax(tf.reduce_max(outer, axis=2), axis=1)\n",
    "            self.a1_est = tf.argmax(tf.reduce_max(outer, axis=1), axis=1)\n",
    "            losses0 = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=l0, labels=self.a0[:, 0])\n",
    "            losses1 = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=l1, labels=self.a1[:, 0])\n",
    "            self.mean_loss = tf.reduce_mean(losses0 + losses1)\n",
    "        \n",
    "        # global step\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        \n",
    "        # optimizer\n",
    "        opt = tf.train.AdamOptimizer(hp.learning_rate)\n",
    "        gs = opt.compute_gradients(self.mean_loss)\n",
    "        gs, vs = zip(*gs)\n",
    "        gs, _ = tf.clip_by_global_norm(gs, hp.grad_clip_norm)\n",
    "        self.train_op = opt.apply_gradients(zip(gs, vs), global_step=self.global_step)\n",
    "    \n",
    "    def eval(self, sess, steps, data_handle, id_to_word):\n",
    "        em = 0\n",
    "        f1 = 0\n",
    "        n = 0\n",
    "        l = 0\n",
    "        for i in range(steps):\n",
    "            _l, c_wids, a0, a1, a0_est, a1_est = sess.run(\n",
    "                [self.mean_loss, self.c_wid,self.a0, self.a1, \n",
    "                 self.a0_est, self.a1_est],\n",
    "                feed_dict={ self.training: False, self.handle: data_handle })\n",
    "            _em, _f1 = score_answers(c_wids, id_to_word, a0, a1, a0_est, a1_est)\n",
    "            em += _em\n",
    "            f1 += _f1\n",
    "            l += _l\n",
    "            n += a0.size\n",
    "        return l/steps, em/steps, f1/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # encode\n",
    "#         with tf.variable_scope('encode'):\n",
    "#             c = rnn_bidir_multi(\n",
    "#                 c_wemb, c_wlens, hp.hidden_dim, 3, \n",
    "#                 hp.dropout_rate, self.training, 'rnn')\n",
    "#             q = rnn_bidir_multi(\n",
    "#                 q_wemb, q_wlens, hp.hidden_dim, 3,\n",
    "#                 hp.dropout_rate, self.training, 'rnn', reuse = True)\n",
    "#            \n",
    "#         # cross-attention\n",
    "#         with tf.variable_scope('cross_attn'):\n",
    "#             c *= tf.expand_dims(c_wmask, axis=-1)\n",
    "#             q *= tf.expand_dims(q_wmask, axis=-1)\n",
    "#             m = attention(\n",
    "#                 c, q, hp.hidden_dim, hp.dropout_rate, \n",
    "#                 self.training)\n",
    "#             m = rnn_bidir(\n",
    "#                 m, c_wlens, hp.hidden_dim, hp.dropout_rate, self.training, 'rnn')\n",
    "#            \n",
    "#         # self-attention\n",
    "#         with tf.variable_scope('self_attn'):\n",
    "#             m *= tf.expand_dims(c_wmask, axis=-1)\n",
    "#             m = attention(\n",
    "#                 m, m, hp.hidden_dim, hp.dropout_rate, \n",
    "#                 self.training)\n",
    "#             m = rnn_bidir(\n",
    "#                 m, c_wlens, hp.hidden_dim, hp.dropout_rate, self.training, 'rnn')\n",
    "#            \n",
    "#         # pointer\n",
    "#         with tf.variable_scope('pointer'):\n",
    "#             # add summary vectors\n",
    "#             s = summarize(\n",
    "#                 m, hp.hidden_dim, hp.dropout_rate, self.training)\n",
    "#             s = tf.expand_dims(s, axis=1)\n",
    "#             s = tf.tile(s, [1, c_max_wlen, 1])\n",
    "#             s = tf.concat([m, s], axis=-1)\n",
    "#            \n",
    "#             # dropout\n",
    "#             s = rnn_dropout(s, hp.dropout_rate, self.training)\n",
    "#            \n",
    "#             # compute logits\n",
    "#             l = tf.layers.dense(s, hp.hidden_dim, activation=tf.nn.tanh, name='w0')\n",
    "#             l = tf.layers.dense(w, 1, use_bias=False, name='w1')\n",
    "#             self._logits0 = l\n",
    "#\n",
    "#         # pointer\n",
    "#         l0 = rnn_dropout(m, hp.dropout_rate, self.training)\n",
    "#         l0 = tf.layers.dense(l0, 1, use_bias=False)\n",
    "#         l0 = tf.squeeze(l0, axis=-1)\n",
    "#         l0 *= c_wmask\n",
    "#\n",
    "#         # estimates\n",
    "#         self.a0_prob = tf.nn.softmax(l0)\n",
    "#         self.a0_est = tf.argmax(self.a0_prob, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../data/SQuAD/data_3.words.embeddings.npy.gz', 'rb') as f:\n",
    "    word_emb = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/SQuAD/data_3.words.txt', 'rt') as f:\n",
    "    id_to_word = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"encoding/Variable:0\": ?\n",
      "parameters for \"encoding/Variable_1:0\": ?\n",
      "parameters for \"encoding/Variable_2:0\": 4800\n",
      "parameters for \"encoding/Variable_3:0\": 4800\n",
      "parameters for \"encoding/Variable_4:0\": ?\n",
      "parameters for \"encoding/Variable_5:0\": ?\n",
      "parameters for \"encoding/Variable_6:0\": 4800\n",
      "parameters for \"encoding/Variable_7:0\": 4800\n",
      "parameters for \"encoding/Variable_8:0\": ?\n",
      "parameters for \"encoding/Variable_9:0\": ?\n",
      "parameters for \"encoding/Variable_10:0\": 4800\n",
      "parameters for \"encoding/Variable_11:0\": 4800\n",
      "parameters for \"attention/dot_attention/attention/inputs/W:0\": 33750\n",
      "parameters for \"attention/dot_attention/attention/memory/W:0\": 33750\n",
      "parameters for \"attention/dot_attention/gate/dense/W:0\": 810000\n",
      "parameters for \"attention/Variable:0\": ?\n",
      "parameters for \"attention/Variable_1:0\": ?\n",
      "parameters for \"attention/Variable_2:0\": 4800\n",
      "parameters for \"attention/Variable_3:0\": 4800\n",
      "parameters for \"match/dot_attention/attention/inputs/W:0\": 11250\n",
      "parameters for \"match/dot_attention/attention/memory/W:0\": 11250\n",
      "parameters for \"match/dot_attention/gate/dense/W:0\": 90000\n",
      "parameters for \"match/Variable:0\": ?\n",
      "parameters for \"match/Variable_1:0\": ?\n",
      "parameters for \"match/Variable_2:0\": 4800\n",
      "parameters for \"match/Variable_3:0\": 4800\n",
      "parameters for \"pointer/summ/s0/W:0\": 11250\n",
      "parameters for \"pointer/summ/s0/b:0\": 75\n",
      "parameters for \"pointer/summ/s/W:0\": 75\n",
      "parameters for \"pointer/ptr_net/pointer/s0/W:0\": 22500\n",
      "parameters for \"pointer/ptr_net/pointer/s/W:0\": 75\n",
      "parameters for \"pointer/ptr_net/gru_cell/gates/kernel:0\": 90000\n",
      "parameters for \"pointer/ptr_net/gru_cell/gates/bias:0\": 300\n",
      "parameters for \"pointer/ptr_net/gru_cell/candidate/kernel:0\": 45000\n",
      "parameters for \"pointer/ptr_net/gru_cell/candidate/bias:0\": 150\n",
      "total parameters: 1207425\n"
     ]
    }
   ],
   "source": [
    "sess = reset_tf(sess)\n",
    "\n",
    "hp = HyperParameters()\n",
    "\n",
    "data_train = get_dataset('../../data/SQuAD/data_3.train.tfrecords.gz', hp)\n",
    "data_dev = get_dataset('../../data/SQuAD/data_3.dev.tfrecords.gz', hp)\n",
    "\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "handle_train = data_train.make_one_shot_iterator().string_handle().eval()\n",
    "handle_dev = data_dev.make_one_shot_iterator().string_handle().eval()\n",
    "\n",
    "data_it = tf.data.Iterator.from_string_handle(\n",
    "    handle, data_train.output_types, data_train.output_shapes)\n",
    "\n",
    "model = RnnModel(hp, word_emb, data_it, handle)\n",
    "dump_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train_small = get_dataset('../../data/SQuAD/data_3.train.tfrecords.gz', hp, limit=1000)\n",
    "# handle_train_small = data_train_small.make_one_shot_iterator().string_handle().eval()\n",
    "# tr = tqdm_notebook(range(1000))\n",
    "# for i in tr:\n",
    "#     l, _, s = sess.run(\n",
    "#         [model.mean_loss, model.train_op, model.global_step],\n",
    "#         feed_dict={ model.training: True, model.handle: handle_train_small })\n",
    "#     tr.set_postfix(loss=l, step=s)\n",
    "#     if (i+1) % 100 == 0:\n",
    "#         print(model.eval(sess, 10, handle_train_small, id_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cc1f1afe9e4ca8a666f2b77a58dbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.summary.FileWriter('../../logs/SQuAD/model_rnet_1.2') as sfw:\n",
    "    tr = tqdm_notebook(range(10000))\n",
    "    for i in tr:\n",
    "        l, _, s = sess.run(\n",
    "            [model.mean_loss, model.train_op, model.global_step],\n",
    "            feed_dict={ model.training: True, model.handle: handle_train })\n",
    "        tr.set_postfix(loss=l, step=s)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            # evaluate\n",
    "            l_train, em_train, f1_train = model.eval(sess, 100, handle_train, id_to_word)\n",
    "            l_dev, em_dev, f1_dev = model.eval(sess, 100, handle_dev, id_to_word)\n",
    "\n",
    "            # summaries\n",
    "            sfw.add_summary(tf.Summary(value=[tf.Summary.Value(tag='train/loss', simple_value=l_train)]), s)\n",
    "            sfw.add_summary(tf.Summary(value=[tf.Summary.Value(tag='train/em', simple_value=em_train)]), s)\n",
    "            sfw.add_summary(tf.Summary(value=[tf.Summary.Value(tag='train/f1', simple_value=f1_train)]), s)\n",
    "            sfw.add_summary(tf.Summary(value=[tf.Summary.Value(tag='dev/loss', simple_value=l_dev)]), s)\n",
    "            sfw.add_summary(tf.Summary(value=[tf.Summary.Value(tag='dev/em', simple_value=em_dev)]), s)\n",
    "            sfw.add_summary(tf.Summary(value=[tf.Summary.Value(tag='dev/f1', simple_value=f1_dev)]), s)\n",
    "            sfw.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
