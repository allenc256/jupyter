{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achang/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_tf(sess = None, log_device_placement = False):\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    return tf.InteractiveSession(config = tf.ConfigProto(log_device_placement = log_device_placement))\n",
    "\n",
    "def dump_statistics():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        print('parameters for \"%s\": %d' % (variable.name, variable_parameters))\n",
    "        total_parameters += variable_parameters\n",
    "    print('total parameters: %d' % total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    context_size = 850\n",
    "    question_size = 60\n",
    "    answers_size = 6\n",
    "    \n",
    "    d_hidden = 128\n",
    "    \n",
    "    num_rnn_layers_contexts = 1\n",
    "    num_rnn_layers_questions = 1\n",
    "\n",
    "    dataset_batch_size = 64\n",
    "    dataset_num_parallel_calls = 4\n",
    "    dataset_prefetch_size = 1000\n",
    "    dataset_shuffle_size = 1000\n",
    "    \n",
    "    gradient_clip_norm = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnModel:\n",
    "    def __init__(self, session, word_embeddings, hparams):\n",
    "        self._session = session\n",
    "        self._word_embeddings = word_embeddings\n",
    "        self._hparams = hparams\n",
    "        \n",
    "    def _parse_example(self, example_proto):\n",
    "        # parse proto\n",
    "        parsed = tf.parse_single_example(example_proto, features = {\n",
    "            'context': tf.VarLenFeature(tf.int64),\n",
    "            'question': tf.VarLenFeature(tf.int64),\n",
    "            'answer_starts': tf.VarLenFeature(tf.int64),\n",
    "            'answer_ends': tf.VarLenFeature(tf.int64), })\n",
    "        \n",
    "        # convert to dense tensors\n",
    "        context = tf.sparse_tensor_to_dense(parsed['context'])\n",
    "        question = tf.sparse_tensor_to_dense(parsed['question'])\n",
    "        answer_starts = tf.sparse_tensor_to_dense(parsed['answer_starts'])\n",
    "        answer_ends = tf.sparse_tensor_to_dense(parsed['answer_ends'])\n",
    "        \n",
    "        # pad tensors\n",
    "        context_len = tf.shape(context)[0]\n",
    "        question_len = tf.shape(question)[0]\n",
    "        answers_len = tf.shape(answer_starts)[0]\n",
    "        zero_vector = self._word_embeddings.shape[0] - 1\n",
    "        context = tf.pad(\n",
    "            context,\n",
    "            [[0, self._hparams.context_size - context_len]],\n",
    "            constant_values = 0)\n",
    "        question = tf.pad(\n",
    "            question,\n",
    "            [[0, self._hparams.question_size - question_len]],\n",
    "            constant_values = 0)\n",
    "        answer_starts = tf.pad(\n",
    "            answer_starts,\n",
    "            [[0, self._hparams.answers_size - answers_len]],\n",
    "            constant_values = -1)\n",
    "        answer_ends = tf.pad(\n",
    "            answer_ends,\n",
    "            [[0, self._hparams.answers_size - answers_len]],\n",
    "            constant_values = -1)\n",
    "        \n",
    "        return (context, context_len, question, question_len, answer_starts, answer_ends)\n",
    "    \n",
    "    def _build_dataset_pipeline(self):\n",
    "        with tf.variable_scope('dataset'):\n",
    "            # placeholders\n",
    "            self._dataset_filenames = tf.placeholder(\n",
    "                tf.string,\n",
    "                shape = [None],\n",
    "                name = 'dataset_filenames')\n",
    "            self._dataset_limit = tf.placeholder_with_default(\n",
    "                tf.constant(-1, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_limit')\n",
    "            self._dataset_shuffle_size = tf.placeholder_with_default(\n",
    "                tf.constant(self._hparams.dataset_batch_size, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_shuffle_size')\n",
    "            self._dataset_batch_size = tf.placeholder_with_default(\n",
    "                tf.constant(self._hparams.dataset_batch_size, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_batch_size')\n",
    "            self._dataset_prefetch_size = tf.placeholder_with_default(\n",
    "                tf.constant(self._hparams.dataset_prefetch_size, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_prefetch_size')\n",
    "\n",
    "            # build dataset\n",
    "            dataset = tf.data.TFRecordDataset(\n",
    "                tf.random_shuffle(self._dataset_filenames),\n",
    "                compression_type='GZIP')\n",
    "            dataset = dataset.take(self._dataset_limit)\n",
    "            dataset = dataset.map(\n",
    "                self._parse_example,\n",
    "                num_parallel_calls = self._hparams.dataset_num_parallel_calls)\n",
    "            dataset = dataset.shuffle(self._dataset_shuffle_size)\n",
    "            dataset = dataset.prefetch(self._dataset_prefetch_size)\n",
    "            dataset = dataset.batch(self._dataset_batch_size)\n",
    "\n",
    "            # build iterator\n",
    "            self._dataset_iterator = dataset.make_initializable_iterator()\n",
    "            (contexts,\n",
    "             context_lens,\n",
    "             questions,\n",
    "             question_lens,\n",
    "             answer_starts,\n",
    "             answer_ends) = self._dataset_iterator.get_next()\n",
    "            \n",
    "            # give key tensors names\n",
    "            self._contexts = tf.identity(contexts, 'contexts')\n",
    "            self._context_lens = tf.identity(context_lens, 'context_lens')\n",
    "            self._questions = tf.identity(questions, 'questions')\n",
    "            self._question_lens = tf.identity(question_lens, 'question_lens')\n",
    "            self._answer_starts = tf.identity(answer_starts, 'answer_starts')\n",
    "            self._answer_ends = tf.identity(answer_ends, 'answer_ends')\n",
    "\n",
    "            # hint static shapes\n",
    "            self._contexts.set_shape([None, self._hparams.context_size])\n",
    "            self._questions.set_shape([None, self._hparams.question_size])\n",
    "            self._answer_starts.set_shape([None, self._hparams.answers_size])\n",
    "            self._answer_ends.set_shape([None, self._hparams.answers_size])\n",
    "\n",
    "            # minibatch size\n",
    "            self._minibatch_size = tf.shape(self._contexts)[0]\n",
    "            self._minibatch_size = tf.identity(self._minibatch_size, 'minibatch_size')\n",
    "    \n",
    "    def _bidirectional_rnn_layers(self, layer, num_layers, size):\n",
    "        # GRU\n",
    "        def build_gru(dropout):\n",
    "            return tf.contrib.cudnn_rnn.CudnnGRU(\n",
    "                num_layers = num_layers,\n",
    "                num_units = size,\n",
    "                input_size = layer.shape[-1].value,\n",
    "                dropout = dropout,\n",
    "                direction = 'bidirectional')\n",
    "        gru_with_dropout = build_gru(self._hparams.dropout_rate)\n",
    "        gru_without_dropout = build_gru(0.0)\n",
    "\n",
    "        # variables\n",
    "        gru_params = tf.get_variable(\n",
    "            'gru_params',\n",
    "            [gru_with_dropout.params_size().eval(session = self._session)])\n",
    "        gru_input_h = tf.get_variable(\n",
    "            'gru_input_h',\n",
    "            [2 * num_layers, size])\n",
    "\n",
    "        # make input hidden state\n",
    "        input_h = tf.expand_dims(gru_input_h, 1)\n",
    "        input_h = tf.tile(input_h, [1, self._minibatch_size, 1])\n",
    "\n",
    "        # make input data time-major\n",
    "        input_data = tf.transpose(layer, perm = [1, 0, 2])\n",
    "        \n",
    "        # run GRU\n",
    "        outputs = tf.cond(\n",
    "            self._training,\n",
    "            lambda: gru_with_dropout(input_data, input_h, gru_params)[0],\n",
    "            lambda: gru_without_dropout(input_data, input_h, gru_params)[0])\n",
    "        \n",
    "        # undo time-major\n",
    "        outputs = tf.transpose(outputs, perm = [1, 0, 2])\n",
    "        \n",
    "#         # maxout\n",
    "#         outputs = tf.maximum(outputs[:, :, :size], outputs[:, :, size:])\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _attention_layer(self,\n",
    "                         keys,\n",
    "                         queries,\n",
    "                         values,\n",
    "                         size = None,\n",
    "                         project = True,\n",
    "                         mask_self = False):\n",
    "        with tf.variable_scope('attention'):\n",
    "            # default size\n",
    "            if size is None:\n",
    "                size = keys.shape[-1].value\n",
    "                \n",
    "            # extract # queries/keys (must be statically known)\n",
    "            num_queries = queries.shape[-2].value\n",
    "            num_keys = keys.shape[-2].value\n",
    "            q = queries\n",
    "            k = keys\n",
    "            \n",
    "            # project\n",
    "            if project:\n",
    "                key_projection = tf.get_variable(\n",
    "                    'key_projection',\n",
    "                    [keys.shape[-1].value, size])\n",
    "                query_projection = tf.get_variable(\n",
    "                    'query_projection',\n",
    "                    [queries.shape[-1].value, size])\n",
    "                q = tf.tensordot(q, query_projection, axes = 1)     # [batch_size, num_queries, size]\n",
    "                q.set_shape([None, queries.shape[-2].value, size])\n",
    "                k = tf.tensordot(k, key_projection, axes = 1)       # [batch_size, num_keys, size]\n",
    "                k.set_shape([None, keys.shape[-2].value, size])\n",
    "                \n",
    "            # compute weights\n",
    "            k = tf.transpose(k, perm = [0, 2, 1])                   # [batch_size, size, num_keys]\n",
    "            w = tf.matmul(q, k)                                     # [batch_size, num_queries, num_keys]\n",
    "            w /= np.sqrt(size)\n",
    "            \n",
    "            # mask self-attention\n",
    "            if mask_self:\n",
    "                infinity= 1e25\n",
    "                mask = [[-infinity if i == j else infinity\n",
    "                    for j in range(num_keys)]\n",
    "                    for i in range(num_queries)]\n",
    "                mask = tf.constant(mask)\n",
    "                mask = tf.expand_dims(mask, axis = 0)               # [1, num_queries, num_keys]\n",
    "                w = tf.minimum(w, mask)\n",
    "\n",
    "            # softmax\n",
    "            w = tf.nn.softmax(w, name = 'weights')\n",
    "            \n",
    "            # dropout\n",
    "            w = tf.layers.dropout(\n",
    "                w,\n",
    "                rate = self._hparams.dropout_rate,\n",
    "                training = self._training)\n",
    "            \n",
    "            # apply weights\n",
    "            return tf.matmul(w, values)\n",
    "                \n",
    "    def _layer_norm(self, layer, epsilon = 1e-6, name = 'ln'):\n",
    "        with tf.variable_scope(name):\n",
    "            size = layer.shape[-1].value\n",
    "            scale = tf.get_variable(\n",
    "                'scale',\n",
    "                [size],\n",
    "                initializer = tf.ones_initializer())\n",
    "            bias = tf.get_variable(\n",
    "                'bias',\n",
    "                [size],\n",
    "                initializer = tf.zeros_initializer())\n",
    "            mean = tf.reduce_mean(\n",
    "                layer,\n",
    "                axis = -1,\n",
    "                keep_dims = True)\n",
    "            variance = tf.reduce_mean(\n",
    "                tf.square(layer - mean),\n",
    "                axis = -1,\n",
    "                keep_dims = True)\n",
    "            norm_layer = (layer - mean) * tf.rsqrt(variance + epsilon)\n",
    "            return norm_layer * scale + bias\n",
    "        \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('model'):\n",
    "            # placeholders\n",
    "            self._training = tf.placeholder(tf.bool, name = 'training')\n",
    "            \n",
    "            # init embedding\n",
    "            word_embeddings = tf.get_variable(\n",
    "                name = \"word_embeddings\",\n",
    "                shape = self._word_embeddings.shape,\n",
    "                initializer = tf.constant_initializer(self._word_embeddings),\n",
    "                trainable = False)\n",
    "            \n",
    "            # embed contexts/questions\n",
    "            contexts_embedded = tf.nn.embedding_lookup(\n",
    "                word_embeddings,\n",
    "                self._contexts)\n",
    "            questions_embedded = tf.nn.embedding_lookup(\n",
    "                word_embeddings,\n",
    "                self._questions)\n",
    "            \n",
    "            # RNN contexts/questions\n",
    "            # TODO: share parameters?\n",
    "            with tf.variable_scope('contexts_rnn'):\n",
    "                contexts_encoded = self._bidirectional_rnn_layers(\n",
    "                    contexts_embedded,\n",
    "                    self._hparams.num_rnn_layers_contexts,\n",
    "                    self._hparams.d_hidden)\n",
    "            with tf.variable_scope('questions_rnn'):\n",
    "                questions_encoded = self._bidirectional_rnn_layers(\n",
    "                    questions_embedded,\n",
    "                    self._hparams.num_rnn_layers_questions,\n",
    "                    self._hparams.d_hidden)\n",
    "                    \n",
    "            # apply masks\n",
    "            contexts_mask = tf.sequence_mask(    # [batch_size, context_size]\n",
    "                self._context_lens,\n",
    "                maxlen = self._hparams.context_size,\n",
    "                dtype = tf.float32)\n",
    "            contexts_mask_exp = tf.expand_dims(contexts_mask, axis = -1)\n",
    "            contexts_encoded *= contexts_mask_exp\n",
    "            questions_mask = tf.sequence_mask(   # [batch_size, context_size]\n",
    "                self._question_lens,\n",
    "                maxlen = self._hparams.question_size,\n",
    "                dtype = tf.float32)\n",
    "            questions_mask_exp = tf.expand_dims(questions_mask, axis = -1)\n",
    "            questions_encoded *= questions_mask_exp\n",
    "\n",
    "            # joint attention layer\n",
    "            with tf.variable_scope('joint_attn'):\n",
    "                attn = self._attention_layer(\n",
    "                    queries = contexts_encoded,\n",
    "                    keys = questions_encoded,\n",
    "                    values = questions_encoded)\n",
    "                joint_encoded = tf.concat(\n",
    "                    [contexts_encoded, attn],\n",
    "                    axis = -1)\n",
    "\n",
    "            # joint self-attention layer\n",
    "            with tf.variable_scope('joint_self_attn'):\n",
    "                attn = self._attention_layer(\n",
    "                    queries = joint_encoded,\n",
    "                    keys = joint_encoded,\n",
    "                    values = joint_encoded,\n",
    "                    project = False,\n",
    "                    mask_self = True)\n",
    "                joint_encoded = tf.concat(\n",
    "                    [joint_encoded, attn],\n",
    "                    axis = -1)\n",
    "            \n",
    "            # joint RNN layer\n",
    "            with tf.variable_scope('joint_rnn'):\n",
    "                # TODO: include question_summary in RNN input?\n",
    "                joint_encoded = self._bidirectional_rnn_layers(\n",
    "                    joint_encoded,\n",
    "                    1,\n",
    "                    self._hparams.d_hidden)\n",
    "                    \n",
    "            # summarize question\n",
    "            with tf.variable_scope('question_summary'):\n",
    "                q = self._layer_norm(questions_encoded)\n",
    "                q = tf.layers.dropout(\n",
    "                    q,\n",
    "                    rate = self._hparams.dropout_rate,\n",
    "                    training = self._training)\n",
    "                weights = tf.layers.dense(q, 1, use_bias = False)   # [batch_size, query_size, 1]\n",
    "                weights = tf.squeeze(weights, axis = -1)\n",
    "                weights = tf.nn.softmax(weights)\n",
    "                weights = tf.expand_dims(weights, axis = -1)\n",
    "                question_summary = questions_encoded * weights      # [batch_size, question_size, d_hidden]\n",
    "                question_summary = tf.reduce_sum(                   # [batch_size, d_hidden]\n",
    "                    question_summary,\n",
    "                    axis = 1)\n",
    "                \n",
    "            # compute answer pointers\n",
    "            with tf.variable_scope('answer'):\n",
    "                q = self._layer_norm(question_summary)\n",
    "                q = tf.layers.dropout(\n",
    "                    q,\n",
    "                    rate = self._hparams.dropout_rate,\n",
    "                    training = self._training)\n",
    "                q = tf.layers.dense(q, joint_encoded.shape[-1].value)\n",
    "                q = tf.expand_dims(q, axis = -1)                # [batch_size, d_hidden, 1]\n",
    "                l = tf.matmul(joint_encoded, q)                 # [batch_size, context_size, 1]\n",
    "                l = tf.squeeze(l, axis = -1)                    # [batch_size, context_size]\n",
    "                l *= contexts_mask\n",
    "                self._answer_start_logits = l\n",
    "\n",
    "#             self._answer_end_logits = tf.layers.dense(\n",
    "#                 joint_layer,\n",
    "#                 1,\n",
    "#                 use_bias = False,\n",
    "#                 name = 'answer_end_logits')\n",
    "#             self._answer_end_logits = tf.squeeze(      # [batch_size, context_size]\n",
    "#                 self._answer_end_logits,\n",
    "#                 axis = -1,\n",
    "#                 name = 'answer_end_logits')\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        with tf.variable_scope('optimize'):\n",
    "            # individual losses\n",
    "            # TODO: incorporate other answers into training\n",
    "            l0 = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = self._answer_starts[:, 0],\n",
    "                logits = self._answer_start_logits)\n",
    "#             l1 = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#                 labels = self._answer_ends[:, 0],\n",
    "#                 logits = self._answer_end_logits)\n",
    "\n",
    "            # total loss\n",
    "            self._total_loss = tf.reduce_sum(l0) # + tf.reduce_sum(l1)\n",
    "            self._total_loss = tf.identity(self._total_loss, 'total_loss')\n",
    "            \n",
    "            # mean loss\n",
    "            self._mean_loss = self._total_loss / tf.cast(self._minibatch_size, tf.float32)\n",
    "            self._mean_loss = tf.identity(self._mean_loss, 'mean_loss')\n",
    "            \n",
    "            # start/end probabilities/estimates\n",
    "            self._answer_start_probs = tf.nn.softmax(\n",
    "                self._answer_start_logits,\n",
    "                name = 'answer_start_logits')\n",
    "            self._answer_start_estimates = tf.argmax(\n",
    "                self._answer_start_probs,\n",
    "                axis = -1,\n",
    "                name = 'answer_start_estimates')\n",
    "#             self._answer_end_probs = tf.nn.softmax(\n",
    "#                 self._answer_end_logits,\n",
    "#                 name = 'answer_end_logits')\n",
    "#             # N.B., mask impossible answers\n",
    "#             mask = 1.0 - tf.sequence_mask(\n",
    "#                 self._answer_start_estimates,\n",
    "#                 self._hparams.context_size,\n",
    "#                 dtype = tf.float32)\n",
    "#             self._answer_end_estimates = tf.argmax(\n",
    "#                 mask * self._answer_end_probs,\n",
    "#                 axis = -1,\n",
    "#                 name = 'answer_end_estimates')\n",
    "            \n",
    "            # exact match accuracy\n",
    "            answer_starts_eq = tf.equal(\n",
    "                self._answer_starts[:, 0],\n",
    "                self._answer_start_estimates)\n",
    "            self._total_exact_matches = tf.reduce_sum(\n",
    "                tf.cast(answer_starts_eq, tf.int64),\n",
    "                name = 'total_exact_matches')\n",
    "#             answer_ends_eq = tf.equal(\n",
    "#                 self._answer_ends[:, 0],\n",
    "#                 self._answer_end_estimates)\n",
    "#             answers_eq = tf.logical_and(\n",
    "#                 answer_starts_eq,\n",
    "#                 answer_ends_eq)\n",
    "#             self._total_exact_matches = tf.reduce_sum(\n",
    "#                 tf.cast(answers_eq, tf.int64),\n",
    "#                 name = 'total_exact_matches')\n",
    "            \n",
    "#             # F1\n",
    "#             a0 = self._answer_starts[:, 0]\n",
    "#             a1 = self._answer_ends[:, 0] + 1\n",
    "#             answer_lens = a1 - a0\n",
    "#             b0 = self._answer_start_estimates\n",
    "#             b1 = self._answer_end_estimates + 1\n",
    "#             answer_estimate_lens = b1 - b0\n",
    "#             tps = tf.maximum(\n",
    "#                 tf.cast(0, tf.int64),\n",
    "#                 tf.minimum(a1, b1) - tf.maximum(a0, b0))\n",
    "#             fps = answer_estimate_lens - tps\n",
    "#             fns = answer_lens - tps\n",
    "#             self._total_true_positives = tf.reduce_sum(\n",
    "#                 tps,\n",
    "#                 name = 'total_true_positives')\n",
    "#             self._total_false_positives = tf.reduce_sum(\n",
    "#                 fps,\n",
    "#                 name = 'total_false_positives')\n",
    "#             self._total_false_negatives = tf.reduce_sum(\n",
    "#                 fns,\n",
    "#                 name = 'total_false_negatives')\n",
    "            self._total_true_positives = tf.constant(0)\n",
    "            self._total_false_positives = tf.constant(0)\n",
    "            self._total_false_negatives = tf.constant(0)\n",
    "            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self._global_step = tf.Variable(0, name = 'global_step', trainable = False)\n",
    "                self._optimizer = tf.train.AdamOptimizer(learning_rate = self._hparams.learning_rate)\n",
    "                \n",
    "                # gradient clipping\n",
    "                gradients, variables = zip(*self._optimizer.compute_gradients(self._mean_loss))\n",
    "                gradients, _ = tf.clip_by_global_norm(\n",
    "                    gradients, \n",
    "                    self._hparams.gradient_clip_norm)\n",
    "                \n",
    "                self._train_op = self._optimizer.apply_gradients(\n",
    "                    zip(gradients, variables),\n",
    "                    global_step = self._global_step)\n",
    "\n",
    "    def process(self,\n",
    "                dataset_filenames,\n",
    "                dataset_limit = -1,\n",
    "                header = 'results',\n",
    "                train = False,\n",
    "                log_file = None):\n",
    "        # initialize dataset to files\n",
    "        self._session.run(self._dataset_iterator.initializer, feed_dict={\n",
    "            self._dataset_filenames: dataset_filenames,\n",
    "            self._dataset_limit: dataset_limit })\n",
    "\n",
    "        cum_loss = 0\n",
    "        cum_num_examples = 0\n",
    "        cum_exact_matches = 0\n",
    "        cum_tps = 0\n",
    "        cum_fps = 0\n",
    "        cum_fns = 0\n",
    "        \n",
    "        # start progress\n",
    "        start = datetime.datetime.now()\n",
    "        progress = tqdm_notebook(leave = False, desc = header)\n",
    "\n",
    "        while True:\n",
    "            # process a minibatch\n",
    "            try:\n",
    "                (_,\n",
    "                 curr_total_loss,\n",
    "                 curr_exact_matches,\n",
    "                 curr_tps,\n",
    "                 curr_fps,\n",
    "                 curr_fns,\n",
    "                 curr_minibatch_size) = self._session.run(\n",
    "                    (self._train_op if train else (),\n",
    "                     self._total_loss,\n",
    "                     self._total_exact_matches,\n",
    "                     self._total_true_positives,\n",
    "                     self._total_false_positives,\n",
    "                     self._total_false_negatives,\n",
    "                     self._minibatch_size),\n",
    "                    feed_dict = { self._training: train })\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "            # update loss stats\n",
    "            cum_loss += curr_total_loss\n",
    "            cum_exact_matches += curr_exact_matches\n",
    "            cum_tps += curr_tps\n",
    "            cum_fps += curr_fps\n",
    "            cum_fns += curr_fns\n",
    "            cum_num_examples += curr_minibatch_size\n",
    "            \n",
    "            # update progress\n",
    "            progress.update(curr_minibatch_size)\n",
    "            progress.set_postfix(loss = cum_loss / cum_num_examples)\n",
    "\n",
    "        # end progress\n",
    "        progress.close()\n",
    "        finish = datetime.datetime.now()\n",
    "        \n",
    "        # precision\n",
    "        precision = 0\n",
    "        if cum_tps + cum_fps > 0:\n",
    "            precision = cum_tps / (cum_tps + cum_fps)\n",
    "            \n",
    "        # recall\n",
    "        recall = 0\n",
    "        if cum_tps + cum_fns > 0:\n",
    "            recall = cum_tps / (cum_tps + cum_fns)\n",
    "            \n",
    "        # F1\n",
    "        F1 = 0\n",
    "        if precision + recall > 0:\n",
    "            F1 = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "        # print/log output\n",
    "        message = '%s: time=%s, step=%d, loss=%g, exact_match=%g, precision=%g, recall=%g, F1=%g' % (\n",
    "            header,\n",
    "            finish - start,\n",
    "            tf.train.global_step(sess, self._global_step),\n",
    "            cum_loss / cum_num_examples,\n",
    "            cum_exact_matches / cum_num_examples,\n",
    "            precision,\n",
    "            recall,\n",
    "            F1)\n",
    "        print(message)\n",
    "        if log_file:\n",
    "            print(message, file=log_file)\n",
    "            log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../data/SQuAD/data_1.vocab.embeddings.npy.gz', 'rb') as f:\n",
    "    word_embeddings = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(path):\n",
    "    return sorted([os.path.join(path, file) for file in os.listdir(path)])\n",
    "\n",
    "train_set = list_files('../../data/SQuAD/data_1.train')\n",
    "dev_set = list_files('../../data/SQuAD/data_1.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"model/contexts_rnn/gru_params:0\": 330240\n",
      "parameters for \"model/contexts_rnn/gru_input_h:0\": 256\n",
      "parameters for \"model/questions_rnn/gru_params:0\": 330240\n",
      "parameters for \"model/questions_rnn/gru_input_h:0\": 256\n",
      "parameters for \"model/joint_attn/attention/key_projection:0\": 65536\n",
      "parameters for \"model/joint_attn/attention/query_projection:0\": 65536\n",
      "parameters for \"model/joint_rnn/gru_params:0\": 886272\n",
      "parameters for \"model/joint_rnn/gru_input_h:0\": 256\n",
      "parameters for \"model/question_summary/ln/scale:0\": 256\n",
      "parameters for \"model/question_summary/ln/bias:0\": 256\n",
      "parameters for \"model/question_summary/dense/kernel:0\": 256\n",
      "parameters for \"model/answer/ln/scale:0\": 256\n",
      "parameters for \"model/answer/ln/bias:0\": 256\n",
      "parameters for \"model/answer/dense/kernel:0\": 65536\n",
      "parameters for \"model/answer/dense/bias:0\": 256\n",
      "total parameters: 1745664\n"
     ]
    }
   ],
   "source": [
    "sess = reset_tf(sess)\n",
    "\n",
    "model = RnnModel(sess, word_embeddings, HyperParameters())\n",
    "model._build_dataset_pipeline()\n",
    "model._build_model()\n",
    "model._build_optimizer()\n",
    "dump_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211e14c6f34642c3908de84553277cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train_0', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train_0: time=0:10:54.938124, step=1369, loss=3.22867, exact_match=0.21329, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c473a28103474f9965e0b94e79e85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev_0', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev_0: time=0:00:23.085462, step=1369, loss=2.90148, exact_match=0.265847, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d3fca21bc040e1b94866d736e33893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train_1', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train_1: time=0:10:36.011760, step=2738, loss=2.52852, exact_match=0.340461, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e256649502f847dbabe4c294d6835435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev_1', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev_1: time=0:00:23.046734, step=2738, loss=2.37291, exact_match=0.376254, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bdd90eec6c4e56b9702b49f987702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train_2', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train_2: time=0:10:36.763171, step=4107, loss=1.9633, exact_match=0.467916, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fd937df2ce448ebc76b5f95ab015e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev_2', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev_2: time=0:00:23.010274, step=4107, loss=2.00709, exact_match=0.466982, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5682921beb244440ad41fcaf5edc9c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train_3', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train_3: time=0:10:37.182197, step=5476, loss=1.59578, exact_match=0.552689, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0d7f1cb6e04578a033a3e0db8b696a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev_3', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev_3: time=0:00:23.076727, step=5476, loss=1.86764, exact_match=0.502176, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e20a5cb26f643f78ac69368517d70d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train_4', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train_4: time=0:10:37.487284, step=6845, loss=1.3587, exact_match=0.605543, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a228cad00634cd2867605903a3e902c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev_4', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev_4: time=0:00:23.059613, step=6845, loss=1.85683, exact_match=0.499243, precision=0, recall=0, F1=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbd34e8c87d488fbde0e6f4ce637567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train_5', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e52d29864931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             log_file = f)\n\u001b[0m\u001b[1;32m      8\u001b[0m         model.process(\n\u001b[1;32m      9\u001b[0m             \u001b[0mdev_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-744e25d08037>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, dataset_filenames, dataset_limit, header, train, log_file)\u001b[0m\n\u001b[1;32m    480\u001b[0m                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_false_negatives\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                      self._minibatch_size),\n\u001b[0;32m--> 482\u001b[0;31m                     feed_dict = { self._training: train })\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('../../logs/SQuAD/model_rnn_1.3.log', 'wt') as f:\n",
    "    for i in range(10):\n",
    "        model.process(\n",
    "            train_set,\n",
    "            header = 'train_%d' % i,\n",
    "            train = True,\n",
    "            log_file = f)\n",
    "        model.process(\n",
    "            dev_set,\n",
    "            header = 'dev_%d' % i,\n",
    "            train = False,\n",
    "            log_file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(\n",
    "    model._dataset_iterator.initializer,\n",
    "    feed_dict = {\n",
    "        model._dataset_filenames: train_set[:1],\n",
    "        model._dataset_limit: 10 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts, context_lens, questions, question_lens, answer_starts, answer_ends, answer_start_estimates, answer_end_estimates = sess.run(\n",
    "    [model._contexts,\n",
    "     model._context_lens,\n",
    "     model._questions,\n",
    "     model._question_lens,\n",
    "     model._answer_starts,\n",
    "     model._answer_ends,\n",
    "     model._answer_start_estimates,\n",
    "     model._answer_end_estimates],\n",
    "    feed_dict = { model._training: False })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5,  571,    2, ...,    0,    0,    0],\n",
       "       [  36, 1448, 2230, ...,    0,    0,    0],\n",
       "       [   5, 3769,   87, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  69,   77,   37, ...,    0,    0,    0],\n",
       "       [   1, 9191, 2659, ...,    0,    0,    0],\n",
       "       [ 181,  832,  562, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29,   5,  78,  43,  34,  25, 124, 117,  55, 110])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_end_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 65,   4,  78,  49,  80, 181, 123, 117,  52, 110])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_starts[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 67,   5,  78,  49,  80, 189, 124, 117,  55, 110])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_ends[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = reset_tf(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = tf.contrib.cudnn_rnn.CudnnGRU(\n",
    "    num_layers = 1,\n",
    "    num_units = 50,\n",
    "    input_size = 100,\n",
    "    direction = 'bidirectional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45600"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.params_size().eval(session = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_params = tf.get_variable(\n",
    "    'gru_params',\n",
    "    [gru.params_size().eval()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h = tf.cast(np.random.rand(2, 30, 50), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.cast(np.random.rand(20, 30, 100), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(20), Dimension(30), Dimension(100)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gru(input_data, input_h, gru_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'CudnnRNN_5:0' shape=(20, 30, 100) dtype=float32>,\n",
       " <tf.Tensor 'CudnnRNN_5:1' shape=(2, 30, 50) dtype=float32>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 30, 100)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].eval().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CudnnGRU in module tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops:\n",
      "\n",
      "class CudnnGRU(_CudnnRNNNoInputC)\n",
      " |  Cudnn implementation of the GRU model.\n",
      " |  Cudnn RNN has an opaque parameter buffer that can be used for inference and\n",
      " |  training. But it is possible that the layout of the parameter buffers\n",
      " |  changes between generations. So it is highly recommended to use\n",
      " |  CudnnOpaqueParamsSaveable to save and restore weights and biases in a\n",
      " |  canonical format.\n",
      " |  \n",
      " |  This is a typical use case:\n",
      " |  \n",
      " |    * The user creates a CudnnRNN model.\n",
      " |    * The user query that parameter buffer size.\n",
      " |    * The user creates a variable of that size that serves as the parameter\n",
      " |        buffers.\n",
      " |    * The user either initialize the parameter buffer, or load the canonical\n",
      " |        weights into the parameter buffer.\n",
      " |    * The user calls the model with the parameter buffer for inference, or\n",
      " |        training.\n",
      " |    * If training, the user creates a Saver object.\n",
      " |    * If training, the user creates a CudnnOpaqueParamsSaveable object from the\n",
      " |        parameter buffer for it to be later saved in the canonical format. When\n",
      " |        creating a CudnnOpaqueParamsSaveable object, a name could be provided,\n",
      " |        which is useful in distinguishing the names of multiple\n",
      " |        CudnnOpaqueParamsSaveable objects (e.g. for an encoder-decoder model).\n",
      " |    * Once a while, the user saves the parameter buffer into model checkpoints\n",
      " |        with Saver.save().\n",
      " |    * When restoring, the user creates a CudnnOpaqueParamsSaveable object and\n",
      " |      uses Saver.restore() to restore the parameter buffer from the canonical\n",
      " |      format to a user-defined format, as well as to restore other savable\n",
      " |      objects in the checkpoint file.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CudnnGRU\n",
      " |      _CudnnRNNNoInputC\n",
      " |      _CudnnRNN\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from _CudnnRNNNoInputC:\n",
      " |  \n",
      " |  __call__(self, input_data, input_h, params, is_training=True)\n",
      " |      Runs the forward step for the Cudnn LSTM model.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_data: the input sequence to the RNN model. A Tensor of shape [?,\n",
      " |          batch_size, input_size].\n",
      " |        input_h: the initial hidden state for h. A Tensor of shape [num_layers,\n",
      " |          batch_size, num_units].\n",
      " |        params: the parameter buffer created for this model.\n",
      " |        is_training: whether this operation will be used in training or inference.\n",
      " |      Returns:\n",
      " |        output: the output sequuence.\n",
      " |        output_h: the final state for h.\n",
      " |  \n",
      " |  __init__(self, num_layers, num_units, input_size, input_mode='linear_input', direction='unidirectional', dtype=tf.float32, dropout=0.0, seed=0)\n",
      " |      Creates a Cudnn RNN model from model without hidden-state C.\n",
      " |      \n",
      " |      Args:\n",
      " |        num_layers: the number of layers for the RNN model.\n",
      " |        num_units: the number of units within the RNN model.\n",
      " |        input_size: the size of the input, it could be different from the\n",
      " |            num_units.\n",
      " |        input_mode: indicate whether there is a linear projection between the\n",
      " |            input and The actual computation before the first layer. It could be\n",
      " |            'skip_input', 'linear_input' or 'auto_select'.\n",
      " |            'skip_input' is only allowed when input_size == num_units;\n",
      " |            'auto_select' implies 'skip_input' when input_size == num_units;\n",
      " |            otherwise, it implies 'linear_input'.\n",
      " |        direction: the direction model that the model operates. Could be either\n",
      " |            'unidirectional' or 'bidirectional'\n",
      " |        dtype: dtype of params, tf.float32 or tf.float64.\n",
      " |        dropout: whether to enable dropout. With it is 0, dropout is disabled.\n",
      " |        seed: the seed used for initializing dropout.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if direction is not 'unidirectional' or 'bidirectional'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CudnnRNN:\n",
      " |  \n",
      " |  canonical_to_params(self, weights, biases)\n",
      " |      Converts params from the canonical format to a specific format of cuDNN.\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a Tensor for weight parameters.\n",
      " |        biases: a Tensor for bias parameters.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A function for the canonical-to-params-to-specific conversion..\n",
      " |  \n",
      " |  params_size(self)\n",
      " |      Calculates the size of the opaque parameter buffer needed for this model.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The calculated parameter buffer size.\n",
      " |  \n",
      " |  params_to_canonical(self, params)\n",
      " |      Converts params from a specific format of cuDNN to the canonical format.\n",
      " |      \n",
      " |      Args:\n",
      " |        params: a Variable for weight and bias parameters.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A function for the specific-to-canonical conversion.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _CudnnRNN:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  direction\n",
      " |  \n",
      " |  input_mode\n",
      " |  \n",
      " |  input_size\n",
      " |  \n",
      " |  num_layers\n",
      " |  \n",
      " |  num_units\n",
      " |  \n",
      " |  rnn_mode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.contrib.cudnn_rnn.CudnnGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [4, 5, 6, 7]], dtype=int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tile(tf.reshape(tf.range(2*4), [2, 1, 4]), [1, 3, 1]).eval()[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(2, 1, 4) dtype=int32>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(tf.reshape(tf.range(2*4), [2, 4]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.reshape(tf.range(2*4), [2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1],\n",
       "        [4, 5]], dtype=int32), array([[2, 3],\n",
       "        [6, 7]], dtype=int32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, :2].eval(), x[:, 2:].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [4, 5, 6, 7]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(x, y = 2*x):\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul:0' shape=(2, 4) dtype=int32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = tf.contrib.cudnn_rnn.CudnnGRU(\n",
    "    num_layers = 1,\n",
    "    num_units = 10,\n",
    "    input_size = 10,\n",
    "    dropout = 0.5,\n",
    "    direction = 'bidirectional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1320"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.params_size().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
