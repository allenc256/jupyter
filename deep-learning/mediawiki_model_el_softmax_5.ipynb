{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achang/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf(sess = None, log_device_placement = False):\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    return tf.InteractiveSession(config = tf.ConfigProto(log_device_placement = log_device_placement))\n",
    "\n",
    "def dump_statistics():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        print('parameters for \"%s\": %d' % (variable.name, variable_parameters))\n",
    "        total_parameters += variable_parameters\n",
    "    print('total parameters: %d' % total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    num_targets = 2000\n",
    "    \n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    context_size = 81\n",
    "    \n",
    "    d_embedding_position = 32\n",
    "    \n",
    "    d_attention = 128\n",
    "    d_attention_ff = 128\n",
    "    \n",
    "    attention_num_layers = 4\n",
    "\n",
    "    dataset_batch_size = 512\n",
    "    dataset_num_parallel_calls = 4\n",
    "    dataset_prefetch_size = 4096\n",
    "    dataset_shuffle_size = 4096\n",
    "    \n",
    "    max_distance_bias = 10\n",
    "    \n",
    "    gradient_clip_norm = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityLinkingModel:\n",
    "    def __init__(self, session, word_embeddings, hp):\n",
    "        self._session = session\n",
    "        self._word_embeddings = word_embeddings\n",
    "        self._hp = hp\n",
    "        \n",
    "    def _parse_example(self, example_proto):\n",
    "        parsed = tf.parse_single_example(example_proto, features = {\n",
    "            'page_id': tf.FixedLenFeature([1], tf.int64),\n",
    "            'target_id': tf.FixedLenFeature([1], tf.int64),\n",
    "            'context_word_ids': tf.FixedLenFeature([self._hp.context_size], tf.int64),\n",
    "            'link_mask': tf.FixedLenFeature([self._hp.context_size], tf.int64) })\n",
    "        \n",
    "#         # apply random crop\n",
    "#         offset = tf.random_uniform(\n",
    "#             [],\n",
    "#             maxval = self._hp.input_context_size - self._hp.context_size,\n",
    "#             dtype = tf.int32)\n",
    "#\n",
    "#         # apply croppings\n",
    "#         context_word_ids = parsed['context_word_ids']\n",
    "#         context_word_ids = context_word_ids[offset:offset + self._hp.context_size]\n",
    "#         link_mask = parsed['link_mask']\n",
    "#         link_mask = link_mask[offset:offset + self._hp.context_size]\n",
    "\n",
    "        context_word_ids = parsed['context_word_ids']\n",
    "        link_mask = parsed['link_mask']\n",
    "        \n",
    "        return (parsed['target_id'], context_word_ids, link_mask)\n",
    "\n",
    "    def _build_data_pipeline(self):\n",
    "        with tf.variable_scope('dataset'):\n",
    "            # placeholders\n",
    "            self._dataset_filenames = tf.placeholder(\n",
    "                tf.string,\n",
    "                shape = [None],\n",
    "                name = 'dataset_filenames')\n",
    "            self._dataset_limit = tf.placeholder_with_default(\n",
    "                tf.constant(-1, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_limit')\n",
    "            self._dataset_shuffle_size = tf.placeholder_with_default(\n",
    "                tf.constant(self._hp.dataset_batch_size, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_shuffle_size')\n",
    "            self._dataset_batch_size = tf.placeholder_with_default(\n",
    "                tf.constant(self._hp.dataset_batch_size, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_batch_size')\n",
    "            self._dataset_prefetch_size = tf.placeholder_with_default(\n",
    "                tf.constant(self._hp.dataset_prefetch_size, tf.int64),\n",
    "                shape = [],\n",
    "                name = 'dataset_prefetch_size')\n",
    "\n",
    "            # build dataset\n",
    "            dataset = tf.data.TFRecordDataset(\n",
    "                tf.random_shuffle(self._dataset_filenames),\n",
    "                compression_type='GZIP')\n",
    "            dataset = dataset.take(self._dataset_limit)\n",
    "            dataset = dataset.map(\n",
    "                self._parse_example,\n",
    "                num_parallel_calls = self._hp.dataset_num_parallel_calls)\n",
    "            dataset = dataset.shuffle(self._dataset_shuffle_size)\n",
    "            dataset = dataset.prefetch(self._dataset_prefetch_size)\n",
    "            dataset = dataset.batch(self._dataset_batch_size)\n",
    "\n",
    "            # build iterator\n",
    "            self._dataset_iterator = dataset.make_initializable_iterator()\n",
    "            (target_labels, context_word_ids, link_mask) = self._dataset_iterator.get_next()\n",
    "            target_labels = tf.squeeze(target_labels, axis = -1)\n",
    "\n",
    "            # give key tensors names\n",
    "            self._context_word_ids = tf.identity(context_word_ids, 'context_word_ids')\n",
    "            self._target_labels = tf.identity(target_labels, 'target_labels')\n",
    "            self._link_mask = tf.identity(link_mask, 'link_mask')\n",
    "\n",
    "            # minibatch size\n",
    "            self._minibatch_size = tf.shape(self._context_word_ids)[0]\n",
    "            self._minibatch_size = tf.identity(self._minibatch_size, 'minibatch_size')\n",
    "            \n",
    "            # positions\n",
    "            p = tf.range(self._hp.context_size, dtype = tf.int64)\n",
    "            p = tf.tile(p, [self._minibatch_size])\n",
    "            p = tf.reshape(\n",
    "                p,\n",
    "                [self._minibatch_size, self._hp.context_size],\n",
    "                name = 'context_positions')\n",
    "            self._context_positions = p\n",
    "    \n",
    "    def _attention_distance_bias(self):\n",
    "        return tf.diag([-1e20] * self._hp.context_size)\n",
    "        \n",
    "#         # weights\n",
    "#         init = [0.0] * self._hp.max_distance_bias\n",
    "#         for i in range(self._hp.max_distance_bias):\n",
    "#             init[i] = -(i / (self._hp.max_distance_bias - 1)) * 4.0 + 2.0\n",
    "#         weights = tf.get_variable(\n",
    "#             'distance_bias',\n",
    "#             initializer = init)\n",
    "#\n",
    "#         # compute banded matrix\n",
    "#         ones = tf.ones([self._hp.context_size, self._hp.context_size])\n",
    "#         bias = tf.diag([-1e20] * self._hp.context_size)\n",
    "#         for i in range(self._hp.max_distance_bias):\n",
    "#             if i == self._hp.max_distance_bias - 1:\n",
    "#                 band = 1.0 - tf.matrix_band_part(\n",
    "#                     ones, \n",
    "#                     self._hp.max_distance_bias - 1, \n",
    "#                     self._hp.max_distance_bias - 1)\n",
    "#             else:\n",
    "#                 band = tf.matrix_band_part(ones, i + 1, i + 1) - tf.matrix_band_part(ones, i, i)\n",
    "#             bias += band * weights[i]\n",
    "#           \n",
    "#         return bias\n",
    "    \n",
    "#         weights = tf.get_variable(\n",
    "#             'distance_bias',\n",
    "#             [self._hp.context_size, self._hp.context_size])\n",
    "#         weights *= 1.0 - tf.diag([1.0] * self._hp.context_size)\n",
    "#         weights += tf.diag([-1e20] * self._hp.context_size)\n",
    "#         return weights\n",
    "\n",
    "            \n",
    "    def _attention_self(self, layer, distance_bias):\n",
    "        with tf.variable_scope('self'):\n",
    "#             # variables\n",
    "#             kernels = tf.get_variable(\n",
    "#                 'kernels',\n",
    "#                 [2, self._hp.d_attention, self._hp.d_attention])\n",
    "            \n",
    "#             # compute weights\n",
    "#             k0 = tf.tensordot(layer, kernels[0], axes = 1) # [batch_size, context_size, d_attention]\n",
    "#             k0.set_shape([None, self._hp.context_size, self._hp.d_attention])\n",
    "#             k1 = tf.tensordot(layer, kernels[1], axes = 1) # [batch_size, context_size, d_attention]\n",
    "#             k1.set_shape([None, self._hp.context_size, self._hp.d_attention])\n",
    "#             k1 = tf.transpose(k1, perm = [0, 2, 1])        # [batch_size, d_attention, context_size]\n",
    "#             w = tf.matmul(k0, k1)                          # [batch_size, context_size, context_size]\n",
    "\n",
    "            # compute weights\n",
    "            k0 = tf.layers.dense(                          # [batch_size, context_size, d_attention]\n",
    "                layer,\n",
    "                self._hp.d_attention)\n",
    "            k1 = tf.transpose(k0, perm = [0, 2, 1])        # [batch_size, d_attention, context_size]\n",
    "            w = tf.matmul(k0, k1)                          # [batch_size, context_size, context_size]\n",
    "            w += tf.expand_dims(distance_bias, axis = 0)\n",
    "            w *= self._attention_scaling_factor\n",
    "            w = tf.nn.softmax(w)\n",
    "            \n",
    "            # apply weights (including residual)\n",
    "            layer += tf.matmul(w, layer)\n",
    "            \n",
    "            # batch norm\n",
    "            layer = tf.layers.batch_normalization(\n",
    "                layer,\n",
    "                training = self._training)\n",
    "\n",
    "            # dropout\n",
    "            layer = tf.layers.dropout(\n",
    "                layer,\n",
    "                rate = self._hp.dropout_rate,\n",
    "                training = self._training)\n",
    "            \n",
    "            return layer\n",
    "    \n",
    "    def _attention_output(self, layer):\n",
    "        with tf.variable_scope('output'):\n",
    "            # variables\n",
    "            kernel = tf.get_variable(\n",
    "                'kernel',\n",
    "                [self._hp.d_attention, self._hp.d_attention])\n",
    "            query = tf.get_variable(\n",
    "                'query',\n",
    "                [self._hp.d_attention, 1])\n",
    "            \n",
    "            # compute weights\n",
    "            k = tf.tensordot(layer, kernel, axes = 1) # [batch_size, context_size, d_attention]\n",
    "            k.set_shape([None, self._hp.context_size, self._hp.d_attention])\n",
    "            w = tf.tensordot(k, query, axes = 1)      # [batch_size, context_size, 1]\n",
    "            w.set_shape([None, self._hp.context_size, 1])\n",
    "            w *= self._attention_scaling_factor\n",
    "            mask = (1.0 - tf.cast(self._link_mask, tf.float32)) * -1e20 # [batch_size, context_size]\n",
    "            mask = tf.expand_dims(mask, axis = -1)    # [batch_size, context_size, 1]\n",
    "            w += mask\n",
    "            w = tf.nn.softmax(w, dim = 1)\n",
    "            \n",
    "            # apply weights\n",
    "            layer *= w\n",
    "            layer = tf.reduce_sum(layer, axis = -2)   # [batch_size, d_attention]\n",
    "            \n",
    "            return layer\n",
    "\n",
    "    def _attention_feed_forward(self, layer):\n",
    "        with tf.variable_scope('ff'):\n",
    "            # hidden layer\n",
    "            layer = tf.layers.dense(\n",
    "                layer,\n",
    "                self._hp.d_attention_ff,\n",
    "                activation = tf.nn.relu,\n",
    "                name = 'fc1')\n",
    "            \n",
    "            # output\n",
    "            layer = tf.layers.dense(\n",
    "                layer,\n",
    "                self._hp.d_attention,\n",
    "                name = 'fc2')\n",
    "            \n",
    "            # batch norm\n",
    "            layer = tf.layers.batch_normalization(\n",
    "                layer,\n",
    "                training = self._training)\n",
    "            \n",
    "            # dropout\n",
    "            layer = tf.layers.dropout(\n",
    "                layer, \n",
    "                rate = self._hp.dropout_rate, \n",
    "                training = self._training)\n",
    "        \n",
    "            return layer\n",
    "            \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('model'):\n",
    "            # placeholder: training flag\n",
    "            self._training = tf.placeholder(tf.bool, name = 'training')\n",
    "            \n",
    "            # embed context words\n",
    "            word_embeddings = tf.get_variable(\n",
    "                'word_embeddings', \n",
    "                shape = self._word_embeddings.shape,\n",
    "                initializer = tf.constant_initializer(self._word_embeddings),\n",
    "                trainable = False)\n",
    "            context_words_embedded = tf.nn.embedding_lookup(\n",
    "                word_embeddings,\n",
    "                self._context_word_ids)\n",
    "\n",
    "            # embed context positions\n",
    "            position_embeddings = tf.get_variable(\n",
    "                'position_embeddings',\n",
    "                [self._hp.context_size, self._hp.d_embedding_position],\n",
    "                dtype=tf.float32)\n",
    "            context_positions_embedded = tf.nn.embedding_lookup(\n",
    "                position_embeddings,\n",
    "                self._context_positions)\n",
    "\n",
    "            # build full context vector (concat embeddings)\n",
    "            context_full = tf.concat(\n",
    "                [context_words_embedded, context_positions_embedded], \n",
    "                axis=-1)\n",
    "            \n",
    "            # build attention layers\n",
    "            with tf.variable_scope('attention'):\n",
    "                # scaling factor\n",
    "                self._attention_scaling_factor = tf.get_variable(\n",
    "                    'scaling_factor',\n",
    "                    shape = [],\n",
    "                    initializer = tf.constant_initializer([1.0 / np.sqrt(self._hp.d_attention)]))\n",
    "                \n",
    "                # distance bias\n",
    "                distance_bias = self._attention_distance_bias()\n",
    "                \n",
    "                # build input vector\n",
    "                context_attention = tf.layers.dense(\n",
    "                    context_full,\n",
    "                    self._hp.d_attention,\n",
    "                    activation=tf.nn.relu,\n",
    "                    name='input')\n",
    "                context_attention = tf.layers.batch_normalization(\n",
    "                    context_attention,\n",
    "                    training=self._training,\n",
    "                    name='input')\n",
    "                context_attention = tf.layers.dropout(\n",
    "                    context_attention,\n",
    "                    rate=self._hp.dropout_rate,\n",
    "                    training=self._training)\n",
    "                \n",
    "                layer = context_attention\n",
    "                for i in range(self._hp.attention_num_layers):\n",
    "                    with tf.variable_scope('layer_%d' % i):\n",
    "                        layer = self._attention_self(layer, distance_bias)\n",
    "#                         layer = self._attention_feed_forward(layer)\n",
    "                    \n",
    "                layer = self._attention_output(layer)\n",
    "            \n",
    "            # build final softmax layer\n",
    "            self._output_logits = tf.layers.dense(\n",
    "                layer,\n",
    "                self._hp.num_targets,\n",
    "                name = 'softmax')\n",
    "\n",
    "    def _build_training_model(self):\n",
    "        with tf.variable_scope('train'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = self._target_labels,\n",
    "                logits = self._output_logits)\n",
    "            \n",
    "            self._total_loss = tf.reduce_sum(losses, name = 'total_loss')\n",
    "            self._mean_loss = tf.reduce_mean(losses, name = 'mean_loss')\n",
    "            \n",
    "            # N.B., tf.nn.softmax here is unnecessary?\n",
    "            output_probs = tf.nn.softmax(self._output_logits)\n",
    "            output_labels = tf.argmax(output_probs, axis=-1)\n",
    "            correct_labels = tf.cast(\n",
    "                tf.equal(output_labels, self._target_labels), \n",
    "                tf.int32)\n",
    "            self._output_labels = tf.identity(output_labels, name = 'output_labels')\n",
    "            self._num_correct_labels = tf.reduce_sum(correct_labels, name = 'num_correct_labels')\n",
    "            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self._global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self._optimizer = tf.train.AdamOptimizer(learning_rate=self._hp.learning_rate)\n",
    "                \n",
    "                # gradient clipping\n",
    "                gradients, variables = zip(*self._optimizer.compute_gradients(self._mean_loss))\n",
    "                gradients, _ = tf.clip_by_global_norm(\n",
    "                    gradients, \n",
    "                    self._hp.gradient_clip_norm)\n",
    "                \n",
    "                self._train_op = self._optimizer.apply_gradients(\n",
    "                    zip(gradients, variables),\n",
    "                    global_step = self._global_step)\n",
    "#                 self._train_op = self._optimizer.minimize(\n",
    "#                     self._mean_loss,\n",
    "#                     global_step=self._global_step)\n",
    "\n",
    "    \n",
    "    def build_model(self):\n",
    "        self._build_data_pipeline()\n",
    "        self._build_model()\n",
    "        self._build_training_model()\n",
    "\n",
    "    def process(self,\n",
    "                dataset_filenames,\n",
    "                dataset_limit = -1,\n",
    "                header = 'results',\n",
    "                train = False,\n",
    "                log_file = None):\n",
    "        # initialize dataset to files\n",
    "        self._session.run(self._dataset_iterator.initializer, feed_dict={\n",
    "            self._dataset_filenames: dataset_filenames,\n",
    "            self._dataset_limit: dataset_limit })\n",
    "\n",
    "        cum_loss = 0\n",
    "        cum_num_examples = 0\n",
    "        cum_correct_examples = 0\n",
    "        \n",
    "        start = datetime.datetime.now()\n",
    "        progress = tqdm_notebook(leave = False, desc = header)\n",
    "\n",
    "        while True:\n",
    "            # process a minibatch\n",
    "            try:\n",
    "                (_,\n",
    "                 curr_total_loss, \n",
    "                 curr_minibatch_size,\n",
    "                 curr_num_correct_labels) = self._session.run(\n",
    "                    (self._train_op if train else (),\n",
    "                     self._total_loss,\n",
    "                     self._minibatch_size,\n",
    "                     self._num_correct_labels),\n",
    "                    feed_dict = { self._training: train })\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "            # update stats/progress\n",
    "            cum_loss += curr_total_loss\n",
    "            cum_num_examples += curr_minibatch_size\n",
    "            cum_correct_examples += curr_num_correct_labels\n",
    "            progress.update(curr_minibatch_size)\n",
    "\n",
    "        progress.close()\n",
    "        finish = datetime.datetime.now()\n",
    "\n",
    "        # print/log output\n",
    "        message = '%s: time=%s, step=%d, loss=%g, accuracy=%g' % (\n",
    "            header,\n",
    "            finish - start,\n",
    "            tf.train.global_step(sess, self._global_step),\n",
    "            cum_loss / cum_num_examples,\n",
    "            cum_correct_examples / cum_num_examples)\n",
    "        print(message)\n",
    "        if log_file:\n",
    "            print(message, file=log_file)\n",
    "            log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = np.load('../data/simplewiki/simplewiki-20171103.el_softmax_4.embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"model/position_embeddings:0\": 2592\n",
      "parameters for \"model/attention/scaling_factor:0\": 1\n",
      "parameters for \"model/attention/input/kernel:0\": 20480\n",
      "parameters for \"model/attention/input/bias:0\": 128\n",
      "parameters for \"model/attention/input/gamma:0\": 128\n",
      "parameters for \"model/attention/input/beta:0\": 128\n",
      "parameters for \"model/attention/layer_0/self/dense/kernel:0\": 16384\n",
      "parameters for \"model/attention/layer_0/self/dense/bias:0\": 128\n",
      "parameters for \"model/attention/layer_0/self/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_0/self/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/attention/layer_1/self/dense/kernel:0\": 16384\n",
      "parameters for \"model/attention/layer_1/self/dense/bias:0\": 128\n",
      "parameters for \"model/attention/layer_1/self/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_1/self/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/attention/layer_2/self/dense/kernel:0\": 16384\n",
      "parameters for \"model/attention/layer_2/self/dense/bias:0\": 128\n",
      "parameters for \"model/attention/layer_2/self/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_2/self/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/attention/layer_3/self/dense/kernel:0\": 16384\n",
      "parameters for \"model/attention/layer_3/self/dense/bias:0\": 128\n",
      "parameters for \"model/attention/layer_3/self/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_3/self/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/attention/output/kernel:0\": 16384\n",
      "parameters for \"model/attention/output/query:0\": 128\n",
      "parameters for \"model/softmax/kernel:0\": 256000\n",
      "parameters for \"model/softmax/bias:0\": 2000\n",
      "total parameters: 365041\n"
     ]
    }
   ],
   "source": [
    "sess = reset_tf(sess)\n",
    "\n",
    "model = EntityLinkingModel(sess, word_embeddings, HyperParameters())\n",
    "model.build_model()\n",
    "dump_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(path):\n",
    "    return sorted([os.path.join(path, file) for file in os.listdir(path)])\n",
    "\n",
    "train_set = list_files('../data/simplewiki/simplewiki-20171103.el_softmax_4.train')\n",
    "dev_set = list_files('../data/simplewiki/simplewiki-20171103.el_softmax_4.dev')\n",
    "test_set = list_files('../data/simplewiki/simplewiki-20171103.el_softmax_4.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7018c4a9a8784a2d9636fa785bed3b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train 0', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train 0: time=0:01:20.586671, step=15536, loss=0.126507, accuracy=0.965295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22b36027c634f08aba0ffed104b75de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev 0', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev 0: time=0:00:00.949344, step=15536, loss=0.146539, accuracy=0.96685\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe2379cd7814c2a9dbcf5fc1f70b35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train 1', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train 1: time=0:01:20.578787, step=16507, loss=0.124153, accuracy=0.965532\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7545a8fb87429492b5a24c543683af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev 1', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev 1: time=0:00:00.951293, step=16507, loss=0.14635, accuracy=0.96795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3115f7a941e84fc2b66a9f820a0732ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train 2', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train 2: time=0:01:20.573286, step=17478, loss=0.12251, accuracy=0.965956\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74133bbbb2384ece8a5ea96b23036c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev 2', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev 2: time=0:00:00.954049, step=17478, loss=0.148622, accuracy=0.9672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca6e91fafd14c44889ce055173a26ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train 3', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train 3: time=0:01:21.092960, step=18449, loss=0.121296, accuracy=0.966059\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb9945d72e84777bd16c17d84fe3916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev 3', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev 3: time=0:00:00.966698, step=18449, loss=0.144493, accuracy=0.9673\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e4dfd5b19f4e0fa62a6449b3a7ccd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train 4', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "train 4: time=0:01:20.037437, step=19420, loss=0.120788, accuracy=0.966158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f33a84046e424fb112860231b1dbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='dev 4', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dev 4: time=0:00:00.948316, step=19420, loss=0.148732, accuracy=0.9674\n"
     ]
    }
   ],
   "source": [
    "with open('../logs/simplewiki/mediawiki_el_softmax_5.log', 'at') as f:\n",
    "    for i in range(5):\n",
    "        model.process(\n",
    "            train_set,\n",
    "            header = 'train %d' % i,\n",
    "            train = True,\n",
    "            log_file = f)\n",
    "        model.process(\n",
    "            dev_set,\n",
    "            header = 'dev %d' % i,\n",
    "            train = False,\n",
    "            log_file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'../models/simplewiki/el_softmax_2/saved_model.pb'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'../models/simplewiki/el_softmax_2/saved_model.pb'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.rmtree(\n",
    "    '../models/simplewiki/el_softmax_5',\n",
    "    ignore_errors = True)\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(\n",
    "    '../models/simplewiki/el_softmax_5')\n",
    "builder.add_meta_graph_and_variables(\n",
    "    sess,\n",
    "    [tf.saved_model.tag_constants.TRAINING])\n",
    "builder.add_meta_graph(\n",
    "    [tf.saved_model.tag_constants.SERVING])\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = reset_tf(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'../models/simplewiki/el_softmax_2/variables/variables'\n"
     ]
    }
   ],
   "source": [
    "_ = tf.saved_model.loader.load(\n",
    "    sess,\n",
    "    [tf.saved_model.tag_constants.TRAINING],\n",
    "    '../models/simplewiki/el_softmax_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"model/word_embeddings:0\": 7680000\n",
      "parameters for \"model/position_embeddings:0\": 1296\n",
      "parameters for \"model/attention_with_ff/input/kernel:0\": 69632\n",
      "parameters for \"model/attention_with_ff/input/bias:0\": 256\n",
      "parameters for \"model/attention_with_ff/input/gamma:0\": 256\n",
      "parameters for \"model/attention_with_ff/input/beta:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_0/attention/projection_weights:0\": 65536\n",
      "parameters for \"model/attention_with_ff/layer_0/attention/output:0\": 65536\n",
      "parameters for \"model/attention_with_ff/layer_0/attention/gamma:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_0/attention/beta:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_0/ff/fc1/kernel:0\": 131072\n",
      "parameters for \"model/attention_with_ff/layer_0/ff/fc1/bias:0\": 512\n",
      "parameters for \"model/attention_with_ff/layer_0/ff/fc2/kernel:0\": 131072\n",
      "parameters for \"model/attention_with_ff/layer_0/ff/fc2/bias:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_0/ff/gamma:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_0/ff/beta:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_1/attention/projection_weights:0\": 65536\n",
      "parameters for \"model/attention_with_ff/layer_1/attention/output:0\": 65536\n",
      "parameters for \"model/attention_with_ff/layer_1/attention/gamma:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_1/attention/beta:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_1/ff/fc1/kernel:0\": 131072\n",
      "parameters for \"model/attention_with_ff/layer_1/ff/fc1/bias:0\": 512\n",
      "parameters for \"model/attention_with_ff/layer_1/ff/fc2/kernel:0\": 131072\n",
      "parameters for \"model/attention_with_ff/layer_1/ff/fc2/bias:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_1/ff/gamma:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_1/ff/beta:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_2/attention/projection_weights:0\": 65536\n",
      "parameters for \"model/attention_with_ff/layer_2/attention/output:0\": 65536\n",
      "parameters for \"model/attention_with_ff/layer_2/attention/gamma:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_2/attention/beta:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_2/ff/fc1/kernel:0\": 131072\n",
      "parameters for \"model/attention_with_ff/layer_2/ff/fc1/bias:0\": 512\n",
      "parameters for \"model/attention_with_ff/layer_2/ff/fc2/kernel:0\": 131072\n",
      "parameters for \"model/attention_with_ff/layer_2/ff/fc2/bias:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_2/ff/gamma:0\": 256\n",
      "parameters for \"model/attention_with_ff/layer_2/ff/beta:0\": 256\n",
      "parameters for \"model/softmax/kernel:0\": 512000\n",
      "parameters for \"model/softmax/bias:0\": 2000\n",
      "total parameters: 9450720\n"
     ]
    }
   ],
   "source": [
    "dump_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iterator = tf.get_default_graph().get_operation_by_name('dataset/MakeIterator')\n",
    "dataset_filenames = tf.get_default_graph().get_tensor_by_name('dataset/dataset_filenames:0')\n",
    "training = tf.get_default_graph().get_tensor_by_name('model/training:0')\n",
    "context_word_ids = tf.get_default_graph().get_tensor_by_name('dataset/context_word_ids:0')\n",
    "link_mask = tf.get_default_graph().get_tensor_by_name('dataset/link_mask:0')\n",
    "target_labels = tf.get_default_graph().get_tensor_by_name('dataset/target_labels:0')\n",
    "output_labels = tf.get_default_graph().get_tensor_by_name('train/ArgMax:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/simplewiki/simplewiki-20171103.el_softmax_4.vocab.txt', 'rt') as f:\n",
    "    vocab = [w.strip() for w in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/simplewiki/simplewiki-20171103.el_softmax_4.targets.txt', 'rt') as f:\n",
    "    targets = [t.strip() for t in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_negative_examples(filenames, limit = None):\n",
    "    # initialize dataset iterator\n",
    "    sess.run(dataset_iterator, feed_dict = {\n",
    "        dataset_filenames: filenames,\n",
    "        training: False })\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    while True:\n",
    "        # compute minibatch\n",
    "        try:\n",
    "            (curr_context_word_ids, curr_target_labels, curr_output_labels, curr_link_mask) = sess.run(\n",
    "                (context_word_ids, target_labels, output_labels, link_mask),\n",
    "                feed_dict = { training: False })\n",
    "        except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "        # loop through examples\n",
    "        for c, t, o, l in zip(curr_context_word_ids, curr_target_labels, curr_output_labels, curr_link_mask):\n",
    "            # skip accurate inferences\n",
    "            if t == o:\n",
    "                continue\n",
    "                \n",
    "            # stop if limit reached\n",
    "            if limit and len(examples) >= limit:\n",
    "                break\n",
    "            \n",
    "            # decode context\n",
    "            words = []\n",
    "            link_words = []\n",
    "            for word_id, link_mask_flag in zip(c, l):\n",
    "                word = vocab[word_id]\n",
    "                if link_mask_flag:\n",
    "                    words.append('_%s_' % word)\n",
    "                    link_words.append(word)\n",
    "                else:\n",
    "                    words.append(word)\n",
    "\n",
    "            # decode example\n",
    "            examples.append([\n",
    "                targets[t], # target label\n",
    "                targets[o], # output label\n",
    "                ' '.join(link_words), # link\n",
    "                ' '.join(words) ]) # context\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = compute_negative_examples(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INHERITANCE',\n",
       " 'HEIR APPARENT',\n",
       " 'heir',\n",
       " '<OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> <OOB> franois michelin ( 15 june 1926 & ndash ; 29 april 2015 ) was a french _heir_ and business executive . he served as the ceo of michelin from 1955 to 1999. michelin was born in clermont - ferrand , auvergne . his grandfather , douard michelin ( 1859 & ndash ; 1940 ) , was the']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/tmp/errors.csv', 'wt') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['target', 'output', 'link', 'context'])\n",
    "    for example in examples:\n",
    "        writer.writerow(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.reshape(tf.range(16), [4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 0,  5,  6,  7],\n",
       "       [ 0,  0, 10, 11],\n",
       "       [ 0,  0,  0, 15]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matrix_band_part(x, 0, -1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 4,  5,  0,  0],\n",
       "       [ 8,  9, 10,  0],\n",
       "       [12, 13, 14, 15]], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matrix_band_part(x, -1, 0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../logs/simplewiki/mediawiki_el_softmax_1.multihead.log.old', 'wt') as f:\n",
    "#     print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../logs/simplewiki/mediawiki_el_softmax_1.multihead.log', 'rt') as f:\n",
    "#     lines = '\\n'.join([l for l in f])\n",
    "    \n",
    "# plt.plot([float(v) for v in re.findall(r'train.*loss=(\\d+\\.\\d+)', lines)], label='train')\n",
    "# plt.plot([float(v) for v in re.findall(r'dev.*loss=(\\d+\\.\\d+)', lines)], label='dev')\n",
    "# plt.title('loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.plot([float(v) for v in re.findall(r'train.*accuracy=(\\d+\\.\\d+)', lines)], label='train')\n",
    "# plt.plot([float(v) for v in re.findall(r'dev.*accuracy=(\\d+\\.\\d+)', lines)], label='dev')\n",
    "# plt.title('accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
