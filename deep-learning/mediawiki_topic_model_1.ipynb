{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import binascii\n",
    "import gzip\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf(sess = None, log_device_placement = False):\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    return tf.InteractiveSession(config = tf.ConfigProto(log_device_placement = log_device_placement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    # adam learning rate\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # number of distinct terms (term indices are expected in 0..range(num_terms))\n",
    "    vocab_size = 30000\n",
    "    \n",
    "    # number of dimensions in hidden layer\n",
    "    hidden_size = 512\n",
    "\n",
    "    # number of dimensions in document embedding\n",
    "    embedding_size = 128\n",
    "    \n",
    "    # dropout rate\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    # number of sequences per batch\n",
    "    pipeline_batch_size = 32\n",
    "    \n",
    "    # number of parsing threads in data pipeline\n",
    "    pipeline_num_parallel_calls = 4\n",
    "    \n",
    "    # size of prefetch in data pipeline\n",
    "    pipeline_prefetch_size = pipeline_batch_size * 16\n",
    "    \n",
    "    # shuffle buffer size\n",
    "    pipeline_shuffle_size = 256\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"input_hidden_layer/kernel:0\": 15360000\n",
      "parameters for \"input_hidden_layer/bias:0\": 512\n",
      "parameters for \"input_embedding_layer/kernel:0\": 131072\n",
      "parameters for \"input_embedding_layer/bias:0\": 256\n",
      "parameters for \"output_hidden_layer/kernel:0\": 131072\n",
      "parameters for \"output_hidden_layer/bias:0\": 512\n",
      "parameters for \"output_layer/dense/kernel:0\": 15360000\n",
      "parameters for \"output_layer/dense/bias:0\": 30000\n",
      "total parameters: 31013424\n"
     ]
    }
   ],
   "source": [
    "sess = reset_tf(sess)\n",
    "\n",
    "# Pipeline\n",
    "# --------\n",
    "\n",
    "# TODO: use SparseTensor / don't use dataset API for speed?\n",
    "\n",
    "def parse_example(example_proto):\n",
    "    features = {\n",
    "        'page_id': tf.FixedLenFeature([1], dtype=tf.string),\n",
    "        'para_id': tf.FixedLenFeature([1], dtype=tf.int64),\n",
    "        'indices': tf.VarLenFeature(tf.int64),\n",
    "        'freqs': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    parsed = tf.parse_single_example(example_proto, features)\n",
    "    page_id = parsed['page_id']\n",
    "    para_id = parsed['para_id']\n",
    "    indices = tf.sparse_tensor_to_dense(parsed['indices'])\n",
    "    freqs = tf.sparse_tensor_to_dense(parsed['freqs'])\n",
    "    return page_id, para_id, tf.cast(tf.sparse_to_dense(indices, [hp.vocab_size], freqs), tf.float32)\n",
    "\n",
    "dataset_filenames = tf.placeholder(tf.string, shape = [None], name = 'dataset_filenames')\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(dataset_filenames)\n",
    "dataset = dataset.map(parse_example,\n",
    "                      num_parallel_calls = hp.pipeline_num_parallel_calls)\n",
    "dataset = dataset.shuffle(hp.pipeline_shuffle_size)\n",
    "dataset = dataset.prefetch(hp.pipeline_prefetch_size)\n",
    "dataset = dataset.batch(hp.pipeline_batch_size)\n",
    "\n",
    "dataset_iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "input_page_id_iter, input_para_id_iter, input_tf_vector_iter = dataset_iterator.get_next()\n",
    "\n",
    "input_page_id = tf.placeholder_with_default(input_page_id_iter, [None, 1], name = 'input_page_id')\n",
    "input_para_id = tf.placeholder_with_default(input_para_id_iter, [None, 1], name = 'input_para_id')\n",
    "input_tf_vector = tf.placeholder_with_default(input_tf_vector_iter, \n",
    "                                              [None, hp.vocab_size],\n",
    "                                              name = 'input_tf_vector')\n",
    "input_tf_vector_count = tf.shape(input_tf_vector)[0]\n",
    "\n",
    "input_tf_vector_norm = tf.reduce_sum(input_tf_vector, axis = -1,  keep_dims = True)\n",
    "input_tf_vector_normalized = input_tf_vector / (input_tf_vector_norm + 1e-8)\n",
    "\n",
    "# Model\n",
    "# -----\n",
    "\n",
    "def layer_dense_with_norm(x, num_units, scope, reuse=None, epsilon=1e-6):\n",
    "    x = tf.layers.dense(x, num_units, activation = tf.nn.relu, name=scope)\n",
    "    return x\n",
    "\n",
    "layer = input_tf_vector_normalized\n",
    "\n",
    "layer = layer_dense_with_norm(layer, hp.hidden_size, 'input_hidden_layer')\n",
    "layer = layer_dense_with_norm(layer, hp.embedding_size, 'input_embedding_layer')\n",
    "layer = layer_dense_with_norm(layer, hp.hidden_size, 'output_hidden_layer')\n",
    "\n",
    "with tf.variable_scope('output_layer'):\n",
    "    output_tf_vector_normalized = tf.layers.dense(layer,hp.vocab_size)\n",
    "\n",
    "# Loss\n",
    "# ----\n",
    "\n",
    "indiv_loss = tf.losses.mean_squared_error(input_tf_vector_normalized,\n",
    "                                          output_tf_vector_normalized,\n",
    "                                          reduction = tf.losses.Reduction.NONE)\n",
    "total_loss = tf.reduce_sum(indiv_loss, name = 'total_loss')\n",
    "mean_loss = tf.div(total_loss, \n",
    "                   (tf.cast(input_tf_vector_count, tf.float32) * hp.vocab_size),\n",
    "                   name = 'mean_loss')\n",
    "\n",
    "# Optimization\n",
    "# ------------\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = hp.learning_rate)\n",
    "train_op = optimizer.minimize(mean_loss)\n",
    "# gradients, variables = zip(*optimizer.compute_gradients(mean_loss))\n",
    "# gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "# train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "# Stats\n",
    "# -----\n",
    "\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print('parameters for \"%s\": %d' % (variable.name, variable_parameters))\n",
    "    total_parameters += variable_parameters\n",
    "print('total parameters: %d' % total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_filename,\n",
    "                     header = 'results',\n",
    "                     train = False,\n",
    "                     show_progress = True):\n",
    "    cum_loss = 0\n",
    "    cum_count = 0\n",
    "\n",
    "    sess.run(dataset_iterator.initializer, feed_dict={\n",
    "        dataset_filenames: [dataset_filename]\n",
    "    })\n",
    "\n",
    "    if show_progress:\n",
    "        progress = tqdm_notebook()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            (_,\n",
    "             curr_loss,\n",
    "             curr_count) = sess.run((train_op if train else [],\n",
    "                                     total_loss,\n",
    "                                     input_tf_vector_count))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "        if show_progress:\n",
    "            progress.update(curr_count)\n",
    "\n",
    "        cum_loss += curr_loss\n",
    "        cum_count += curr_count\n",
    "\n",
    "    if show_progress:\n",
    "        progress.close()\n",
    "\n",
    "    print('%s: loss=%g (%g/%d)' % (header, cum_loss/cum_count, cum_loss, cum_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:27, 2756.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0: loss=0.0455689 (33655.4/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "224it [00:00, 2201.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   0: loss=0.0369412 (738.825/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:27, 2756.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1: loss=0.0342598 (25303/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2345.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   1: loss=0.0321963 (643.926/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:28, 2755.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2: loss=0.0306681 (22650.3/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2354.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   2: loss=0.0297304 (594.608/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:27, 2756.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 3: loss=0.0283451 (20934.6/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2356.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   3: loss=0.027965 (559.299/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:26, 2774.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 4: loss=0.026858 (19836.3/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2334.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   4: loss=0.0269292 (538.584/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2781.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 5: loss=0.0259038 (19131.6/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2354.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   5: loss=0.0262503 (525.006/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2777.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 6: loss=0.0253363 (18712.4/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2361.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   6: loss=0.025709 (514.18/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:26, 2774.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 7: loss=0.0249534 (18429.6/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2332.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   7: loss=0.025424 (508.48/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2784.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 8: loss=0.0246289 (18189.9/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2325.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   8: loss=0.0251662 (503.324/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2783.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 9: loss=0.0243284 (17968.1/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2366.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   9: loss=0.0248962 (497.924/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2778.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 10: loss=0.0240583 (17768.6/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2355.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   10: loss=0.0246107 (492.214/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:26, 2775.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 11: loss=0.0238136 (17587.8/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2374.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   11: loss=0.0243625 (487.25/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:26, 2776.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 12: loss=0.0235875 (17420.8/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2358.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   12: loss=0.024112 (482.239/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2777.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 13: loss=0.0233658 (17257.1/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2311.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   13: loss=0.0238682 (477.363/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2781.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 14: loss=0.0231433 (17092.8/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2326.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   14: loss=0.0236582 (473.165/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:26, 2776.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 15: loss=0.022931 (16936/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2305.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   15: loss=0.023437 (468.74/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2784.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 16: loss=0.0227327 (16789.5/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2392.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   16: loss=0.0232161 (464.323/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2778.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 17: loss=0.0225506 (16655/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2362.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   17: loss=0.0230452 (460.905/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738562it [04:25, 2780.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 18: loss=0.022395 (16540.1/738562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2406.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev   18: loss=0.0229194 (458.388/20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54976it [00:19, 2751.17it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-91c20fff1df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                      \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                      show_progress = True)\n\u001b[0m\u001b[1;32m      6\u001b[0m     evaluate_dataset('../data/simplewiki/simplewiki-20171103.topic_model.30k.dev.tfrecords',\n\u001b[1;32m      7\u001b[0m                      \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dev   %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7c877b866029>\u001b[0m in \u001b[0;36mevaluate_dataset\u001b[0;34m(dataset_filename, header, train, show_progress)\u001b[0m\n\u001b[1;32m     19\u001b[0m              curr_count) = sess.run((train_op if train else [],\n\u001b[1;32m     20\u001b[0m                                      \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                      input_tf_vector_count))\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "55072it [00:30, 2751.17it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    evaluate_dataset('../data/simplewiki/simplewiki-20171103.topic_model.30k.train.tfrecords',\n",
    "                     header = 'train %d' % epoch,\n",
    "                     train = True,\n",
    "                     show_progress = True)\n",
    "    evaluate_dataset('../data/simplewiki/simplewiki-20171103.topic_model.30k.dev.tfrecords',\n",
    "                     header = 'dev   %d' % epoch,\n",
    "                     train = False,\n",
    "                     show_progress = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'../models/simplewiki/topic_model_1_256/saved_model.pb'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'../models/simplewiki/topic_model_1_256/saved_model.pb'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = tf.saved_model.builder.SavedModelBuilder('../models/simplewiki/topic_model_1_128')\n",
    "builder.add_meta_graph_and_variables(sess, ['training'])\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'../models/simplewiki/topic_model_1_256/variables/variables'\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "sess = reset_tf()\n",
    "tf.saved_model.loader.load(sess, ['training'], '../models/simplewiki/topic_model_1_256')\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filenames = tf.get_default_graph().get_operation_by_name('dataset_filenames').outputs[0]\n",
    "embedding_layer = tf.get_default_graph().get_operation_by_name('input_embedding_layer/Relu').outputs[0]\n",
    "input_page_id = tf.get_default_graph().get_operation_by_name('input_page_id').outputs[0]\n",
    "input_para_id = tf.get_default_graph().get_operation_by_name('input_para_id').outputs[0]\n",
    "make_iterator = tf.get_default_graph().get_operation_by_name('MakeIterator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_embeddings(dataset_filename, output_file):\n",
    "    sess.run(make_iterator, feed_dict={\n",
    "        dataset_filenames: [dataset_filename]\n",
    "    })\n",
    "    \n",
    "    progress = tqdm_notebook()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            (curr_input_page_id, \n",
    "             curr_input_para_id, \n",
    "             curr_embedding_layer) = sess.run((input_page_id, input_para_id, embedding_layer))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        for i in range(curr_input_page_id.shape[0]):\n",
    "            json.dump({\n",
    "                'page_id': curr_input_page_id[i][0].hex(), \n",
    "                'para_id': int(curr_input_para_id[i][0]), \n",
    "                'embedding': curr_embedding_layer[i].tolist()\n",
    "            }, output_file)\n",
    "        progress.update(curr_input_page_id.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce515e8960d4d43bc74542ad7bffe4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef30bf464c54472a974badd3304c04b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/achang/anaconda3/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/achang/anaconda3/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a6afae0b7f48c2a82db2782dca2bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gzip.open('../data/simplewiki/simplewiki-20171103.embeddings.30k.json.gz', 'wt', encoding='utf-8') as f:\n",
    "    dump_embeddings('../data/simplewiki/simplewiki-20171103.topic_model.30k.dev.tfrecords', f)\n",
    "    dump_embeddings('../data/simplewiki/simplewiki-20171103.topic_model.30k.test.tfrecords', f)\n",
    "    dump_embeddings('../data/simplewiki/simplewiki-20171103.topic_model.30k.train.tfrecords', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
