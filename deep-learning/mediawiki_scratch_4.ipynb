{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from modules import bpencoding\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from intervaltree import Interval, IntervalTree\n",
    "from tqdm import tqdm\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/simplewiki/simplewiki-20171103.sentences.json.gz', 'rt', encoding='utf8') as f:\n",
    "    sentences = json.load(f)\n",
    "# N.B., globally pre-shuffle data since we'll be streaming it during training,\n",
    "# and will only be able to shuffle within a small lookahead buffer\n",
    "shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/simplewiki/simplewiki-20171103.encoder_table_10k.txt', 'rt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# reserve index 10000 for \"unknown\" token\n",
    "table = [l.strip() for l in lines][:9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = bpencoding.Encoder(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens(tokens, sentence):\n",
    "    point = 0\n",
    "    offsets = []\n",
    "    for token in tokens:\n",
    "        if token == '``' or token == \"''\":\n",
    "            token = '\"'\n",
    "        try:\n",
    "            start = sentence.index(token, point)\n",
    "        except ValueError:\n",
    "            raise ValueError('substring \"{}\" not found in \"{}\"'.format(token, sentence))\n",
    "        point = start + len(token)\n",
    "        offsets.append((start, point))\n",
    "    return offsets\n",
    "\n",
    "def span_tokenize(sentence):\n",
    "    return align_tokens(nltk.word_tokenize(sentence), sentence)\n",
    "\n",
    "def generate_example(sentence, encoder):\n",
    "    links = IntervalTree()\n",
    "    for l in sentence['links']:\n",
    "        links[l['start']:l['finish']] = l['target']\n",
    "\n",
    "    text = sentence['text'].lower()\n",
    "    text = text.replace(\"``\", '\"')\n",
    "    text = text.replace(\"''\", '\"')\n",
    "    inputs = []\n",
    "    word_endings = []\n",
    "    targets = []\n",
    "    \n",
    "    for s in span_tokenize(text): \n",
    "        offset = s[0]\n",
    "        word = text[s[0]:s[1]]\n",
    "        wfs = encoder.encode(word)\n",
    "        for i, wf in enumerate(wfs):\n",
    "            start = offset\n",
    "            end = offset + len(wf.text)\n",
    "            inputs.append(wf.index)\n",
    "            word_endings.append(int(i == len(wfs)-1))\n",
    "            targets.append(int(bool(links[start:end])))\n",
    "            offset = end\n",
    "            \n",
    "    return tf.train.Example(features = tf.train.Features(feature = {\n",
    "        'inputs': tf.train.Feature(int64_list = tf.train.Int64List(value = inputs)),\n",
    "        'word_endings': tf.train.Feature(int64_list = tf.train.Int64List(value = word_endings)),\n",
    "        'targets': tf.train.Feature(int64_list = tf.train.Int64List(value = targets)),\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = sentences[50000:]\n",
    "test_sentences = sentences[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecords(sentences, file):\n",
    "    with tf.python_io.TFRecordWriter(file) as writer:\n",
    "        for s in tqdm(sentences):\n",
    "            example = generate_example(s, encoder)\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1045155/1045155 [10:34<00:00, 1647.44it/s]\n",
      "100%|██████████| 50000/50000 [00:29<00:00, 1680.80it/s]\n"
     ]
    }
   ],
   "source": [
    "write_tfrecords(train_sentences, '../data/simplewiki/simplewiki-20171103.entity_recognition.train.tfrecords')\n",
    "write_tfrecords(test_sentences, '../data/simplewiki/simplewiki-20171103.entity_recognition.test.tfrecords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
