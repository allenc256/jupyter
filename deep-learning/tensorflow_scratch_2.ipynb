{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import urllib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_tf():\n",
    "    global sess\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "n_h = 64\n",
    "n_steps = 20\n",
    "n_x = 256\n",
    "\n",
    "X_raw = tf.placeholder(tf.int32, [None, n_steps], name='X_raw')\n",
    "Y_raw = tf.placeholder(tf.int32, [None, n_steps], name='Y_raw')\n",
    "H_init = tf.placeholder(tf.float32, [None, n_h], name='H_init')\n",
    "\n",
    "W_xh = tf.get_variable(\"W_xh\", shape=[n_x, n_h])\n",
    "W_hh = tf.get_variable(\"W_hh\", shape=[n_h, n_h])\n",
    "W_ha = tf.get_variable(\"W_ha\", shape=[n_h, n_x])\n",
    "b_h = tf.get_variable(\"b_h\", shape=[1, n_h])\n",
    "b_a = tf.get_variable(\"b_a\", shape=[1, n_x])\n",
    "\n",
    "X   = [None] + tf.unstack(tf.one_hot(X_raw, n_x), axis=1)\n",
    "Y   = [None] + tf.unstack(tf.one_hot(Y_raw, n_x), axis=1)\n",
    "H   = [H_init] + [None] * n_steps\n",
    "Z_h = [None] * (n_steps+1)\n",
    "Z_a = [None] * (n_steps+1)\n",
    "A   = [None] * (n_steps+1)\n",
    "\n",
    "for i in range(1, n_steps + 1):\n",
    "    Z_h[i] = tf.matmul(X[i], W_xh) + tf.matmul(H[i - 1], W_hh) + b_h\n",
    "    H[i]   = tf.tanh(Z_h[i])\n",
    "    Z_a[i] = tf.matmul(H[i], W_ha) + b_a\n",
    "    A[i]   = tf.nn.softmax(Z_a[i])\n",
    "    \n",
    "loss = tf.constant(0.0)\n",
    "for i in range(1, n_steps):\n",
    "    loss += tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=Y[i], logits=Z_a[i]))\n",
    "loss /= tf.cast(tf.size(X_raw), tf.float32)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(gens, n_steps):\n",
    "    return np.array([[next(gen) for _ in range(n_steps)] for gen in gens])\n",
    "    \n",
    "def generate_repeating(n_x, offset = 0):\n",
    "    i = offset\n",
    "    while True:\n",
    "        yield i % n_x\n",
    "        i += 1\n",
    "\n",
    "def generate_repeating_from_text(text, offset = 0):\n",
    "    i = offset\n",
    "    while True:\n",
    "        yield ord(text[i % len(text)])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = [random.randrange(len(text)) for _ in range(64)]\n",
    "X_gens = [generate_repeating_from_text(text, offset) for offset in offsets]\n",
    "Y_gens = [generate_repeating_from_text(text, offset + 1) for offset in offsets]\n",
    "\n",
    "H_curr = np.random.randn(len(X_gens), n_h)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 4.781926155090332\n",
      "iteration: 100, loss: 2.099501132965088\n",
      "iteration: 200, loss: 1.8674484491348267\n",
      "iteration: 300, loss: 1.6519635915756226\n",
      "iteration: 400, loss: 1.5003186464309692\n",
      "iteration: 500, loss: 1.4926798343658447\n",
      "iteration: 600, loss: 1.3521798849105835\n",
      "iteration: 700, loss: 1.2380061149597168\n",
      "iteration: 800, loss: 1.2008930444717407\n",
      "iteration: 900, loss: 1.131629228591919\n",
      "iteration: 1000, loss: 1.0302391052246094\n",
      "iteration: 1100, loss: 1.0959913730621338\n",
      "iteration: 1200, loss: 0.9784004092216492\n",
      "iteration: 1300, loss: 0.9142093658447266\n",
      "iteration: 1400, loss: 0.9143632650375366\n",
      "iteration: 1500, loss: 0.8206866979598999\n",
      "iteration: 1600, loss: 0.8921475410461426\n",
      "iteration: 1700, loss: 0.7915443778038025\n",
      "iteration: 1800, loss: 0.7742353677749634\n",
      "iteration: 1900, loss: 0.787238597869873\n",
      "iteration: 2000, loss: 0.8067941665649414\n",
      "iteration: 2100, loss: 0.8449389338493347\n",
      "iteration: 2200, loss: 0.7789803147315979\n",
      "iteration: 2300, loss: 0.7443720102310181\n",
      "iteration: 2400, loss: 0.765365481376648\n",
      "iteration: 2500, loss: 0.7141337990760803\n",
      "iteration: 2600, loss: 0.6837679147720337\n",
      "iteration: 2700, loss: 0.6567438840866089\n",
      "iteration: 2800, loss: 0.7586601972579956\n",
      "iteration: 2900, loss: 0.7308756113052368\n",
      "iteration: 3000, loss: 0.6846997141838074\n",
      "iteration: 3100, loss: 0.6252380013465881\n",
      "iteration: 3200, loss: 0.6741737723350525\n",
      "iteration: 3300, loss: 0.6290110349655151\n",
      "iteration: 3400, loss: 0.7074998021125793\n",
      "iteration: 3500, loss: 0.7050379514694214\n",
      "iteration: 3600, loss: 0.6901246309280396\n",
      "iteration: 3700, loss: 0.6815789937973022\n",
      "iteration: 3800, loss: 0.6095955967903137\n",
      "iteration: 3900, loss: 0.5656923651695251\n",
      "iteration: 4000, loss: 0.689541220664978\n",
      "iteration: 4100, loss: 0.6076543927192688\n",
      "iteration: 4200, loss: 0.5766063928604126\n",
      "iteration: 4300, loss: 0.5716530084609985\n",
      "iteration: 4400, loss: 0.6131499409675598\n",
      "iteration: 4500, loss: 0.5886737108230591\n",
      "iteration: 4600, loss: 0.604130744934082\n",
      "iteration: 4700, loss: 0.6012402772903442\n",
      "iteration: 4800, loss: 0.655392050743103\n",
      "iteration: 4900, loss: 0.6028033494949341\n",
      "iteration: 5000, loss: 0.58804851770401\n",
      "iteration: 5100, loss: 0.569642186164856\n",
      "iteration: 5200, loss: 0.592659056186676\n",
      "iteration: 5300, loss: 0.6128838062286377\n",
      "iteration: 5400, loss: 0.6807475686073303\n",
      "iteration: 5500, loss: 0.5486311316490173\n",
      "iteration: 5600, loss: 0.5807039141654968\n",
      "iteration: 5700, loss: 0.658243715763092\n",
      "iteration: 5800, loss: 0.5516218543052673\n",
      "iteration: 5900, loss: 0.6063657402992249\n",
      "iteration: 6000, loss: 0.5387805104255676\n",
      "iteration: 6100, loss: 0.597474992275238\n",
      "iteration: 6200, loss: 0.5789389610290527\n",
      "iteration: 6300, loss: 0.570822536945343\n",
      "iteration: 6400, loss: 0.4992292821407318\n",
      "iteration: 6500, loss: 0.6958200931549072\n",
      "iteration: 6600, loss: 0.5594351291656494\n",
      "iteration: 6700, loss: 0.5678610801696777\n",
      "iteration: 6800, loss: 0.5735005140304565\n",
      "iteration: 6900, loss: 0.5555117130279541\n",
      "iteration: 7000, loss: 0.511933445930481\n",
      "iteration: 7100, loss: 0.6229256391525269\n",
      "iteration: 7200, loss: 0.5091339349746704\n",
      "iteration: 7300, loss: 0.6312046051025391\n",
      "iteration: 7400, loss: 0.5273138284683228\n",
      "iteration: 7500, loss: 0.5038276314735413\n",
      "iteration: 7600, loss: 0.543128490447998\n",
      "iteration: 7700, loss: 0.4908977448940277\n",
      "iteration: 7800, loss: 0.5008296370506287\n",
      "iteration: 7900, loss: 0.6409927606582642\n",
      "iteration: 8000, loss: 0.521678626537323\n",
      "iteration: 8100, loss: 0.5172653794288635\n",
      "iteration: 8200, loss: 0.5850578546524048\n",
      "iteration: 8300, loss: 0.5418566465377808\n",
      "iteration: 8400, loss: 0.6599088907241821\n",
      "iteration: 8500, loss: 0.5144447088241577\n",
      "iteration: 8600, loss: 0.5776159763336182\n",
      "iteration: 8700, loss: 0.49464672803878784\n",
      "iteration: 8800, loss: 0.5829954147338867\n",
      "iteration: 8900, loss: 0.4959731698036194\n",
      "iteration: 9000, loss: 0.739521861076355\n",
      "iteration: 9100, loss: 0.6070882081985474\n",
      "iteration: 9200, loss: 0.5352514386177063\n",
      "iteration: 9300, loss: 0.5026019215583801\n",
      "iteration: 9400, loss: 0.5744913816452026\n",
      "iteration: 9500, loss: 0.5507921576499939\n",
      "iteration: 9600, loss: 0.5276639461517334\n",
      "iteration: 9700, loss: 0.5004574060440063\n",
      "iteration: 9800, loss: 0.4335026741027832\n",
      "iteration: 9900, loss: 0.6373697519302368\n",
      "iteration: 10000, loss: 0.48204565048217773\n",
      "iteration: 10100, loss: 0.6792423725128174\n",
      "iteration: 10200, loss: 0.5566449761390686\n",
      "iteration: 10300, loss: 0.5357431173324585\n",
      "iteration: 10400, loss: 0.5768483877182007\n",
      "iteration: 10500, loss: 0.6097570657730103\n",
      "iteration: 10600, loss: 0.5233801603317261\n",
      "iteration: 10700, loss: 0.5201022624969482\n",
      "iteration: 10800, loss: 0.5671277046203613\n",
      "iteration: 10900, loss: 0.5271373987197876\n",
      "iteration: 11000, loss: 0.49173521995544434\n",
      "iteration: 11100, loss: 0.5168585777282715\n",
      "iteration: 11200, loss: 0.5531899333000183\n",
      "iteration: 11300, loss: 0.540237545967102\n",
      "iteration: 11400, loss: 0.517534077167511\n",
      "iteration: 11500, loss: 0.7302182912826538\n",
      "iteration: 11600, loss: 0.5398608446121216\n",
      "iteration: 11700, loss: 0.5057103633880615\n",
      "iteration: 11800, loss: 0.43625059723854065\n",
      "iteration: 11900, loss: 0.8126562237739563\n",
      "iteration: 12000, loss: 0.4968551695346832\n",
      "iteration: 12100, loss: 0.4412824213504791\n",
      "iteration: 12200, loss: 0.4401888847351074\n",
      "iteration: 12300, loss: 0.5034292340278625\n",
      "iteration: 12400, loss: 0.6785183548927307\n",
      "iteration: 12500, loss: 0.5180819034576416\n",
      "iteration: 12600, loss: 0.44890132546424866\n",
      "iteration: 12700, loss: 0.5580884218215942\n",
      "iteration: 12800, loss: 0.500971257686615\n",
      "iteration: 12900, loss: 0.4922912120819092\n",
      "iteration: 13000, loss: 0.4248908460140228\n",
      "iteration: 13100, loss: 0.5485361814498901\n",
      "iteration: 13200, loss: 0.5469520688056946\n",
      "iteration: 13300, loss: 0.4560111463069916\n",
      "iteration: 13400, loss: 0.6412785649299622\n",
      "iteration: 13500, loss: 0.45921435952186584\n",
      "iteration: 13600, loss: 0.4721967279911041\n",
      "iteration: 13700, loss: 0.5815333127975464\n",
      "iteration: 13800, loss: 0.7732227444648743\n",
      "iteration: 13900, loss: 0.5570533871650696\n",
      "iteration: 14000, loss: 0.47101593017578125\n",
      "iteration: 14100, loss: 0.43290939927101135\n",
      "iteration: 14200, loss: 0.45082443952560425\n",
      "iteration: 14300, loss: 0.5946723222732544\n",
      "iteration: 14400, loss: 0.4144807457923889\n",
      "iteration: 14500, loss: 0.45750847458839417\n",
      "iteration: 14600, loss: 0.5203075408935547\n",
      "iteration: 14700, loss: 0.559834897518158\n",
      "iteration: 14800, loss: 0.48262009024620056\n",
      "iteration: 14900, loss: 0.5658590793609619\n",
      "iteration: 15000, loss: 0.5026646852493286\n",
      "iteration: 15100, loss: 0.5004583597183228\n",
      "iteration: 15200, loss: 0.4568583369255066\n",
      "iteration: 15300, loss: 0.6401468515396118\n",
      "iteration: 15400, loss: 0.6769067645072937\n",
      "iteration: 15500, loss: 0.42170295119285583\n",
      "iteration: 15600, loss: 0.4212125241756439\n",
      "iteration: 15700, loss: 0.42273783683776855\n",
      "iteration: 15800, loss: 0.7796135544776917\n",
      "iteration: 15900, loss: 0.5080634355545044\n",
      "iteration: 16000, loss: 0.5118778944015503\n",
      "iteration: 16100, loss: 0.5451160669326782\n",
      "iteration: 16200, loss: 0.45728546380996704\n",
      "iteration: 16300, loss: 0.5320779085159302\n",
      "iteration: 16400, loss: 0.6208551526069641\n",
      "iteration: 16500, loss: 0.4214983582496643\n",
      "iteration: 16600, loss: 0.4678369462490082\n",
      "iteration: 16700, loss: 0.44601696729660034\n",
      "iteration: 16800, loss: 0.5363841652870178\n",
      "iteration: 16900, loss: 0.48244038224220276\n",
      "iteration: 17000, loss: 0.4601864814758301\n",
      "iteration: 17100, loss: 0.576688826084137\n",
      "iteration: 17200, loss: 0.4878760278224945\n",
      "iteration: 17300, loss: 0.5260156393051147\n",
      "iteration: 17400, loss: 0.4530750811100006\n",
      "iteration: 17500, loss: 0.5691310167312622\n",
      "iteration: 17600, loss: 0.5414003133773804\n",
      "iteration: 17700, loss: 0.5246374011039734\n",
      "iteration: 17800, loss: 0.46137309074401855\n",
      "iteration: 17900, loss: 0.5115764737129211\n",
      "iteration: 18000, loss: 0.4579700827598572\n",
      "iteration: 18100, loss: 0.8132818341255188\n",
      "iteration: 18200, loss: 0.49832862615585327\n",
      "iteration: 18300, loss: 0.48479634523391724\n",
      "iteration: 18400, loss: 0.5108305811882019\n",
      "iteration: 18500, loss: 0.5824370980262756\n",
      "iteration: 18600, loss: 0.47947829961776733\n",
      "iteration: 18700, loss: 0.43877631425857544\n",
      "iteration: 18800, loss: 0.5466118454933167\n",
      "iteration: 18900, loss: 0.4528472423553467\n",
      "iteration: 19000, loss: 0.5236613750457764\n",
      "iteration: 19100, loss: 0.5132553577423096\n",
      "iteration: 19200, loss: 0.38973158597946167\n",
      "iteration: 19300, loss: 0.3827592432498932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 19400, loss: 0.5876443386077881\n",
      "iteration: 19500, loss: 0.44702666997909546\n",
      "iteration: 19600, loss: 0.5731972455978394\n",
      "iteration: 19700, loss: 0.6051748991012573\n",
      "iteration: 19800, loss: 0.4877173900604248\n",
      "iteration: 19900, loss: 0.4617454409599304\n",
      "iteration: 20000, loss: 0.6603344678878784\n",
      "iteration: 20100, loss: 0.5008080005645752\n",
      "iteration: 20200, loss: 0.4053410589694977\n",
      "iteration: 20300, loss: 0.5765550136566162\n",
      "iteration: 20400, loss: 0.5283342003822327\n",
      "iteration: 20500, loss: 0.6232984662055969\n",
      "iteration: 20600, loss: 0.450822651386261\n",
      "iteration: 20700, loss: 0.4552507996559143\n",
      "iteration: 20800, loss: 0.5929408669471741\n",
      "iteration: 20900, loss: 0.5004450082778931\n",
      "iteration: 21000, loss: 0.5066055059432983\n",
      "iteration: 21100, loss: 0.48326197266578674\n",
      "iteration: 21200, loss: 0.39515435695648193\n",
      "iteration: 21300, loss: 0.48623543977737427\n",
      "iteration: 21400, loss: 0.664553165435791\n",
      "iteration: 21500, loss: 0.6491064429283142\n",
      "iteration: 21600, loss: 0.4922238290309906\n",
      "iteration: 21700, loss: 0.41677728295326233\n",
      "iteration: 21800, loss: 0.7131230235099792\n",
      "iteration: 21900, loss: 0.6001817584037781\n",
      "iteration: 22000, loss: 0.4394489824771881\n",
      "iteration: 22100, loss: 0.4382880628108978\n",
      "iteration: 22200, loss: 0.5288820266723633\n",
      "iteration: 22300, loss: 0.4495796263217926\n",
      "iteration: 22400, loss: 0.39416566491127014\n",
      "iteration: 22500, loss: 0.5565556287765503\n",
      "iteration: 22600, loss: 0.6461125612258911\n",
      "iteration: 22700, loss: 0.43465733528137207\n",
      "iteration: 22800, loss: 0.42609506845474243\n",
      "iteration: 22900, loss: 0.45225557684898376\n",
      "iteration: 23000, loss: 0.4583507180213928\n",
      "iteration: 23100, loss: 0.5122644901275635\n",
      "iteration: 23200, loss: 0.6060304641723633\n",
      "iteration: 23300, loss: 0.7934958338737488\n",
      "iteration: 23400, loss: 0.5817463994026184\n",
      "iteration: 23500, loss: 0.38622888922691345\n",
      "iteration: 23600, loss: 0.602699875831604\n",
      "iteration: 23700, loss: 0.4716918468475342\n",
      "iteration: 23800, loss: 0.5408169627189636\n",
      "iteration: 23900, loss: 0.5311876535415649\n",
      "iteration: 24000, loss: 0.4561530649662018\n",
      "iteration: 24100, loss: 0.462563693523407\n",
      "iteration: 24200, loss: 0.7339125871658325\n",
      "iteration: 24300, loss: 0.5323189496994019\n",
      "iteration: 24400, loss: 0.46163421869277954\n",
      "iteration: 24500, loss: 0.4836270213127136\n",
      "iteration: 24600, loss: 0.5011119246482849\n",
      "iteration: 24700, loss: 0.5315963625907898\n",
      "iteration: 24800, loss: 1.2390027046203613\n",
      "iteration: 24900, loss: 0.6157609820365906\n",
      "iteration: 25000, loss: 0.47641974687576294\n",
      "iteration: 25100, loss: 0.46336811780929565\n",
      "iteration: 25200, loss: 0.6125245690345764\n",
      "iteration: 25300, loss: 0.5095725655555725\n",
      "iteration: 25400, loss: 0.5563305616378784\n",
      "iteration: 25500, loss: 0.5730486512184143\n",
      "iteration: 25600, loss: 0.4728460907936096\n",
      "iteration: 25700, loss: 0.4601842761039734\n",
      "iteration: 25800, loss: 0.4519537091255188\n",
      "iteration: 25900, loss: 0.6675416231155396\n",
      "iteration: 26000, loss: 0.5223729610443115\n",
      "iteration: 26100, loss: 0.4495998024940491\n",
      "iteration: 26200, loss: 0.4341956079006195\n",
      "iteration: 26300, loss: 0.7415637373924255\n",
      "iteration: 26400, loss: 0.6952511072158813\n",
      "iteration: 26500, loss: 0.41764459013938904\n",
      "iteration: 26600, loss: 0.5549368858337402\n",
      "iteration: 26700, loss: 0.5385985374450684\n",
      "iteration: 26800, loss: 0.6797789931297302\n",
      "iteration: 26900, loss: 0.5309743881225586\n",
      "iteration: 27000, loss: 0.3894374668598175\n",
      "iteration: 27100, loss: 0.5189653038978577\n",
      "iteration: 27200, loss: 0.5328916907310486\n",
      "iteration: 27300, loss: 0.5369504690170288\n",
      "iteration: 27400, loss: 0.4494559168815613\n",
      "iteration: 27500, loss: 0.5794796347618103\n",
      "iteration: 27600, loss: 0.6171417236328125\n",
      "iteration: 27700, loss: 0.482954740524292\n",
      "iteration: 27800, loss: 0.5112314820289612\n",
      "iteration: 27900, loss: 0.5101596713066101\n",
      "iteration: 28000, loss: 0.5823938250541687\n",
      "iteration: 28100, loss: 0.44808751344680786\n",
      "iteration: 28200, loss: 0.5489913821220398\n",
      "iteration: 28300, loss: 0.3742355704307556\n",
      "iteration: 28400, loss: 0.4793442189693451\n",
      "iteration: 28500, loss: 0.5953676104545593\n",
      "iteration: 28600, loss: 0.7358953356742859\n",
      "iteration: 28700, loss: 0.4411105513572693\n",
      "iteration: 28800, loss: 0.4471246600151062\n",
      "iteration: 28900, loss: 0.45525264739990234\n",
      "iteration: 29000, loss: 0.8708561658859253\n",
      "iteration: 29100, loss: 0.5495399236679077\n",
      "iteration: 29200, loss: 0.3998531699180603\n",
      "iteration: 29300, loss: 0.46943673491477966\n",
      "iteration: 29400, loss: 0.4798871576786041\n",
      "iteration: 29500, loss: 0.5497862100601196\n",
      "iteration: 29600, loss: 0.5193718075752258\n",
      "iteration: 29700, loss: 0.5365885496139526\n",
      "iteration: 29800, loss: 0.4597460627555847\n",
      "iteration: 29900, loss: 0.6088764071464539\n",
      "iteration: 30000, loss: 0.6006290912628174\n",
      "iteration: 30100, loss: 0.44208377599716187\n",
      "iteration: 30200, loss: 0.39817509055137634\n",
      "iteration: 30300, loss: 0.49084487557411194\n",
      "iteration: 30400, loss: 0.797073245048523\n",
      "iteration: 30500, loss: 0.41934505105018616\n",
      "iteration: 30600, loss: 0.6877490282058716\n",
      "iteration: 30700, loss: 0.5742601156234741\n",
      "iteration: 30800, loss: 0.4985877573490143\n",
      "iteration: 30900, loss: 0.4654470384120941\n",
      "iteration: 31000, loss: 0.4117949604988098\n",
      "iteration: 31100, loss: 0.7545770406723022\n",
      "iteration: 31200, loss: 0.6648106575012207\n",
      "iteration: 31300, loss: 0.46485406160354614\n",
      "iteration: 31400, loss: 0.44901618361473083\n",
      "iteration: 31500, loss: 0.4985085427761078\n",
      "iteration: 31600, loss: 0.6563810110092163\n",
      "iteration: 31700, loss: 0.7332376837730408\n",
      "iteration: 31800, loss: 0.5708631277084351\n",
      "iteration: 31900, loss: 0.425348699092865\n",
      "iteration: 32000, loss: 0.45489901304244995\n",
      "iteration: 32100, loss: 0.47494158148765564\n",
      "iteration: 32200, loss: 0.5116661787033081\n",
      "iteration: 32300, loss: 0.4719722270965576\n",
      "iteration: 32400, loss: 0.8857994079589844\n",
      "iteration: 32500, loss: 0.5746095180511475\n",
      "iteration: 32600, loss: 0.47751036286354065\n",
      "iteration: 32700, loss: 0.4682394564151764\n",
      "iteration: 32800, loss: 0.611891508102417\n",
      "iteration: 32900, loss: 0.49613529443740845\n",
      "iteration: 33000, loss: 0.5438101887702942\n",
      "iteration: 33100, loss: 0.5103812217712402\n",
      "iteration: 33200, loss: 0.4654879570007324\n",
      "iteration: 33300, loss: 0.6783443093299866\n",
      "iteration: 33400, loss: 0.6162958741188049\n",
      "iteration: 33500, loss: 0.5800982713699341\n",
      "iteration: 33600, loss: 0.45087018609046936\n",
      "iteration: 33700, loss: 0.548130452632904\n",
      "iteration: 33800, loss: 0.4421062469482422\n",
      "iteration: 33900, loss: 0.7959990501403809\n",
      "iteration: 34000, loss: 0.4877393841743469\n",
      "iteration: 34100, loss: 0.40872645378112793\n",
      "iteration: 34200, loss: 0.406668096780777\n",
      "iteration: 34300, loss: 0.7115837335586548\n",
      "iteration: 34400, loss: 0.48122698068618774\n",
      "iteration: 34500, loss: 0.500883936882019\n",
      "iteration: 34600, loss: 0.6636995673179626\n",
      "iteration: 34700, loss: 0.6260914206504822\n",
      "iteration: 34800, loss: 0.710830569267273\n",
      "iteration: 34900, loss: 0.5719839334487915\n",
      "iteration: 35000, loss: 0.48158326745033264\n",
      "iteration: 35100, loss: 0.4847520887851715\n",
      "iteration: 35200, loss: 0.8981834650039673\n",
      "iteration: 35300, loss: 0.9053190350532532\n",
      "iteration: 35400, loss: 0.5631712675094604\n",
      "iteration: 35500, loss: 0.48158007860183716\n",
      "iteration: 35600, loss: 0.4663633704185486\n",
      "iteration: 35700, loss: 0.4257975220680237\n",
      "iteration: 35800, loss: 0.7685796022415161\n",
      "iteration: 35900, loss: 0.6713685989379883\n",
      "iteration: 36000, loss: 0.4794909358024597\n",
      "iteration: 36100, loss: 0.47812527418136597\n",
      "iteration: 36200, loss: 0.4300144612789154\n",
      "iteration: 36300, loss: 0.7251588106155396\n",
      "iteration: 36400, loss: 0.5872324109077454\n",
      "iteration: 36500, loss: 0.5173901319503784\n",
      "iteration: 36600, loss: 0.44786524772644043\n",
      "iteration: 36700, loss: 0.522722065448761\n",
      "iteration: 36800, loss: 0.6628305912017822\n",
      "iteration: 36900, loss: 0.6232245564460754\n",
      "iteration: 37000, loss: 0.4414598345756531\n",
      "iteration: 37100, loss: 0.4165760576725006\n",
      "iteration: 37200, loss: 0.7590066194534302\n",
      "iteration: 37300, loss: 0.7054797410964966\n",
      "iteration: 37400, loss: 0.4986851215362549\n",
      "iteration: 37500, loss: 0.46793919801712036\n",
      "iteration: 37600, loss: 0.6535687446594238\n",
      "iteration: 37700, loss: 0.5307988524436951\n",
      "iteration: 37800, loss: 0.5817204713821411\n",
      "iteration: 37900, loss: 0.47831177711486816\n",
      "iteration: 38000, loss: 0.4742487370967865\n",
      "iteration: 38100, loss: 0.49419984221458435\n",
      "iteration: 38200, loss: 0.5280882716178894\n",
      "iteration: 38300, loss: 0.8021496534347534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 38400, loss: 0.5467836856842041\n",
      "iteration: 38500, loss: 0.48591312766075134\n",
      "iteration: 38600, loss: 0.42390093207359314\n",
      "iteration: 38700, loss: 1.1557247638702393\n",
      "iteration: 38800, loss: 0.6767517924308777\n",
      "iteration: 38900, loss: 0.6495878100395203\n",
      "iteration: 39000, loss: 0.5635347962379456\n",
      "iteration: 39100, loss: 0.3988146185874939\n",
      "iteration: 39200, loss: 0.45693239569664\n",
      "iteration: 39300, loss: 0.5794999003410339\n",
      "iteration: 39400, loss: 0.5743770003318787\n",
      "iteration: 39500, loss: 0.5441269874572754\n",
      "iteration: 39600, loss: 0.6777375936508179\n",
      "iteration: 39700, loss: 0.530519425868988\n",
      "iteration: 39800, loss: 0.6580115556716919\n",
      "iteration: 39900, loss: 0.5708794593811035\n",
      "iteration: 40000, loss: 0.45716363191604614\n",
      "iteration: 40100, loss: 0.5350274443626404\n",
      "iteration: 40200, loss: 1.4171885251998901\n",
      "iteration: 40300, loss: 1.049281120300293\n",
      "iteration: 40400, loss: 0.9267711639404297\n",
      "iteration: 40500, loss: 0.8382158279418945\n",
      "iteration: 40600, loss: 0.7350532412528992\n",
      "iteration: 40700, loss: 0.8278180956840515\n",
      "iteration: 40800, loss: 0.634922981262207\n",
      "iteration: 40900, loss: 0.750475287437439\n",
      "iteration: 41000, loss: 0.7260328531265259\n",
      "iteration: 41100, loss: 0.6620110273361206\n",
      "iteration: 41200, loss: 0.6815064549446106\n",
      "iteration: 41300, loss: 0.6048138737678528\n",
      "iteration: 41400, loss: 0.6434544920921326\n",
      "iteration: 41500, loss: 0.6874512434005737\n",
      "iteration: 41600, loss: 0.6663052439689636\n",
      "iteration: 41700, loss: 0.6272121071815491\n",
      "iteration: 41800, loss: 0.5754601955413818\n",
      "iteration: 41900, loss: 0.569769024848938\n",
      "iteration: 42000, loss: 0.6815966367721558\n",
      "iteration: 42100, loss: 0.5741773247718811\n",
      "iteration: 42200, loss: 0.568576455116272\n",
      "iteration: 42300, loss: 0.6326597929000854\n",
      "iteration: 42400, loss: 0.6087584495544434\n",
      "iteration: 42500, loss: 0.6558419466018677\n",
      "iteration: 42600, loss: 0.6398367881774902\n",
      "iteration: 42700, loss: 0.5671341419219971\n",
      "iteration: 42800, loss: 0.6206651926040649\n",
      "iteration: 42900, loss: 0.6023343801498413\n",
      "iteration: 43000, loss: 0.6477958559989929\n",
      "iteration: 43100, loss: 0.5628291368484497\n",
      "iteration: 43200, loss: 0.5742329359054565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-34edd70afa4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mH_init\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mH_curr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     }\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mH_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iters = 50000\n",
    "\n",
    "for i in range(n_iters):\n",
    "    X_batch = next_batch(X_gens, n_steps)\n",
    "    Y_batch = next_batch(Y_gens, n_steps)\n",
    "    feed_dict = {\n",
    "        X_raw: X_batch,\n",
    "        Y_raw: Y_batch,\n",
    "        H_init: H_curr\n",
    "    }\n",
    "    sess.run(train_step, feed_dict)\n",
    "    H_curr = sess.run(H[-1], feed_dict)\n",
    "    if i % 100 == 0:\n",
    "        print(f'\\riteration: {i}, loss: {sess.run(loss, feed_dict)}',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x_curr, H_curr):\n",
    "    feed_dict = {\n",
    "        X_raw: [[x_curr] * n_steps],\n",
    "        H_init: H_curr.reshape(1, n_h)\n",
    "    }\n",
    "    H_new, p = sess.run((H[1], A[1][0]), feed_dict)\n",
    "    x_new = np.random.choice(len(p), p = p)\n",
    "    return x_new, H_new\n",
    "\n",
    "def generate_samples(x_curr, H_curr):\n",
    "    while True:\n",
    "        yield x_curr\n",
    "        x_curr, H_curr = sample(x_curr, H_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00ct. \\r\\n\\r\\n    Mr. Rabbit heard there. Mas?!hoodes, rale, batily wanked nhedly digging, he begancerne.'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = generate_samples(0, H_curr[0])\n",
    "\n",
    "''.join([chr(next(g)) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = None\n",
    "with urllib.request.urlopen('http://textfiles.com/stories/13chil.txt') as response:\n",
    "    text = response.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the sun did not shine.\\nit was too wet to play.\\nso we sat in the house\\nall that cold, cold, wet day.\\n\\ni sat there with sally.\\nwe sat there, we two.\\nand i said, 'how i wish\\nwe had something to do!'\\n\\ntoo wet to go out\\nand too cold to play ball.\\nso we sat in the house.\\nwe did nothing at all.\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"the sun did not shine.\n",
    "it was too wet to play.\n",
    "so we sat in the house\n",
    "all that cold, cold, wet day.\n",
    "\n",
    "i sat there with sally.\n",
    "we sat there, we two.\n",
    "and i said, 'how i wish\n",
    "we had something to do!'\n",
    "\n",
    "too wet to go out\n",
    "and too cold to play ball.\n",
    "so we sat in the house.\n",
    "we did nothing at all.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8457"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
