{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import gzip\n",
    "import json\n",
    "import datetime\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf():\n",
    "    global sess\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    # maximum number of symbols in an input sequence\n",
    "    max_sequence_length = 50\n",
    "    \n",
    "    # number of symbols in vocabulary\n",
    "    # (symbols are expected to be in range(vocab_size))\n",
    "    vocab_size = 10000\n",
    "\n",
    "    # number of dimensions in input embeddings\n",
    "    embedding_size = 256\n",
    "    \n",
    "    # number of sequences per batch\n",
    "    batch_size = 128\n",
    "    \n",
    "    # number of target classes\n",
    "    num_target_classes = 2\n",
    "    \n",
    "    # number of parsing threads in data pipeline\n",
    "    dataset_pipeline_parallel_calls = 4\n",
    "    \n",
    "    # size of prefetch in data pipeline\n",
    "    dataset_pipeline_prefetch = batch_size * 16\n",
    "    \n",
    "    # shuffle buffer size\n",
    "    dataset_pipeline_shuffle_buffer_size = 10000\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example_proto, max_sequence_length=hp.max_sequence_length):\n",
    "    features = {\n",
    "        'inputs': tf.VarLenFeature(tf.int64),\n",
    "        'word_endings': tf.VarLenFeature(tf.int64),\n",
    "        'targets': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    \n",
    "    parsed = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    def convert_and_pad(sparse_tensor):\n",
    "        result = tf.sparse_tensor_to_dense(sparse_tensor)\n",
    "        # TODO: properly ignore elements which are too large (right now we just clip)\n",
    "        result = result[:max_sequence_length]\n",
    "        result = tf.pad(result, [[0, max_sequence_length - tf.shape(result)[0]]])\n",
    "        return result\n",
    "    \n",
    "    return (convert_and_pad(parsed['inputs']),\n",
    "            tf.shape(parsed['inputs'])[0],\n",
    "            convert_and_pad(parsed['word_endings']),\n",
    "            convert_and_pad(parsed['targets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "# Data pipeline\n",
    "# -------------\n",
    "\n",
    "dataset_filenames = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(dataset_filenames)\n",
    "dataset = dataset.map(parse_example, \n",
    "                      num_parallel_calls = hp.dataset_pipeline_parallel_calls)\n",
    "dataset = dataset.shuffle(hp.dataset_pipeline_shuffle_buffer_size)\n",
    "dataset = dataset.prefetch(hp.dataset_pipeline_prefetch)\n",
    "dataset = dataset.batch(hp.batch_size)\n",
    "\n",
    "dataset_iterator = dataset.make_initializable_iterator()\n",
    "input_sequences, input_lengths, input_word_endings, target_sequences = dataset_iterator.get_next()\n",
    "\n",
    "# Placeholders\n",
    "# ------------\n",
    "\n",
    "# sequences of input positions (not a placeholder)\n",
    "input_positions = tf.range(hp.max_sequence_length, dtype=tf.int32)\n",
    "input_positions = tf.tile(input_positions, [tf.shape(input_sequences)[0]])\n",
    "input_positions = tf.reshape(input_positions, \n",
    "                             (tf.shape(input_sequences)[0], hp.max_sequence_length), \n",
    "                             name = 'input_positions')\n",
    "\n",
    "# Embeddings\n",
    "# ----------\n",
    "\n",
    "# sequences of input embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_sequence_embeddings = tf.get_variable('input_sequence_embeddings', \n",
    "                                            (hp.vocab_size, hp.embedding_size))\n",
    "input_sequences_embedded = tf.nn.embedding_lookup(input_sequence_embeddings, \n",
    "                                                  input_sequences,\n",
    "                                                  name = 'input_sequences_embedded')\n",
    "\n",
    "# sequences of input position embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_position_embeddings = tf.get_variable('input_position_embeddings', \n",
    "                                            (hp.max_sequence_length, hp.embedding_size))\n",
    "input_positions_embedded = tf.nn.embedding_lookup(input_position_embeddings, input_positions)\n",
    "\n",
    "# sequences of word ending embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_word_ending_embeddings = tf.get_variable('input_word_ending_embeddings',\n",
    "                                               (2, hp.embedding_size))\n",
    "input_word_endings_embedded = tf.nn.embedding_lookup(input_word_ending_embeddings, \n",
    "                                                     input_word_endings,\n",
    "                                                     name = 'input_word_endings_embedded')\n",
    "\n",
    "# Sequence mask\n",
    "# -------------\n",
    "\n",
    "sequence_mask = tf.sequence_mask(input_lengths,\n",
    "                                 hp.max_sequence_length,\n",
    "                                 dtype = tf.float32)\n",
    "# expand dimensions to support broadcasting\n",
    "expanded_sequence_mask = tf.expand_dims(sequence_mask, \n",
    "                                        2, \n",
    "                                        name = 'sequence')\n",
    "\n",
    "input_combined_embedded = tf.add_n([input_sequences_embedded, \n",
    "                                    input_positions_embedded, \n",
    "                                    input_word_endings_embedded])\n",
    "# TODO: is this necessary?\n",
    "input_combined_embedded = tf.multiply(input_combined_embedded,\n",
    "                                      expanded_sequence_mask,\n",
    "                                      name = 'input_combined_embedded')\n",
    "\n",
    "# Layer normalization\n",
    "# -------------------\n",
    "\n",
    "def layer_norm(x, scope, reuse=None, epsilon=1e-6):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        dim = x.get_shape()[-1]\n",
    "        scale = tf.get_variable(\n",
    "            \"layer_norm_scale\", [dim], initializer=tf.ones_initializer())\n",
    "        bias = tf.get_variable(\n",
    "            \"layer_norm_bias\", [dim], initializer=tf.zeros_initializer())\n",
    "        result = layer_norm_compute(x, epsilon, scale, bias)\n",
    "        return result\n",
    "\n",
    "def layer_norm_compute(x, epsilon, scale, bias):\n",
    "    # TODO: incorporate length into layer normalization?\n",
    "    mean = tf.reduce_mean(x, axis=[-1], keep_dims=True)\n",
    "    variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keep_dims=True)\n",
    "    norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n",
    "    return norm_x * scale + bias\n",
    "    \n",
    "# Attention\n",
    "# ---------\n",
    "\n",
    "def attention_layer(A):\n",
    "    A_T = tf.transpose(A, perm=[0, 2, 1])\n",
    "    scaled_logits = tf.matmul(A, A_T) / tf.sqrt(tf.cast(tf.shape(A)[-1], tf.float32))\n",
    "    return tf.matmul(tf.nn.softmax(scaled_logits), A)\n",
    "\n",
    "input_attention_layer_1 = layer_norm(input_combined_embedded + attention_layer(input_combined_embedded),\n",
    "                                     'input_attention_layer_1_norm')\n",
    "\n",
    "# Feed-forward\n",
    "# ------------\n",
    "\n",
    "def feed_forward_layer(A, num_units, scope='feed_forward', reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        A = tf.layers.dense(A, num_units, activation=tf.nn.relu)\n",
    "        return tf.layers.dense(A, num_units)\n",
    "\n",
    "input_feed_forward_layer_1 = layer_norm(input_attention_layer_1 + feed_forward_layer(input_attention_layer_1, hp.embedding_size),\n",
    "                                        'input_feed_forward_layer_1_norm')\n",
    "\n",
    "input_feed_forward_layer_1 *= expanded_sequence_mask\n",
    "\n",
    "# Softmax\n",
    "# -------\n",
    "\n",
    "output_logits = tf.layers.dense(input_feed_forward_layer_1, hp.num_target_classes)\n",
    "\n",
    "# Loss\n",
    "# ----\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_sequences,\n",
    "                                                        logits=output_logits)\n",
    "losses *= sequence_mask\n",
    "\n",
    "total_loss = tf.reduce_sum(losses)\n",
    "total_input_length = tf.reduce_sum(input_lengths)\n",
    "mean_loss  = total_loss / tf.cast(total_input_length, tf.float32)\n",
    "\n",
    "# Training\n",
    "# --------\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "train_op = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "\n",
    "# Summaries\n",
    "# ---------\n",
    "\n",
    "tf.summary.scalar('mean_loss', mean_loss)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"input_sequence_embeddings:0\": 2560000\n",
      "parameters for \"input_position_embeddings:0\": 12800\n",
      "parameters for \"input_word_ending_embeddings:0\": 512\n",
      "parameters for \"input_attention_layer_1_norm/layer_norm_scale:0\": 256\n",
      "parameters for \"input_attention_layer_1_norm/layer_norm_bias:0\": 256\n",
      "parameters for \"feed_forward/dense/kernel:0\": 65536\n",
      "parameters for \"feed_forward/dense/bias:0\": 256\n",
      "parameters for \"feed_forward/dense_1/kernel:0\": 65536\n",
      "parameters for \"feed_forward/dense_1/bias:0\": 256\n",
      "parameters for \"input_feed_forward_layer_1_norm/layer_norm_scale:0\": 256\n",
      "parameters for \"input_feed_forward_layer_1_norm/layer_norm_bias:0\": 256\n",
      "parameters for \"dense/kernel:0\": 512\n",
      "parameters for \"dense/bias:0\": 2\n",
      "total parameters: 2706434\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print('parameters for \"%s\": %d' % (variable.name, variable_parameters))\n",
    "    total_parameters += variable_parameters\n",
    "print('total parameters: %d' % total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100 batches: loss=0.297045, rate=166597 chars/s\n",
      "processed 200 batches: loss=0.261772, rate=208868 chars/s\n",
      "processed 300 batches: loss=0.246022, rate=228627 chars/s\n",
      "processed 400 batches: loss=0.238562, rate=239534 chars/s\n",
      "processed 500 batches: loss=0.230077, rate=251411 chars/s\n",
      "processed 600 batches: loss=0.223813, rate=258601 chars/s\n",
      "processed 700 batches: loss=0.221072, rate=262410 chars/s\n",
      "processed 800 batches: loss=0.218694, rate=265194 chars/s\n",
      "processed 900 batches: loss=0.217063, rate=267772 chars/s\n",
      "processed 1000 batches: loss=0.215631, rate=269627 chars/s\n",
      "processed 1100 batches: loss=0.214681, rate=269136 chars/s\n",
      "processed 1200 batches: loss=0.213761, rate=268199 chars/s\n",
      "processed 1300 batches: loss=0.212945, rate=269559 chars/s\n",
      "processed 1400 batches: loss=0.212298, rate=270843 chars/s\n",
      "processed 1500 batches: loss=0.209311, rate=274446 chars/s\n",
      "processed 1600 batches: loss=0.209058, rate=275122 chars/s\n",
      "processed 1700 batches: loss=0.208685, rate=275541 chars/s\n",
      "processed 1800 batches: loss=0.208124, rate=273878 chars/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-164696e38aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                                                            \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                            \u001b[0mtotal_input_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                                            merged_summaries))\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "# options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "# run_metadata = tf.RunMetadata()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    num_batches = 0\n",
    "    cum_loss = 0\n",
    "    cum_input_length = 0\n",
    "    \n",
    "    sess.run(dataset_iterator.initializer, feed_dict={\n",
    "        dataset_filenames: ['../data/simplewiki/simplewiki-20171103.entity_recognition.tfrecords']\n",
    "    })\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    while True:\n",
    "        num_batches += 1\n",
    "        try:\n",
    "            _, curr_loss, curr_input_length, _ = sess.run((train_op,\n",
    "                                                           total_loss,\n",
    "                                                           total_input_length,\n",
    "                                                           merged_summaries))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "            \n",
    "#         timeline = timeline.Timeline(run_metadata.step_stats)\n",
    "#         trace = timeline.generate_chrome_trace_format()\n",
    "#         with open('trace_%d.json' % num_batches, 'w') as f:\n",
    "#             f.write(trace)\n",
    "            \n",
    "        cum_loss += curr_loss\n",
    "        cum_input_length += curr_input_length\n",
    "        \n",
    "        if num_batches % 100 == 0:\n",
    "            finish = datetime.datetime.now()\n",
    "            elapsed = (finish - start).total_seconds()\n",
    "            print('processed %d batches: loss=%g, rate=%g chars/s' % (num_batches, \n",
    "                                                                      cum_loss/cum_input_length, \n",
    "                                                                      float(cum_input_length)/elapsed))\n",
    "            \n",
    "    finish = datetime.datetime.now()\n",
    "    elapsed = (finish - start).total_seconds()\n",
    "    print('epoch %d: loss=%g, time=%g s' % (epoch, cum_loss/cum_input_length, elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
