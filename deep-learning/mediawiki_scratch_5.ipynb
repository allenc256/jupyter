{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf():\n",
    "    global sess\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    # maximum number of symbols in an input sequence\n",
    "    max_sequence_length = 50\n",
    "    \n",
    "    # number of symbols in vocabulary\n",
    "    # (symbols are expected to be in range(vocab_size))\n",
    "    vocab_size = 10000\n",
    "\n",
    "    # number of dimensions in input embeddings\n",
    "    embedding_size = 128\n",
    "    \n",
    "    # number of sequences per batch\n",
    "    batch_size = 64\n",
    "    \n",
    "    # number of target classes\n",
    "    num_target_classes = 2\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "# Placeholders\n",
    "# ------------\n",
    "\n",
    "# sequences of input symbols\n",
    "input_sequences = tf.placeholder(tf.int32, \n",
    "                                 shape = (hp.batch_size, hp.max_sequence_length))\n",
    "# length of each sequence\n",
    "input_lengths = tf.placeholder(tf.int32, \n",
    "                               shape = (hp.batch_size))\n",
    "# sequences of word ending markers\n",
    "input_word_endings = tf.placeholder(tf.int32, \n",
    "                                    shape = (hp.batch_size, hp.max_sequence_length))\n",
    "# sequences of target symbols\n",
    "target_sequences = tf.placeholder(tf.int32, \n",
    "                                  shape = (hp.batch_size, hp.max_sequence_length))\n",
    "\n",
    "# sequences of input positions (not a placeholder)\n",
    "input_positions = tf.range(hp.max_sequence_length, dtype=tf.int32)\n",
    "input_positions = tf.tile(input_positions, [hp.batch_size])\n",
    "input_positions = tf.reshape(input_positions, \n",
    "                             (hp.batch_size, hp.max_sequence_length), \n",
    "                             name = 'input_positions')\n",
    "\n",
    "# Embeddings\n",
    "# ----------\n",
    "\n",
    "# sequences of input embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_sequence_embeddings = tf.get_variable('input_sequence_embeddings', \n",
    "                                            (hp.vocab_size, hp.embedding_size))\n",
    "input_sequences_embedded = tf.nn.embedding_lookup(input_sequence_embeddings, \n",
    "                                                  input_sequences,\n",
    "                                                  name = 'input_sequences_embedded')\n",
    "\n",
    "# sequences of input position embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_position_embeddings = tf.get_variable('input_position_embeddings', \n",
    "                                            (hp.max_sequence_length, hp.embedding_size))\n",
    "input_positions_embedded = tf.nn.embedding_lookup(input_position_embeddings, input_positions)\n",
    "\n",
    "# sequences of word ending embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_word_ending_embeddings = tf.get_variable('input_word_ending_embeddings',\n",
    "                                               (2, hp.embedding_size))\n",
    "input_word_endings_embedded = tf.nn.embedding_lookup(input_word_ending_embeddings, \n",
    "                                                     input_word_endings,\n",
    "                                                     name = 'input_word_endings_embedded')\n",
    "\n",
    "# Sequence mask\n",
    "# -------------\n",
    "\n",
    "sequence_mask = tf.sequence_mask(input_lengths,\n",
    "                                 hp.max_sequence_length,\n",
    "                                 dtype = tf.float32)\n",
    "# expand dimensions to support broadcasting\n",
    "expanded_sequence_mask = tf.expand_dims(sequence_mask, \n",
    "                                        2, \n",
    "                                        name = 'sequence')\n",
    "\n",
    "input_combined_embedded = tf.add_n([input_sequences_embedded, \n",
    "                                    input_positions_embedded, \n",
    "                                    input_word_endings_embedded])\n",
    "# TODO: is this necessary?\n",
    "input_combined_embedded = tf.multiply(input_combined_embedded,\n",
    "                                      expanded_sequence_mask,\n",
    "                                      name = 'input_combined_embedded')\n",
    "\n",
    "# Attention\n",
    "# ---------\n",
    "\n",
    "def attention_layer(A):\n",
    "    A_T = tf.transpose(A, perm=[0, 2, 1])\n",
    "    scaled_logits = tf.matmul(A, A_T) / tf.sqrt(tf.cast(tf.shape(A)[-1], tf.float32))\n",
    "    return tf.matmul(tf.nn.softmax(scaled_logits), A)\n",
    "\n",
    "# TODO: layer normalization (possibly with masking)\n",
    "input_attention_layer_1 = input_combined_embedded + attention_layer(input_combined_embedded)\n",
    "\n",
    "# Feed-forward\n",
    "# ------------\n",
    "\n",
    "def feed_forward_layer(A, num_units, scope='feed_forward', reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        A = tf.layers.dense(A, num_units, activation=tf.nn.relu)\n",
    "        return tf.layers.dense(A, num_units)\n",
    "\n",
    "# TODO: layer normalization (possibly with masking)\n",
    "input_feed_forward_layer_1 = input_attention_layer_1 + feed_forward_layer(input_attention_layer_1, hp.embedding_size)\n",
    "\n",
    "# TODO: how important is this masking?\n",
    "input_feed_forward_layer_1 *= expanded_sequence_mask\n",
    "\n",
    "# Softmax\n",
    "# -------\n",
    "\n",
    "output_logits = tf.layers.dense(input_feed_forward_layer_1, hp.num_target_classes)\n",
    "\n",
    "# Loss\n",
    "# ----\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_sequences,\n",
    "                                                        logits=output_logits)\n",
    "losses *= sequence_mask\n",
    "\n",
    "total_loss = tf.reduce_sum(losses)\n",
    "mean_loss  = total_loss / tf.cast(tf.reduce_sum(input_lengths), tf.float32)\n",
    "\n",
    "# Training\n",
    "# --------\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "# gradient clipping\n",
    "gradients, variables = zip(*optimizer.compute_gradients(mean_loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "train_op = optimizer.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "sess.run(tf.shape(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
