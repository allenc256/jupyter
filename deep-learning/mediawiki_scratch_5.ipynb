{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import gzip\n",
    "import json\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf():\n",
    "    global sess\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    # maximum number of symbols in an input sequence\n",
    "    max_sequence_length = 50\n",
    "    \n",
    "    # number of symbols in vocabulary\n",
    "    # (symbols are expected to be in range(vocab_size))\n",
    "    vocab_size = 10000\n",
    "\n",
    "    # number of dimensions in input embeddings\n",
    "    embedding_size = 128\n",
    "    \n",
    "    # number of sequences per batch\n",
    "    batch_size = 64\n",
    "    \n",
    "    # number of target classes\n",
    "    num_target_classes = 2\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Name: <unknown>, Key: word_endings, Index: 0.  Number of int64 values != expected.  Values size: 16 but output shape: [50]\n\t [[Node: ParseSingleExample/ParseExample/ParseExample = ParseExample[Ndense=3, Nsparse=0, Tdense=[DT_INT64, DT_INT64, DT_INT64], dense_shapes=[[50], [50], [50]], sparse_types=[]](ParseSingleExample/ExpandDims, ParseSingleExample/ParseExample/ParseExample/names, ParseSingleExample/ParseExample/ParseExample/dense_keys_0, ParseSingleExample/ParseExample/ParseExample/dense_keys_1, ParseSingleExample/ParseExample/ParseExample/dense_keys_2, ParseSingleExample/ParseExample/Const, ParseSingleExample/ParseExample/Const_1, ParseSingleExample/ParseExample/Const_2)]]\n\t [[Node: IteratorGetNext_11 = IteratorGetNext[output_shapes=[[50], [50]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_5)]]\n\nCaused by op 'ParseSingleExample/ParseExample/ParseExample', defined at:\n  File \"/home/achang/anaconda3/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/achang/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-05ce1f77378b>\", line 4, in <module>\n    'targets': tf.VarLenFeature(tf.int64)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 759, in parse_single_example\n    dense_types, dense_defaults, dense_shapes, name)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 833, in _parse_single_example_raw\n    name=name)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 706, in _parse_example_raw\n    name=name)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 263, in _parse_example\n    dense_shapes=dense_shapes, name=name)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Name: <unknown>, Key: word_endings, Index: 0.  Number of int64 values != expected.  Values size: 16 but output shape: [50]\n\t [[Node: ParseSingleExample/ParseExample/ParseExample = ParseExample[Ndense=3, Nsparse=0, Tdense=[DT_INT64, DT_INT64, DT_INT64], dense_shapes=[[50], [50], [50]], sparse_types=[]](ParseSingleExample/ExpandDims, ParseSingleExample/ParseExample/ParseExample/names, ParseSingleExample/ParseExample/ParseExample/dense_keys_0, ParseSingleExample/ParseExample/ParseExample/dense_keys_1, ParseSingleExample/ParseExample/ParseExample/dense_keys_2, ParseSingleExample/ParseExample/Const, ParseSingleExample/ParseExample/Const_1, ParseSingleExample/ParseExample/Const_2)]]\n\t [[Node: IteratorGetNext_11 = IteratorGetNext[output_shapes=[[50], [50]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_5)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Name: <unknown>, Key: word_endings, Index: 0.  Number of int64 values != expected.  Values size: 16 but output shape: [50]\n\t [[Node: ParseSingleExample/ParseExample/ParseExample = ParseExample[Ndense=3, Nsparse=0, Tdense=[DT_INT64, DT_INT64, DT_INT64], dense_shapes=[[50], [50], [50]], sparse_types=[]](ParseSingleExample/ExpandDims, ParseSingleExample/ParseExample/ParseExample/names, ParseSingleExample/ParseExample/ParseExample/dense_keys_0, ParseSingleExample/ParseExample/ParseExample/dense_keys_1, ParseSingleExample/ParseExample/ParseExample/dense_keys_2, ParseSingleExample/ParseExample/Const, ParseSingleExample/ParseExample/Const_1, ParseSingleExample/ParseExample/Const_2)]]\n\t [[Node: IteratorGetNext_11 = IteratorGetNext[output_shapes=[[50], [50]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_5)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-54ebdfd29c45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnext_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Name: <unknown>, Key: word_endings, Index: 0.  Number of int64 values != expected.  Values size: 16 but output shape: [50]\n\t [[Node: ParseSingleExample/ParseExample/ParseExample = ParseExample[Ndense=3, Nsparse=0, Tdense=[DT_INT64, DT_INT64, DT_INT64], dense_shapes=[[50], [50], [50]], sparse_types=[]](ParseSingleExample/ExpandDims, ParseSingleExample/ParseExample/ParseExample/names, ParseSingleExample/ParseExample/ParseExample/dense_keys_0, ParseSingleExample/ParseExample/ParseExample/dense_keys_1, ParseSingleExample/ParseExample/ParseExample/dense_keys_2, ParseSingleExample/ParseExample/Const, ParseSingleExample/ParseExample/Const_1, ParseSingleExample/ParseExample/Const_2)]]\n\t [[Node: IteratorGetNext_11 = IteratorGetNext[output_shapes=[[50], [50]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_5)]]\n\nCaused by op 'ParseSingleExample/ParseExample/ParseExample', defined at:\n  File \"/home/achang/anaconda3/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/achang/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-05ce1f77378b>\", line 4, in <module>\n    'targets': tf.VarLenFeature(tf.int64)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 759, in parse_single_example\n    dense_types, dense_defaults, dense_shapes, name)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 833, in _parse_single_example_raw\n    name=name)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 706, in _parse_example_raw\n    name=name)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 263, in _parse_example\n    dense_shapes=dense_shapes, name=name)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/achang/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Name: <unknown>, Key: word_endings, Index: 0.  Number of int64 values != expected.  Values size: 16 but output shape: [50]\n\t [[Node: ParseSingleExample/ParseExample/ParseExample = ParseExample[Ndense=3, Nsparse=0, Tdense=[DT_INT64, DT_INT64, DT_INT64], dense_shapes=[[50], [50], [50]], sparse_types=[]](ParseSingleExample/ExpandDims, ParseSingleExample/ParseExample/ParseExample/names, ParseSingleExample/ParseExample/ParseExample/dense_keys_0, ParseSingleExample/ParseExample/ParseExample/dense_keys_1, ParseSingleExample/ParseExample/ParseExample/dense_keys_2, ParseSingleExample/ParseExample/Const, ParseSingleExample/ParseExample/Const_1, ParseSingleExample/ParseExample/Const_2)]]\n\t [[Node: IteratorGetNext_11 = IteratorGetNext[output_shapes=[[50], [50]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_5)]]\n"
     ]
    }
   ],
   "source": [
    "dataset_filenames = [\"../data/simplewiki/simplewiki-20171103.entity_recognition.tfrecords\"]\n",
    "\n",
    "def parse_example(example_proto):\n",
    "    features = {\n",
    "        'inputs': tf.FixedLenFeature((50), tf.int64),\n",
    "        'word_endings': tf.FixedLenFeature((50), tf.int64),\n",
    "        'targets': tf.FixedLenFeature((50), tf.int64)\n",
    "    }\n",
    "    parsed = tf.parse_single_example(example_proto, features)\n",
    "    return parsed['inputs'], parsed['targets']\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(dataset_filenames)\n",
    "dataset = dataset.map(parse_example)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "proto = sess.run(iterator.get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': <tf.Tensor 'ParseSingleExample_1/Squeeze_inputs:0' shape=() dtype=int64>,\n",
       " 'targets': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f72dc4bdcc0>,\n",
       " 'word_endings': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f72df4f50b8>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.parse_single_example(proto, {\n",
    "    'inputs': tf.FixedLenFeature((), tf.int64),\n",
    "    'word_endings': tf.VarLenFeature(tf.int64),\n",
    "    'targets': tf.VarLenFeature(tf.int64)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095155/1095155 [00:08<00:00, 130964.65it/s]\n"
     ]
    }
   ],
   "source": [
    "input_sequences_dataset = []\n",
    "input_lengths_dataset = []\n",
    "input_word_endings_dataset = []\n",
    "target_sequences_dataset = []\n",
    "\n",
    "for sentence in tqdm(dataset):\n",
    "    inputs = sentence['inputs']\n",
    "    input_length = len(inputs)\n",
    "    word_endings = sentence['word_endings']\n",
    "    targets = sentence['targets']\n",
    "    \n",
    "    if input_length > hp.max_sequence_length:\n",
    "        continue\n",
    "    \n",
    "    inputs = inputs + [0] * (hp.max_sequence_length - input_length)\n",
    "    word_endings = word_endings + [0] * (hp.max_sequence_length - input_length)\n",
    "    targets = targets + [0] * (hp.max_sequence_length - input_length)\n",
    "    \n",
    "    input_sequences_dataset.append(inputs)\n",
    "    input_lengths_dataset.append(input_length)\n",
    "    input_word_endings_dataset.append(word_endings)\n",
    "    target_sequences_dataset.append(targets)\n",
    "\n",
    "input_sequences_dataset = tf.convert_to_tensor(input_sequences_dataset)\n",
    "input_lengths_dataset = tf.convert_to_tensor(input_lengths_dataset)\n",
    "input_word_endings_dataset = tf.convert_to_tensor(input_word_endings_dataset)\n",
    "target_sequences_dataset = tf.convert_to_tensor(target_sequences_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = tf.constant(['train/img1.png', 'train/img2.png',\n",
    "                          'train/img3.png', 'train/img4.png',\n",
    "                          'train/img5.png', 'train/img6.png'])\n",
    "train_labels = tf.constant([0, 0, 0, 1, 1, 1])\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_imgs, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "# Placeholders\n",
    "# ------------\n",
    "\n",
    "# sequences of input symbols\n",
    "input_sequences = tf.placeholder(tf.int32, \n",
    "                                 shape = (hp.batch_size, hp.max_sequence_length))\n",
    "# length of each sequence\n",
    "input_lengths = tf.placeholder(tf.int32, \n",
    "                               shape = (hp.batch_size))\n",
    "# sequences of word ending markers\n",
    "input_word_endings = tf.placeholder(tf.int32, \n",
    "                                    shape = (hp.batch_size, hp.max_sequence_length))\n",
    "# sequences of target symbols\n",
    "target_sequences = tf.placeholder(tf.int32, \n",
    "                                  shape = (hp.batch_size, hp.max_sequence_length))\n",
    "\n",
    "# sequences of input positions (not a placeholder)\n",
    "input_positions = tf.range(hp.max_sequence_length, dtype=tf.int32)\n",
    "input_positions = tf.tile(input_positions, [hp.batch_size])\n",
    "input_positions = tf.reshape(input_positions, \n",
    "                             (hp.batch_size, hp.max_sequence_length), \n",
    "                             name = 'input_positions')\n",
    "\n",
    "# Embeddings\n",
    "# ----------\n",
    "\n",
    "# sequences of input embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_sequence_embeddings = tf.get_variable('input_sequence_embeddings', \n",
    "                                            (hp.vocab_size, hp.embedding_size))\n",
    "input_sequences_embedded = tf.nn.embedding_lookup(input_sequence_embeddings, \n",
    "                                                  input_sequences,\n",
    "                                                  name = 'input_sequences_embedded')\n",
    "\n",
    "# sequences of input position embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_position_embeddings = tf.get_variable('input_position_embeddings', \n",
    "                                            (hp.max_sequence_length, hp.embedding_size))\n",
    "input_positions_embedded = tf.nn.embedding_lookup(input_position_embeddings, input_positions)\n",
    "\n",
    "# sequences of word ending embeddings w/ shape:\n",
    "#   (hp.batch_size, hp.max_sequence_length, hp.embedding_size)\n",
    "input_word_ending_embeddings = tf.get_variable('input_word_ending_embeddings',\n",
    "                                               (2, hp.embedding_size))\n",
    "input_word_endings_embedded = tf.nn.embedding_lookup(input_word_ending_embeddings, \n",
    "                                                     input_word_endings,\n",
    "                                                     name = 'input_word_endings_embedded')\n",
    "\n",
    "# Sequence mask\n",
    "# -------------\n",
    "\n",
    "sequence_mask = tf.sequence_mask(input_lengths,\n",
    "                                 hp.max_sequence_length,\n",
    "                                 dtype = tf.float32)\n",
    "# expand dimensions to support broadcasting\n",
    "expanded_sequence_mask = tf.expand_dims(sequence_mask, \n",
    "                                        2, \n",
    "                                        name = 'sequence')\n",
    "\n",
    "input_combined_embedded = tf.add_n([input_sequences_embedded, \n",
    "                                    input_positions_embedded, \n",
    "                                    input_word_endings_embedded])\n",
    "# TODO: is this necessary?\n",
    "input_combined_embedded = tf.multiply(input_combined_embedded,\n",
    "                                      expanded_sequence_mask,\n",
    "                                      name = 'input_combined_embedded')\n",
    "\n",
    "# Attention\n",
    "# ---------\n",
    "\n",
    "def attention_layer(A):\n",
    "    A_T = tf.transpose(A, perm=[0, 2, 1])\n",
    "    scaled_logits = tf.matmul(A, A_T) / tf.sqrt(tf.cast(tf.shape(A)[-1], tf.float32))\n",
    "    return tf.matmul(tf.nn.softmax(scaled_logits), A)\n",
    "\n",
    "# TODO: layer normalization (possibly with masking)\n",
    "input_attention_layer_1 = input_combined_embedded + attention_layer(input_combined_embedded)\n",
    "\n",
    "# Feed-forward\n",
    "# ------------\n",
    "\n",
    "def feed_forward_layer(A, num_units, scope='feed_forward', reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        A = tf.layers.dense(A, num_units, activation=tf.nn.relu)\n",
    "        return tf.layers.dense(A, num_units)\n",
    "\n",
    "# TODO: layer normalization (possibly with masking)\n",
    "input_feed_forward_layer_1 = input_attention_layer_1 + feed_forward_layer(input_attention_layer_1, hp.embedding_size)\n",
    "\n",
    "# TODO: how important is this masking?\n",
    "input_feed_forward_layer_1 *= expanded_sequence_mask\n",
    "\n",
    "# Softmax\n",
    "# -------\n",
    "\n",
    "output_logits = tf.layers.dense(input_feed_forward_layer_1, hp.num_target_classes)\n",
    "\n",
    "# Loss\n",
    "# ----\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_sequences,\n",
    "                                                        logits=output_logits)\n",
    "losses *= sequence_mask\n",
    "\n",
    "total_loss = tf.reduce_sum(losses)\n",
    "mean_loss  = total_loss / tf.cast(tf.reduce_sum(input_lengths), tf.float32)\n",
    "\n",
    "# Training\n",
    "# --------\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "\n",
    "# gradient clipping\n",
    "gradients, variables = zip(*optimizer.compute_gradients(mean_loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "train_op = optimizer.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_indices = np.arange(len(input_sequences_dataset))\n",
    "\n",
    "# for predictable rng\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: batch 0/16517\n",
      "epoch 0: batch 1000/16517\n",
      "epoch 0: batch 2000/16517\n",
      "epoch 0: batch 3000/16517\n",
      "epoch 0: batch 4000/16517\n",
      "epoch 0: batch 5000/16517\n",
      "epoch 0: batch 6000/16517\n",
      "epoch 0: batch 7000/16517\n",
      "epoch 0: batch 8000/16517\n",
      "epoch 0: batch 9000/16517\n",
      "epoch 0: batch 10000/16517\n",
      "epoch 0: batch 11000/16517\n",
      "epoch 0: batch 12000/16517\n",
      "epoch 0: batch 13000/16517\n",
      "epoch 0: batch 14000/16517\n",
      "epoch 0: batch 15000/16517\n",
      "epoch 0: batch 16000/16517\n",
      "epoch 0: loss 0.201803\n",
      "epoch 1: batch 0/16517\n",
      "epoch 1: batch 1000/16517\n",
      "epoch 1: batch 2000/16517\n",
      "epoch 1: batch 3000/16517\n",
      "epoch 1: batch 4000/16517\n",
      "epoch 1: batch 5000/16517\n",
      "epoch 1: batch 6000/16517\n",
      "epoch 1: batch 7000/16517\n",
      "epoch 1: batch 8000/16517\n",
      "epoch 1: batch 9000/16517\n",
      "epoch 1: batch 10000/16517\n",
      "epoch 1: batch 11000/16517\n",
      "epoch 1: batch 12000/16517\n",
      "epoch 1: batch 13000/16517\n",
      "epoch 1: batch 14000/16517\n",
      "epoch 1: batch 15000/16517\n",
      "epoch 1: batch 16000/16517\n",
      "epoch 1: loss 0.194863\n",
      "epoch 2: batch 0/16517\n",
      "epoch 2: batch 1000/16517\n",
      "epoch 2: batch 2000/16517\n",
      "epoch 2: batch 3000/16517\n",
      "epoch 2: batch 4000/16517\n",
      "epoch 2: batch 5000/16517\n",
      "epoch 2: batch 6000/16517\n",
      "epoch 2: batch 7000/16517\n",
      "epoch 2: batch 8000/16517\n",
      "epoch 2: batch 9000/16517\n",
      "epoch 2: batch 10000/16517\n",
      "epoch 2: batch 11000/16517\n",
      "epoch 2: batch 12000/16517\n",
      "epoch 2: batch 13000/16517\n",
      "epoch 2: batch 14000/16517\n",
      "epoch 2: batch 15000/16517\n",
      "epoch 2: batch 16000/16517\n",
      "epoch 2: loss 0.190571\n",
      "epoch 3: batch 0/16517\n",
      "epoch 3: batch 1000/16517\n",
      "epoch 3: batch 2000/16517\n",
      "epoch 3: batch 3000/16517\n",
      "epoch 3: batch 4000/16517\n",
      "epoch 3: batch 5000/16517\n",
      "epoch 3: batch 6000/16517\n",
      "epoch 3: batch 7000/16517\n",
      "epoch 3: batch 8000/16517\n",
      "epoch 3: batch 9000/16517\n",
      "epoch 3: batch 10000/16517\n",
      "epoch 3: batch 11000/16517\n",
      "epoch 3: batch 12000/16517\n",
      "epoch 3: batch 13000/16517\n",
      "epoch 3: batch 14000/16517\n",
      "epoch 3: batch 15000/16517\n",
      "epoch 3: batch 16000/16517\n",
      "epoch 3: loss 0.187686\n",
      "epoch 4: batch 0/16517\n",
      "epoch 4: batch 1000/16517\n",
      "epoch 4: batch 2000/16517\n",
      "epoch 4: batch 3000/16517\n",
      "epoch 4: batch 4000/16517\n",
      "epoch 4: batch 5000/16517\n",
      "epoch 4: batch 6000/16517\n",
      "epoch 4: batch 7000/16517\n",
      "epoch 4: batch 8000/16517\n",
      "epoch 4: batch 9000/16517\n",
      "epoch 4: batch 10000/16517\n",
      "epoch 4: batch 11000/16517\n",
      "epoch 4: batch 12000/16517\n",
      "epoch 4: batch 13000/16517\n",
      "epoch 4: batch 14000/16517\n",
      "epoch 4: batch 15000/16517\n",
      "epoch 4: batch 16000/16517\n",
      "epoch 4: loss 0.18559\n",
      "epoch 5: batch 0/16517\n",
      "epoch 5: batch 1000/16517\n",
      "epoch 5: batch 2000/16517\n",
      "epoch 5: batch 3000/16517\n",
      "epoch 5: batch 4000/16517\n",
      "epoch 5: batch 5000/16517\n",
      "epoch 5: batch 6000/16517\n",
      "epoch 5: batch 7000/16517\n",
      "epoch 5: batch 8000/16517\n",
      "epoch 5: batch 9000/16517\n",
      "epoch 5: batch 10000/16517\n",
      "epoch 5: batch 11000/16517\n",
      "epoch 5: batch 12000/16517\n",
      "epoch 5: batch 13000/16517\n",
      "epoch 5: batch 14000/16517\n",
      "epoch 5: batch 15000/16517\n",
      "epoch 5: batch 16000/16517\n",
      "epoch 5: loss 0.183944\n",
      "epoch 6: batch 0/16517\n",
      "epoch 6: batch 1000/16517\n",
      "epoch 6: batch 2000/16517\n",
      "epoch 6: batch 3000/16517\n",
      "epoch 6: batch 4000/16517\n",
      "epoch 6: batch 5000/16517\n",
      "epoch 6: batch 6000/16517\n",
      "epoch 6: batch 7000/16517\n",
      "epoch 6: batch 8000/16517\n",
      "epoch 6: batch 9000/16517\n",
      "epoch 6: batch 10000/16517\n",
      "epoch 6: batch 11000/16517\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-30b4cf7511cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcurr_target_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_sequences_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mcurr_input_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "curr_input_sequences = np.zeros((hp.batch_size, hp.max_sequence_length), dtype=np.int32)\n",
    "curr_input_lengths = np.zeros(hp.batch_size, dtype=np.int32)\n",
    "curr_input_word_endings = np.zeros((hp.batch_size, hp.max_sequence_length), dtype=np.int32)\n",
    "curr_target_sequences = np.zeros((hp.batch_size, hp.max_sequence_length), dtype=np.int32)\n",
    "\n",
    "total_input_length = np.sum(input_lengths_dataset)\n",
    "num_batches = (len(batch_indices) + hp.batch_size - 1) // hp.batch_size\n",
    "\n",
    "for epoch in range(20):\n",
    "    np.random.shuffle(batch_indices)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        if i % 1000 == 0:\n",
    "            print('epoch %d: batch %d/%d' % (epoch, i, num_batches))\n",
    "        \n",
    "        curr_indices = batch_indices[i*hp.batch_size:(i+1)*hp.batch_size]\n",
    "\n",
    "        curr_input_sequences.fill(0)\n",
    "        curr_input_lengths.fill(0)\n",
    "        curr_input_word_endings.fill(0)\n",
    "        curr_target_sequences.fill(0)\n",
    "\n",
    "        x = input_sequences_dataset[curr_indices]\n",
    "        curr_input_sequences[:x.shape[0]] = x\n",
    "\n",
    "        x = input_lengths_dataset[curr_indices]\n",
    "        curr_input_lengths[:x.shape[0]] = x\n",
    "\n",
    "        x = input_word_endings_dataset[curr_indices]\n",
    "        curr_input_word_endings[:x.shape[0]] = x\n",
    "\n",
    "        x = target_sequences_dataset[curr_indices]\n",
    "        curr_target_sequences[:x.shape[0]] = x\n",
    "\n",
    "        feed_dict = {\n",
    "            input_sequences: curr_input_sequences,\n",
    "            input_lengths: curr_input_lengths,\n",
    "            input_word_endings: curr_input_word_endings,\n",
    "            target_sequences: curr_target_sequences\n",
    "        }\n",
    "\n",
    "        curr_loss, _ = sess.run((total_loss, train_op), feed_dict = feed_dict)\n",
    "        epoch_loss += curr_loss\n",
    "    \n",
    "    epoch_loss /= total_input_length\n",
    "    print('epoch %d: loss %g' % (epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
