{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import modules.mediawiki as mw\n",
    "from tensorflow.python.client import timeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    # adam learning rate\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    # maximum number of symbols in an input sequence\n",
    "    max_sequence_length = 40\n",
    "\n",
    "    # number of symbols in vocabulary\n",
    "    # (symbols are expected to be in range(vocab_size))\n",
    "    vocab_size = 10000\n",
    "\n",
    "    # number of dimensions in input embeddings\n",
    "    embedding_size = 128\n",
    "    \n",
    "    # number of hidden units in ff layer \n",
    "    ff_size = 128\n",
    "    \n",
    "    # number of combined (attention + feed forward) layers\n",
    "    num_layers = 1\n",
    "    \n",
    "    # dropout rate\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    # number of sequences per batch\n",
    "    pipeline_batch_size = 1024\n",
    "    \n",
    "    # number of parsing threads in data pipeline\n",
    "    pipeline_num_parallel_calls = 4\n",
    "    \n",
    "    # size of prefetch in data pipeline\n",
    "    pipeline_prefetch_size = pipeline_batch_size * 16\n",
    "    \n",
    "    # shuffle buffer size\n",
    "    pipeline_shuffle_size = 10000\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(mw.BaseModel):\n",
    "    def __init__(self, hp):\n",
    "        super().__init__(hp)\n",
    "    \n",
    "    def _build_prediction_model_internal(self):\n",
    "        # Embeddings\n",
    "        # ----------\n",
    "        \n",
    "        with tf.variable_scope('embeddings'):\n",
    "            input_sequence_embeddings = tf.get_variable('input_sequence_embeddings', \n",
    "                                                        (self._hp.vocab_size, self._hp.embedding_size))\n",
    "            input_sequences_embedded = tf.nn.embedding_lookup(input_sequence_embeddings, \n",
    "                                                              self._input_sequences)\n",
    "\n",
    "            input_position_embeddings = tf.get_variable('input_position_embeddings', \n",
    "                                                        (self._hp.max_sequence_length, self._hp.embedding_size))\n",
    "            input_positions_embedded = tf.nn.embedding_lookup(input_position_embeddings, self._input_positions)\n",
    "\n",
    "            input_word_ending_embeddings = tf.get_variable('input_word_ending_embeddings',\n",
    "                                                           (2, self._hp.embedding_size))\n",
    "            input_word_endings_embedded = tf.nn.embedding_lookup(input_word_ending_embeddings, \n",
    "                                                                 self._input_word_endings)\n",
    "\n",
    "            sequence_mask = tf.sequence_mask(self._input_lengths,\n",
    "                                             self._hp.max_sequence_length,\n",
    "                                             dtype = tf.float32)\n",
    "            sequence_mask = tf.expand_dims(sequence_mask, 2)\n",
    "\n",
    "            input_combined_embedded = tf.add_n([input_sequences_embedded, \n",
    "                                                input_positions_embedded, \n",
    "                                                input_word_endings_embedded])\n",
    "            input_combined_embedded *= sequence_mask\n",
    "            input_combined_embedded = tf.layers.dropout(input_combined_embedded,\n",
    "                                                        rate = self._hp.dropout_rate,\n",
    "                                                        training = self._is_training)\n",
    "        \n",
    "        # Layer normalization\n",
    "        # -------------------\n",
    "\n",
    "        def layer_norm(x, scope, reuse=None, epsilon=1e-6):\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                num_units = self._hp.embedding_size\n",
    "                scale = tf.get_variable(\n",
    "                    \"layer_norm_scale\", [num_units], initializer=tf.ones_initializer())\n",
    "                bias = tf.get_variable(\n",
    "                    \"layer_norm_bias\", [num_units], initializer=tf.zeros_initializer())\n",
    "                result = layer_norm_compute(x, epsilon, scale, bias)\n",
    "                return result\n",
    "\n",
    "        def layer_norm_compute(x, epsilon, scale, bias):\n",
    "            # TODO: incorporate length into layer normalization?\n",
    "            mean = tf.reduce_mean(x, axis=[-1], keep_dims=True)\n",
    "            variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keep_dims=True)\n",
    "            norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n",
    "            return norm_x * scale + bias\n",
    "\n",
    "        # Attention\n",
    "        # ---------\n",
    "\n",
    "        def attention_layer(A):\n",
    "            A_T = tf.transpose(A, perm=[0, 2, 1])\n",
    "            scaled_logits = tf.matmul(A, A_T) / tf.sqrt(tf.cast(tf.shape(A)[-1], tf.float32))\n",
    "            result = tf.matmul(tf.nn.softmax(scaled_logits), A)\n",
    "            result = tf.layers.dropout(result, rate = self._hp.dropout_rate, training = self._is_training)\n",
    "            return result\n",
    "\n",
    "        # Feed-forward\n",
    "        # ------------\n",
    "\n",
    "        def feed_forward_layer(A, scope, reuse=None):\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                A = tf.layers.dense(A, self._hp.ff_size, activation=tf.nn.relu, name='fc1')\n",
    "                A = tf.layers.dense(A, self._hp.embedding_size, name='fc2')\n",
    "                A = tf.layers.dropout(A, rate = self._hp.dropout_rate, training = self._is_training)\n",
    "                return A\n",
    "\n",
    "        # Layers\n",
    "        # ------\n",
    "\n",
    "        def combined_layer(A, scope, reuse=None):\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                A = layer_norm(A + attention_layer(A), scope='attention_norm')\n",
    "                A = layer_norm(A + feed_forward_layer(A, 'ff'), scope='ff_norm')\n",
    "                A *= sequence_mask\n",
    "                return A\n",
    "\n",
    "        with tf.variable_scope('layers'):\n",
    "            layer = input_combined_embedded\n",
    "            for i in range(self._hp.num_layers):\n",
    "                layer = combined_layer(layer, 'layer_%d' % i)\n",
    "            \n",
    "        # Softmax\n",
    "        # -------\n",
    "\n",
    "        with tf.variable_scope('softmax'):\n",
    "            output_logits = tf.layers.dense(layer, 2, name = 'softmax')\n",
    "        \n",
    "        return output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"embeddings/input_sequence_embeddings:0\": 1280000\n",
      "parameters for \"embeddings/input_position_embeddings:0\": 5120\n",
      "parameters for \"embeddings/input_word_ending_embeddings:0\": 256\n",
      "parameters for \"layers/layer_0/attention_norm/layer_norm_scale:0\": 128\n",
      "parameters for \"layers/layer_0/attention_norm/layer_norm_bias:0\": 128\n",
      "parameters for \"layers/layer_0/ff/fc1/kernel:0\": 16384\n",
      "parameters for \"layers/layer_0/ff/fc1/bias:0\": 128\n",
      "parameters for \"layers/layer_0/ff/fc2/kernel:0\": 16384\n",
      "parameters for \"layers/layer_0/ff/fc2/bias:0\": 128\n",
      "parameters for \"layers/layer_0/ff_norm/layer_norm_scale:0\": 128\n",
      "parameters for \"layers/layer_0/ff_norm/layer_norm_bias:0\": 128\n",
      "parameters for \"softmax/softmax/kernel:0\": 256\n",
      "parameters for \"softmax/softmax/bias:0\": 2\n",
      "total parameters: 1319170\n"
     ]
    }
   ],
   "source": [
    "sess = mw.reset_tf(sess)\n",
    "model = AttentionModel(hp)\n",
    "model.dump_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16583215it [00:33, 483301.68it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.evaluate_dataset(sess,\n",
    "                           '../data/simplewiki/simplewiki-20171103.entity_recognition.train.tfrecords',\n",
    "                           header='train %d' % epoch,\n",
    "                           train=True,\n",
    "                           show_progress=True)\n",
    "    model.evaluate_dataset(sess,\n",
    "                           '../data/simplewiki/simplewiki-20171103.entity_recognition.dev.tfrecords',\n",
    "                           header='dev %d' % epoch,\n",
    "                           train=False,\n",
    "                           show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
