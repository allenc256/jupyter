{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_tf():\n",
    "    global sess\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "seq_length = 12\n",
    "batch_size = 1\n",
    "# embedding_size = 64\n",
    "hidden_size = 12\n",
    "vocab_size = 12\n",
    "\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "# TODO: fill in sequence_length as per \n",
    "# http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n",
    "input_lengths = tf.placeholder(tf.int32, [batch_size])\n",
    "target_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(hidden_size)\n",
    "\n",
    "initial_states = rnn_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "embedded_inputs = tf.one_hot(input_data, vocab_size)\n",
    "\n",
    "# embedding = tf.get_variable('embedding', [vocab_size, embedding_size])\n",
    "# embedded_inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, vocab_size])\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "\n",
    "outputs, final_states = tf.nn.dynamic_rnn(rnn_cell,\n",
    "                                          embedded_inputs, \n",
    "                                          initial_state=initial_states, \n",
    "                                          sequence_length=input_lengths)\n",
    "\n",
    "flat_outputs = tf.reshape(outputs, [-1, hidden_size])\n",
    "flat_targets = tf.reshape(target_data, [-1])\n",
    "\n",
    "flat_output_logits = tf.matmul(flat_outputs, softmax_w) + softmax_b\n",
    "flat_output_probs = tf.nn.softmax(flat_output_logits)\n",
    "\n",
    "# flat_loss_mask = tf.sign(tf.to_float(flat_targets))\n",
    "# flat_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=flat_output_logits, labels=flat_targets) * flat_loss_mask\n",
    "flat_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=flat_output_logits, labels=flat_targets)\n",
    "\n",
    "mean_loss = tf.reduce_mean(flat_losses)\n",
    "total_loss = tf.reduce_sum(flat_losses)\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# gvs = optimizer.compute_gradients(mean_loss)\n",
    "# capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "# train_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(mean_loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = \"\"\"the sun did not shine.\n",
    "it was too wet to play.\n",
    "so we sat in the house\n",
    "all that cold, cold, wet day.\n",
    "\n",
    "i sat there with sally.\n",
    "we sat there, we two.\n",
    "and i said, 'how i wish\n",
    "we had something to do!'\n",
    "\n",
    "too wet to go out\n",
    "and too cold to play ball.\n",
    "so we sat in the house.\n",
    "we did nothing at all.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batches(array, batch_size, seq_length):\n",
    "    num_seqs = (len(array) + seq_length - 1) // seq_length\n",
    "    num_seqs_per_batch = (num_seqs + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(num_seqs_per_batch):\n",
    "        seqs = []\n",
    "        seq_lens = []\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            offset = (j*num_seqs_per_batch + i)*seq_length\n",
    "            \n",
    "            seq = array[offset:offset+seq_length]\n",
    "            seq_len = len(seq)\n",
    "            seq = np.pad(seq, (0,seq_length-len(seq)), 'constant', constant_values=0)\n",
    "            \n",
    "            seqs.append(seq)\n",
    "            seq_lens.append(seq_len)\n",
    "            \n",
    "        yield np.stack(seqs), seq_lens\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_input, count):\n",
    "    curr_initial_states  = np.zeros(initial_states.shape)\n",
    "    curr_input_data = np.zeros(input_data.shape)\n",
    "    curr_input_lengths = [1] + [0] * (batch_size - 1)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i in range(count):\n",
    "        ps, curr_initial_states = sess.run((flat_output_probs, final_states), feed_dict = {\n",
    "            input_data: curr_input_data,\n",
    "            input_lengths: curr_input_lengths,\n",
    "            initial_states: curr_initial_states\n",
    "        })\n",
    "        result.append(np.random.choice(len(ps[0]), p = ps[0]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss=1.3683575254699494\n",
      "epoch 10: loss=1.1983166629844426\n",
      "epoch 20: loss=1.2701033632099006\n",
      "epoch 30: loss=1.8854995571362432\n",
      "epoch 40: loss=2.0423659314677276\n",
      "epoch 50: loss=1.9313351169280475\n",
      "epoch 60: loss=1.669676526498296\n",
      "epoch 70: loss=1.2333975004402187\n",
      "epoch 80: loss=1.4025340894373453\n",
      "epoch 90: loss=1.5118542265808956\n"
     ]
    }
   ],
   "source": [
    "train_array = np.array([ord(ch) for ch in train_text])\n",
    "\n",
    "for i in range(100):\n",
    "    epoch_loss = 0.0\n",
    "    curr_initial_states = np.zeros(initial_states.shape)\n",
    "    train_input_batches = generate_batches(train_array[:-1], batch_size, seq_length)\n",
    "    train_target_batches = generate_batches(train_array[1:], batch_size, seq_length)\n",
    "    \n",
    "    for (curr_input_data, curr_input_lens), (curr_target_data, _) in zip(train_input_batches, train_target_batches):\n",
    "        feed_dict = {\n",
    "            input_data: curr_input_data, \n",
    "            input_lengths: curr_input_lens,\n",
    "            target_data: curr_target_data,\n",
    "            initial_states: curr_initial_states }\n",
    "        _, curr_loss, curr_initial_states = sess.run((train_op, total_loss, final_states), feed_dict = feed_dict)\n",
    "        epoch_loss += curr_loss\n",
    "        \n",
    "    epoch_loss /= len(train_array) - 1\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f'epoch {i}: loss={epoch_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.33371767,  0.22937587, -0.04559825, -0.03681822, -0.10696881,\n",
       "          0.06150694,  0.33818346, -0.33722278,  0.27803215, -0.310305  ,\n",
       "         -0.34002498,  0.0376387 ],\n",
       "        [ 0.34987506, -0.5340544 , -0.22623123, -0.08637503, -0.3168208 ,\n",
       "         -0.23286073, -0.47730848, -0.13026679,  0.14726035,  0.15282296,\n",
       "         -0.18769382, -0.41964367],\n",
       "        [-0.02668606,  0.00458021,  0.00452762,  0.02079288, -0.11766426,\n",
       "          0.10440546, -0.51964724,  0.5665893 , -0.32957262, -0.01001556,\n",
       "         -0.60038722,  0.45001325],\n",
       "        [-0.03273053,  0.14313154,  0.26246089,  0.42619997,  0.37390828,\n",
       "          0.15373884, -0.55407733,  0.63674074,  0.34912387,  0.03527905,\n",
       "         -0.07635973, -0.02045194],\n",
       "        [ 0.051635  , -0.02781652, -0.07707702, -0.25481865,  0.55797064,\n",
       "         -0.22971286,  0.24407208, -0.16768903, -0.49062645,  0.34846655,\n",
       "         -0.17940263, -0.01900988],\n",
       "        [ 0.29846254,  0.03369355, -0.09964083,  0.14160274,  0.19294849,\n",
       "         -0.12062408,  0.27207425, -0.38306981,  0.1942566 , -0.0262229 ,\n",
       "          0.463615  ,  0.40868181],\n",
       "        [ 0.14894138,  0.31199893, -0.24430244,  0.13925371, -0.2358436 ,\n",
       "          0.16108951,  0.26825148,  0.3755706 , -0.66281152, -0.21172436,\n",
       "         -0.11421629, -0.05495993],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_initial_states = np.zeros(initial_states.shape)\n",
    "curr_input_data = [list(range(12))]\n",
    "curr_target_data = [list(range(1,13))]\n",
    "curr_input_lengths = [12]\n",
    "\n",
    "feed_dict = {\n",
    "    input_data: curr_input_data, \n",
    "    input_lengths: curr_input_lens,\n",
    "    target_data: curr_target_data,\n",
    "    initial_states: curr_initial_states\n",
    "}\n",
    "\n",
    "# for i in range(500):\n",
    "#     _, curr_loss = sess.run((train_op, total_loss), feed_dict = feed_dict)\n",
    "#     if i % 50:\n",
    "#         print(curr_loss)\n",
    "\n",
    "# print(curr_input_data)\n",
    "# print(curr_target_data)\n",
    "#np.argmax(sess.run(flat_output_probs, feed_dict = feed_dict), axis=1)\n",
    "\n",
    "sess.run(outputs, feed_dict = feed_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
