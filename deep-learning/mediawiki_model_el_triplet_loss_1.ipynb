{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf(sess = None, log_device_placement = False):\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    return tf.InteractiveSession(config = tf.ConfigProto(log_device_placement = log_device_placement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    vocab_size = 30000\n",
    "    \n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    context_size = 81\n",
    "    context_center_index = context_size // 2\n",
    "    \n",
    "    d_embedding_position = 16\n",
    "    d_embedding_word = 128\n",
    "    \n",
    "    d_attention = 128\n",
    "    d_attention_ff = 256\n",
    "    \n",
    "    attention_num_layers = 3\n",
    "\n",
    "    d_attention_embedding_layers = [128]\n",
    "    d_candidate_embedding_layers = [256]\n",
    "    \n",
    "    d_output_embedding = 128\n",
    "    \n",
    "    triplet_loss_margin = 0.5\n",
    "    \n",
    "    pipeline_batch_size = 128\n",
    "    # number of negative samples per positive example\n",
    "    pipeline_num_negative_samples = 5\n",
    "    pipeline_num_parallel_calls = 4\n",
    "    pipeline_prefetch_size = pipeline_batch_size * 16\n",
    "    pipeline_shuffle_size = 5000\n",
    "    \n",
    "    embed_page_tfs_batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityLinkingModel:\n",
    "    def __init__(self, hp):\n",
    "        self._hp = hp\n",
    "\n",
    "    def _parse_example(self, example_proto):\n",
    "        features = {\n",
    "            'page_id': tf.FixedLenFeature([1], tf.int64),\n",
    "            'target_id': tf.FixedLenFeature([1], tf.int64),\n",
    "            'target_word_ids': tf.VarLenFeature(tf.int64),\n",
    "            'target_word_freqs': tf.VarLenFeature(tf.int64),\n",
    "            'word_ids': tf.FixedLenFeature([self._hp.context_size], tf.int64)\n",
    "        }\n",
    "\n",
    "        parsed = tf.parse_single_example(example_proto, features)\n",
    "        \n",
    "        target_word_ids = tf.sparse_tensor_to_dense(parsed['target_word_ids'])\n",
    "        target_word_freqs = tf.sparse_tensor_to_dense(parsed['target_word_freqs'])\n",
    "        target_tf = tf.sparse_to_dense(\n",
    "            target_word_ids,\n",
    "            [self._hp.vocab_size],\n",
    "            target_word_freqs)\n",
    "        \n",
    "        return (\n",
    "            parsed['page_id'],\n",
    "            parsed['word_ids'],\n",
    "            parsed['target_id'],\n",
    "            target_tf)\n",
    "\n",
    "    def _build_data_pipeline(self):\n",
    "        with tf.variable_scope('dataset'):\n",
    "            # placeholder: examples filenames\n",
    "            self._dataset_filenames = tf.placeholder(tf.string, shape = [None])\n",
    "\n",
    "            # build examples dataset\n",
    "            dataset = tf.data.TFRecordDataset(\n",
    "                self._dataset_filenames,\n",
    "                compression_type='GZIP')\n",
    "            dataset = dataset.map(\n",
    "                self._parse_example,\n",
    "                num_parallel_calls = self._hp.pipeline_num_parallel_calls)\n",
    "            dataset = dataset.shuffle(self._hp.pipeline_shuffle_size)\n",
    "            dataset = dataset.prefetch(self._hp.pipeline_prefetch_size)\n",
    "            dataset = dataset.batch(self._hp.pipeline_batch_size)\n",
    "\n",
    "            # build dataset iterator\n",
    "            self._dataset_iterator = dataset.make_initializable_iterator()\n",
    "            (input_page_ids,\n",
    "             input_context_word_ids,\n",
    "             target_page_ids,\n",
    "             target_page_tfs) = self._dataset_iterator.get_next()\n",
    "            \n",
    "            input_page_ids = tf.placeholder_with_default(\n",
    "                input_page_ids,\n",
    "                [None, 1],\n",
    "                name='input_page_ids')\n",
    "            input_context_word_ids = tf.placeholder_with_default(\n",
    "                input_context_word_ids,\n",
    "                [None, self._hp.context_size],\n",
    "                name='input_context_word_ids')\n",
    "            target_page_ids = tf.placeholder_with_default(\n",
    "                target_page_ids,\n",
    "                [None, 1],\n",
    "                name='target_page_ids')\n",
    "            target_page_tfs = tf.placeholder_with_default(\n",
    "                target_page_tfs,\n",
    "                [None, self._hp.vocab_size],\n",
    "                name='target_page_tfs')\n",
    "            \n",
    "            minibatch_size = tf.shape(input_page_ids)[0]\n",
    "            sample_size = self._hp.pipeline_num_negative_samples + 1\n",
    "            expanded_minibatch_size = minibatch_size * sample_size\n",
    "            \n",
    "            x = tf.reshape(tf.range(minibatch_size), [minibatch_size, 1])\n",
    "            y = tf.reshape(tf.range(1, sample_size+1), [1, sample_size])\n",
    "            z = tf.ones([1, sample_size], dtype=tf.int32)\n",
    "            pos_indices = tf.reshape(\n",
    "                x * z,\n",
    "                [expanded_minibatch_size])\n",
    "            neg_indices = tf.reshape(\n",
    "                tf.mod(x + y, minibatch_size),\n",
    "                [expanded_minibatch_size])\n",
    "            \n",
    "            self._anchor_context_word_ids = tf.nn.embedding_lookup(\n",
    "                input_context_word_ids,\n",
    "                pos_indices)\n",
    "            \n",
    "            pos_page_tfs = tf.nn.embedding_lookup(\n",
    "                target_page_tfs,\n",
    "                pos_indices)\n",
    "            neg_page_tfs = tf.nn.embedding_lookup(\n",
    "                target_page_tfs,\n",
    "                neg_indices)\n",
    "#             candidate_page_tfs = tf.cast(candidate_page_tfs, tf.float32)\n",
    "#             page_tf_norms = tf.reduce_sum(candidate_page_tfs, axis=-1, keep_dims=True) + 1e-6\n",
    "#             self._candidate_page_dists = tf.concat(\n",
    "#                 [candidate_page_tfs / page_tf_norms, page_tf_norms],\n",
    "#                 axis=-1)\n",
    "            self._pos_page_dists = tf.cast(pos_page_tfs, tf.float32)\n",
    "            self._neg_page_dists = tf.cast(neg_page_tfs, tf.float32)\n",
    "            \n",
    "            pos_page_ids = tf.nn.embedding_lookup(\n",
    "                target_page_ids,\n",
    "                pos_indices)\n",
    "            neg_page_ids = tf.nn.embedding_lookup(\n",
    "                target_page_ids,\n",
    "                neg_indices)\n",
    "            self._loss_mask = tf.cast(\n",
    "                tf.logical_not(tf.equal(pos_page_ids, neg_page_ids)),\n",
    "                tf.float32)\n",
    "            \n",
    "            # positions\n",
    "            p = tf.range(self._hp.context_size, dtype = tf.int64)\n",
    "            p = tf.tile(p, [expanded_minibatch_size])\n",
    "            p = tf.reshape(p, [expanded_minibatch_size, self._hp.context_size])\n",
    "            self._anchor_context_positions = p\n",
    "            \n",
    "            # placeholder: training flag\n",
    "            self._is_training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            self._minibatch_size = expanded_minibatch_size\n",
    "\n",
    "    def _layer_norm(self, x, num_units, scope=None, reuse=None, epsilon=1e-6):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            return tf.layers.batch_normalization(x, training=self._is_training)\n",
    "#             scale = tf.get_variable(\n",
    "#                 \"layer_norm_scale\", [num_units], initializer=tf.ones_initializer())\n",
    "#             bias = tf.get_variable(\n",
    "#                 \"layer_norm_bias\", [num_units], initializer=tf.zeros_initializer())\n",
    "#             result = self._layer_norm_compute(x, epsilon, scale, bias)\n",
    "#             return result\n",
    "\n",
    "    def _layer_norm_compute(self, x, epsilon, scale, bias):\n",
    "        mean = tf.reduce_mean(x, axis=[-1], keep_dims=True)\n",
    "        variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keep_dims=True)\n",
    "        norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n",
    "        return norm_x * scale + bias\n",
    "    \n",
    "    def _attention_layer(self, A):\n",
    "        A_T = tf.transpose(A, perm=[0, 2, 1])\n",
    "        scaled_logits = tf.matmul(A, A_T) / tf.sqrt(tf.cast(tf.shape(A)[-1], tf.float32))\n",
    "        result = tf.matmul(tf.nn.softmax(scaled_logits), A)\n",
    "        result = tf.layers.dropout(\n",
    "            result, \n",
    "            rate=self._hp.dropout_rate,\n",
    "            training=self._is_training)\n",
    "        return result\n",
    "\n",
    "    def _attention_ff_layer(self, A, scope, reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            A = tf.layers.dense(A, self._hp.d_attention_ff, activation=tf.nn.relu, name='fc1')\n",
    "            A = tf.layers.dense(A, self._hp.d_attention, name='fc2')\n",
    "            A = tf.layers.dropout(\n",
    "                A, \n",
    "                rate=self._hp.dropout_rate, \n",
    "                training=self._is_training)\n",
    "            return A\n",
    "    \n",
    "    def _attention_full_layer(self, A, scope, reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            A = self._layer_norm(\n",
    "                A + self._attention_layer(A), \n",
    "                num_units=self._hp.d_attention,\n",
    "                scope='attention_norm')\n",
    "            A = self._layer_norm(\n",
    "                A + self._attention_ff_layer(A, 'ff', reuse),\n",
    "                num_units=self._hp.d_attention,\n",
    "                scope='attention_ff_norm')\n",
    "            return A\n",
    "\n",
    "    def _candidate_embedding(self, A, scope, reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            layer = A\n",
    "            for i, d_layer in enumerate(self._hp.d_candidate_embedding_layers):\n",
    "                layer = tf.layers.dense(\n",
    "                    layer, \n",
    "                    d_layer,\n",
    "                    activation=tf.nn.relu,\n",
    "                    name='layer_%d' % i,\n",
    "                    reuse=reuse)\n",
    "                layer = tf.layers.batch_normalization(\n",
    "                    layer,\n",
    "                    training=self._is_training,\n",
    "                    name='layer_%d_batch_norm' % i,\n",
    "                    reuse=reuse)\n",
    "                layer = tf.layers.dropout(\n",
    "                    layer,\n",
    "                    rate=self._hp.dropout_rate,\n",
    "                    training=self._is_training)\n",
    "            layer = tf.layers.dense(\n",
    "                layer,\n",
    "                self._hp.d_output_embedding,\n",
    "                name='layer_embedding',\n",
    "                reuse=reuse)\n",
    "            return tf.nn.l2_normalize(layer, dim=-1)\n",
    "            \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('model'):\n",
    "            # embed context words\n",
    "            word_embeddings = tf.get_variable(\n",
    "                'word_embeddings', \n",
    "                [self._hp.vocab_size, self._hp.d_embedding_word])\n",
    "            anchor_context_words_embedded = tf.nn.embedding_lookup(\n",
    "                word_embeddings,\n",
    "                self._anchor_context_word_ids)\n",
    "\n",
    "            # embed context positions\n",
    "            position_embeddings = tf.get_variable(\n",
    "                'position_embeddings',\n",
    "                [self._hp.context_size, self._hp.d_embedding_position],\n",
    "                dtype=tf.float32)\n",
    "            anchor_context_positions_embedded = tf.nn.embedding_lookup(\n",
    "                position_embeddings,\n",
    "                self._anchor_context_positions)\n",
    "\n",
    "            # build full context vector (concat embeddings)\n",
    "            anchor_context_full = tf.concat(\n",
    "                [anchor_context_words_embedded, anchor_context_positions_embedded], \n",
    "                axis=-1)\n",
    "            \n",
    "            # build attention input vector\n",
    "            anchor_attention = tf.layers.dense(\n",
    "                anchor_context_full,\n",
    "                self._hp.d_attention,\n",
    "                activation=tf.nn.relu,\n",
    "                name='anchor_attention')\n",
    "            anchor_attention = self._layer_norm(\n",
    "                anchor_attention,\n",
    "                num_units=self._hp.d_attention,\n",
    "                scope='anchor_attention')\n",
    "            anchor_attention = tf.layers.dropout(\n",
    "                anchor_attention,\n",
    "                rate=self._hp.dropout_rate,\n",
    "                training=self._is_training)\n",
    "            \n",
    "            with tf.variable_scope('anchor'):\n",
    "                # attention layers\n",
    "                layer = anchor_attention\n",
    "                for i in range(self._hp.attention_num_layers):\n",
    "                    layer = self._attention_full_layer(layer, 'layer_%d' % i)\n",
    "                # post-attention embedding\n",
    "                layer = layer[:, self._hp.context_center_index, :]\n",
    "                for i, d_layer in enumerate(self._hp.d_attention_embedding_layers):\n",
    "                    layer = tf.layers.dense(\n",
    "                        layer,\n",
    "                        d_layer,\n",
    "                        activation=tf.nn.relu,\n",
    "                        name='layer_embedding_%d' % i)\n",
    "                    layer = tf.layers.batch_normalization(\n",
    "                        layer,\n",
    "                        training=self._is_training,\n",
    "                        name='layer_embedding_%d_batch_norm' % i)\n",
    "                    layer = tf.layers.dropout(\n",
    "                        layer,\n",
    "                        rate=self._hp.dropout_rate,\n",
    "                        training=self._is_training)\n",
    "                layer = tf.layers.dense(\n",
    "                    layer,\n",
    "                    self._hp.d_output_embedding,\n",
    "                    name='layer_embedding_final')\n",
    "            self._anchor_embedded = tf.nn.l2_normalize(layer, dim=-1)\n",
    "\n",
    "            # build positive/negative layers\n",
    "            self._pos_embedded = self._candidate_embedding(\n",
    "                self._pos_page_dists, \n",
    "                scope='candidate', \n",
    "                reuse=False)\n",
    "            self._neg_embedded = self._candidate_embedding(\n",
    "                self._neg_page_dists, \n",
    "                scope='candidate', \n",
    "                reuse=True)\n",
    "\n",
    "    def _build_training_model(self):\n",
    "        with tf.variable_scope('train'):\n",
    "            pos_dist = tf.norm(\n",
    "                self._anchor_embedded - self._pos_embedded,\n",
    "                axis=-1,\n",
    "                keep_dims=True)\n",
    "            neg_dist = tf.norm(\n",
    "                self._anchor_embedded - self._neg_embedded,\n",
    "                axis=-1,\n",
    "                keep_dims=True)\n",
    "            \n",
    "            losses = tf.maximum(\n",
    "                pos_dist - neg_dist + self._hp.triplet_loss_margin,\n",
    "                0)\n",
    "            losses *= self._loss_mask\n",
    "            \n",
    "            self._total_loss = tf.reduce_sum(losses)\n",
    "            self._mean_loss = tf.reduce_mean(losses)\n",
    "            \n",
    "            c = tf.less(losses, self._hp.triplet_loss_margin)\n",
    "            c = tf.cast(c, tf.int32)\n",
    "            self._num_correct_labels = tf.reduce_sum(c)\n",
    "            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self._global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self._optimizer = tf.train.AdamOptimizer(learning_rate=self._hp.learning_rate)\n",
    "                self._train_op = self._optimizer.minimize(\n",
    "                    self._mean_loss,\n",
    "                    global_step=self._global_step)\n",
    "                \n",
    "    def dump_statistics(self):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            print('parameters for \"%s\": %d' % (variable.name, variable_parameters))\n",
    "            total_parameters += variable_parameters\n",
    "        print('total parameters: %d' % total_parameters)\n",
    "        \n",
    "    def process(self,\n",
    "                sess,\n",
    "                dataset_filename,\n",
    "                options = None,\n",
    "                run_metadata = None,\n",
    "                header = 'results',\n",
    "                train = False,\n",
    "                show_progress = True,\n",
    "                log_file = None):\n",
    "        cum_loss = 0\n",
    "        cum_num_examples = 0\n",
    "        cum_correct_examples = 0\n",
    "        \n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        sess.run(self._dataset_iterator.initializer, feed_dict={\n",
    "            self._dataset_filenames: [dataset_filename]\n",
    "        })\n",
    "\n",
    "        if show_progress:\n",
    "            progress = tqdm_notebook()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                (_,\n",
    "                 curr_total_loss, \n",
    "                 curr_minibatch_size,\n",
    "                 curr_num_correct_labels) = sess.run(\n",
    "                    (self._train_op if train else [],\n",
    "                     self._total_loss,\n",
    "                     self._minibatch_size,\n",
    "                     self._num_correct_labels),\n",
    "                    feed_dict = { self._is_training: train },\n",
    "                    options = options,\n",
    "                    run_metadata = run_metadata)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "            if show_progress:\n",
    "                progress.update(curr_minibatch_size)\n",
    "\n",
    "            cum_loss += curr_total_loss\n",
    "            cum_num_examples += curr_minibatch_size\n",
    "            cum_correct_examples += curr_num_correct_labels\n",
    "\n",
    "        if show_progress:\n",
    "            progress.close()\n",
    "            \n",
    "        finish = datetime.datetime.now()\n",
    "\n",
    "        message = '%s (%d) (%s): loss=%g, accuracy=%g' % (\n",
    "            header,\n",
    "            tf.train.global_step(sess, self._global_step),\n",
    "            finish-start,\n",
    "            cum_loss/cum_num_examples,\n",
    "            cum_correct_examples/cum_num_examples)\n",
    "        print(message)\n",
    "        if log_file:\n",
    "            print(message, file=log_file)\n",
    "            log_file.flush()\n",
    "            \n",
    "    def evaluate(self, sess, dataset_filename, page_tfs, limit):\n",
    "        embedded_page_tfs = self._embed_page_tfs(sess, page_tfs)\n",
    "        batch_size = 1024\n",
    "        \n",
    "        sess.run(self._dataset_iterator.initializer, feed_dict={\n",
    "            self._dataset_filenames: [dataset_filename]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"model/word_embeddings:0\": 3840000\n",
      "parameters for \"model/position_embeddings:0\": 1296\n",
      "parameters for \"model/anchor_attention/kernel:0\": 18432\n",
      "parameters for \"model/anchor_attention/bias:0\": 128\n",
      "parameters for \"model/anchor_attention/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/anchor_attention/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/anchor/layer_0/attention_norm/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/anchor/layer_0/attention_norm/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/anchor/layer_0/ff/fc1/kernel:0\": 32768\n",
      "parameters for \"model/anchor/layer_0/ff/fc1/bias:0\": 256\n",
      "parameters for \"model/anchor/layer_0/ff/fc2/kernel:0\": 32768\n",
      "parameters for \"model/anchor/layer_0/ff/fc2/bias:0\": 128\n",
      "parameters for \"model/anchor/layer_0/attention_ff_norm/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/anchor/layer_0/attention_ff_norm/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/anchor/layer_1/attention_norm/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/anchor/layer_1/attention_norm/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/anchor/layer_1/ff/fc1/kernel:0\": 32768\n",
      "parameters for \"model/anchor/layer_1/ff/fc1/bias:0\": 256\n",
      "parameters for \"model/anchor/layer_1/ff/fc2/kernel:0\": 32768\n",
      "parameters for \"model/anchor/layer_1/ff/fc2/bias:0\": 128\n",
      "parameters for \"model/anchor/layer_1/attention_ff_norm/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/anchor/layer_1/attention_ff_norm/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/anchor/layer_2/attention_norm/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/anchor/layer_2/attention_norm/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/anchor/layer_2/ff/fc1/kernel:0\": 32768\n",
      "parameters for \"model/anchor/layer_2/ff/fc1/bias:0\": 256\n",
      "parameters for \"model/anchor/layer_2/ff/fc2/kernel:0\": 32768\n",
      "parameters for \"model/anchor/layer_2/ff/fc2/bias:0\": 128\n",
      "parameters for \"model/anchor/layer_2/attention_ff_norm/batch_normalization/gamma:0\": 128\n",
      "parameters for \"model/anchor/layer_2/attention_ff_norm/batch_normalization/beta:0\": 128\n",
      "parameters for \"model/anchor/layer_embedding_0/kernel:0\": 16384\n",
      "parameters for \"model/anchor/layer_embedding_0/bias:0\": 128\n",
      "parameters for \"model/anchor/layer_embedding_0_batch_norm/gamma:0\": 128\n",
      "parameters for \"model/anchor/layer_embedding_0_batch_norm/beta:0\": 128\n",
      "parameters for \"model/anchor/layer_embedding_final/kernel:0\": 16384\n",
      "parameters for \"model/anchor/layer_embedding_final/bias:0\": 128\n",
      "parameters for \"model/candidate/layer_0/kernel:0\": 7680000\n",
      "parameters for \"model/candidate/layer_0/bias:0\": 256\n",
      "parameters for \"model/candidate/layer_0_batch_norm/gamma:0\": 256\n",
      "parameters for \"model/candidate/layer_0_batch_norm/beta:0\": 256\n",
      "parameters for \"model/candidate/layer_embedding/kernel:0\": 32768\n",
      "parameters for \"model/candidate/layer_embedding/bias:0\": 128\n",
      "total parameters: 11806352\n"
     ]
    }
   ],
   "source": [
    "sess = reset_tf(sess)\n",
    "\n",
    "model = EntityLinkingModel(HyperParameters())\n",
    "model._build_data_pipeline()\n",
    "model._build_model()\n",
    "model._build_training_model()\n",
    "model.dump_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2201678168e4a339bf84874706e598b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 0 (235) (0:00:54.218661): loss=0.243635, accuracy=0.793928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1335355231424454914d4046137f0e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 0 (235) (0:00:26.258545): loss=0.443067, accuracy=0.596906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da7af001c4d48d3b11517b469b507dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 1 (470) (0:00:53.456649): loss=0.0715242, accuracy=0.955939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1abbaa7a14466e9925757e003734bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 1 (470) (0:00:26.039631): loss=0.288039, accuracy=0.748922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef68567c2fd846468dfaea62a64349a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 2 (705) (0:00:53.574088): loss=0.0346475, accuracy=0.985011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ad2e39515f4a56b6f6b5bd45514fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 2 (705) (0:00:26.152587): loss=0.102347, accuracy=0.923828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a2960ca37d42169f05fa44e5b046e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 3 (940) (0:00:53.176926): loss=0.0233398, accuracy=0.992206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8e77545e9a4b6db2981ba6a295e2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 3 (940) (0:00:26.127982): loss=0.0994088, accuracy=0.925072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fa2d1fd42647dabcc3b076f99b8352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 4 (1175) (0:00:53.986909): loss=0.0185829, accuracy=0.994917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23e10d646194981a4847b3a240436ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 4 (1175) (0:00:25.832809): loss=0.10002, accuracy=0.924194\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.process(\n",
    "        sess,\n",
    "        '../data/simplewiki/simplewiki-20171103.entity_linking.test.tfrecords.gz',\n",
    "        header='test %d' % i,\n",
    "        train = True,\n",
    "        show_progress = True)\n",
    "    model.process(\n",
    "        sess,\n",
    "        '../data/simplewiki/simplewiki-20171103.entity_linking.dev.tfrecords.gz',\n",
    "        header='dev %d' % i,\n",
    "        train = False,\n",
    "        show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/tmp/mediawiki_model_entity_linking_1.log', 'wt', encoding='utf-8') as f:\n",
    "#     for i in range(50):\n",
    "#         model.process(\n",
    "#             sess,\n",
    "#             '../data/simplewiki/simplewiki-20171103.entity_linking.train.tfrecords.gz',\n",
    "#             header='train %d' % i,\n",
    "#             train = True,\n",
    "#             show_progress = True,\n",
    "#             log_file=f)\n",
    "#         model.process(\n",
    "#             sess,\n",
    "#             '../data/simplewiki/simplewiki-20171103.entity_linking.dev.tfrecords.gz',\n",
    "#             header='dev %d' % i,\n",
    "#             train = False,\n",
    "#             show_progress = True,\n",
    "#             log_file=f)\n",
    "#         builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
