{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf(sess = None, log_device_placement = False):\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    return tf.InteractiveSession(config = tf.ConfigProto(log_device_placement = log_device_placement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    vocab_size = 30000\n",
    "    \n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    context_size = 81\n",
    "    context_center_index = context_size // 2\n",
    "    \n",
    "    d_embedding_position = 16\n",
    "    d_embedding_word = 128\n",
    "    \n",
    "    d_attention = 128\n",
    "    d_attention_ff = 256\n",
    "    \n",
    "    attention_num_layers = 3\n",
    "\n",
    "    d_candidate_layers = [256, 128]\n",
    "    d_logistic_layers = [64, 16]\n",
    "    \n",
    "    pipeline_batch_size = 256\n",
    "    # number of negative samples per positive example\n",
    "    pipeline_num_negative_samples = 1\n",
    "    pipeline_num_parallel_calls = 4\n",
    "    pipeline_prefetch_size = pipeline_batch_size * 16\n",
    "    pipeline_shuffle_size = 5000\n",
    "    \n",
    "    embed_page_tfs_batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityLinkingModel:\n",
    "    def __init__(self, session, hp):\n",
    "        self._session = session\n",
    "        self._hp = hp\n",
    "        \n",
    "    def _parse_example(self, example_proto):\n",
    "        features = {\n",
    "            'page_id': tf.FixedLenFeature([1], tf.int64),\n",
    "            'target_id': tf.FixedLenFeature([1], tf.int64),\n",
    "            'target_word_ids': tf.VarLenFeature(tf.int64),\n",
    "            'target_word_freqs': tf.VarLenFeature(tf.int64),\n",
    "            'word_ids': tf.FixedLenFeature([self._hp.context_size], tf.int64)\n",
    "        }\n",
    "\n",
    "        parsed = tf.parse_single_example(example_proto, features)\n",
    "        \n",
    "        target_word_ids = tf.sparse_tensor_to_dense(parsed['target_word_ids'])\n",
    "        target_word_freqs = tf.sparse_tensor_to_dense(parsed['target_word_freqs'])\n",
    "        target_tf = tf.sparse_to_dense(\n",
    "            target_word_ids,\n",
    "            [self._hp.vocab_size],\n",
    "            target_word_freqs)\n",
    "        \n",
    "        return (\n",
    "            parsed['page_id'],\n",
    "            parsed['word_ids'],\n",
    "            parsed['target_id'],\n",
    "            target_tf)\n",
    "    \n",
    "    def _create_negative_samples_inner(self, page_ids, word_ids, target_ids, target_tfs):\n",
    "        in_batch_size = word_ids.shape[0]\n",
    "        out_batch_size = (1 + self._hp.pipeline_num_negative_samples) * in_batch_size\n",
    "\n",
    "        # batch too small to do negative sampling\n",
    "        if in_batch_size < self._hp.pipeline_num_negative_samples:\n",
    "            return (word_ids, target_tfs, [1] * in_batch_size)\n",
    "        # handle getting called by pipeline before the model is ready/initialized\n",
    "        if not hasattr(self, '_context_word_ids'):\n",
    "            return (word_ids, target_tfs, [1] * in_batch_size)\n",
    "        \n",
    "        context_embedded, candidate_embedded, kernels, biases = self._session.run(\n",
    "            (self._context_attention_embedded,\n",
    "             self._candidate_page_dists_embedded,\n",
    "             self._logistic_kernels,\n",
    "             self._logistic_biases),\n",
    "            feed_dict = {\n",
    "                self._context_word_ids: word_ids,\n",
    "                self._candidate_page_dists: target_tfs,\n",
    "                self._training: False })\n",
    "        \n",
    "        context_word_ids = np.zeros(\n",
    "            (out_batch_size, word_ids.shape[1]),\n",
    "            dtype=word_ids.dtype)\n",
    "        candidate_page_dists = np.zeros(\n",
    "            (out_batch_size, target_tfs.shape[1]),\n",
    "            dtype=target_tfs.dtype)\n",
    "        target_labels = np.zeros(\n",
    "            out_batch_size,\n",
    "            dtype=np.int64)\n",
    "        \n",
    "        for i in range(in_batch_size):\n",
    "            # hand-compute logits\n",
    "            a = np.concatenate(\n",
    "                [np.tile(context_embedded[i], [in_batch_size, 1]), candidate_embedded],\n",
    "                axis=-1)\n",
    "            for k, b in zip(kernels, biases):\n",
    "                a = np.matmul(a, k) + b\n",
    "                if b.size > 1:\n",
    "                    a = np.maximum(a, 0)\n",
    "            a *= np.logical_not(np.equal(target_ids[i], target_ids))\n",
    "            a = a.flatten()\n",
    "\n",
    "            # find highest logits as negative samples\n",
    "            neg_samples = np.argpartition(\n",
    "                a,\n",
    "                -self._hp.pipeline_num_negative_samples)[-self._hp.pipeline_num_negative_samples:]\n",
    "            \n",
    "            # set positive sample\n",
    "            base_i = i * (1 + self._hp.pipeline_num_negative_samples)\n",
    "            context_word_ids[base_i] = word_ids[i]\n",
    "            candidate_page_dists[base_i] = target_tfs[i]\n",
    "            target_labels[base_i] = 1\n",
    "            \n",
    "            # set negative samples\n",
    "            for j, k in enumerate(neg_samples):\n",
    "                samp_i = base_i + j + 1\n",
    "                context_word_ids[samp_i] = word_ids[i]\n",
    "                candidate_page_dists[samp_i] = target_tfs[k]\n",
    "                target_labels[samp_i] = 1 if target_ids[i] == target_ids[k] else 0\n",
    "        \n",
    "        return (context_word_ids, candidate_page_dists, target_labels)\n",
    "    \n",
    "    def _create_negative_samples(self, *args):\n",
    "        return tf.tuple(tf.py_func(\n",
    "            self._create_negative_samples_inner,\n",
    "            args,\n",
    "            [tf.int64, tf.int64, tf.int64]))\n",
    "\n",
    "    def _build_data_pipeline(self):\n",
    "        with tf.variable_scope('dataset'):\n",
    "            # placeholder: examples filenames\n",
    "            self._dataset_filenames = tf.placeholder(tf.string, shape = [None])\n",
    "\n",
    "            # build examples dataset\n",
    "            dataset = tf.data.TFRecordDataset(\n",
    "                self._dataset_filenames,\n",
    "                compression_type='GZIP')\n",
    "            dataset = dataset.map(\n",
    "                self._parse_example,\n",
    "                num_parallel_calls = self._hp.pipeline_num_parallel_calls)\n",
    "            dataset = dataset.shuffle(self._hp.pipeline_shuffle_size)\n",
    "            dataset = dataset.prefetch(self._hp.pipeline_prefetch_size)\n",
    "            dataset = dataset.batch(self._hp.pipeline_batch_size)\n",
    "            dataset = dataset.map(\n",
    "                self._create_negative_samples,\n",
    "                # TODO: make this bigger?\n",
    "                num_parallel_calls = 1)\n",
    "\n",
    "            # build dataset iterator\n",
    "            self._dataset_iterator = dataset.make_initializable_iterator()\n",
    "            (context_word_ids,\n",
    "             candidate_page_dists,\n",
    "             target_labels) = self._dataset_iterator.get_next()\n",
    "            \n",
    "            candidate_page_dists = tf.cast(candidate_page_dists, tf.float32)\n",
    "\n",
    "            # placeholders\n",
    "            self._context_word_ids = tf.placeholder_with_default(\n",
    "                context_word_ids,\n",
    "                shape = [None, self._hp.context_size],\n",
    "                name = 'input_context_word_ids')\n",
    "            self._candidate_page_dists = tf.placeholder_with_default(\n",
    "                candidate_page_dists,\n",
    "                shape = [None, self._hp.vocab_size],\n",
    "                name = 'candidate_page_dists')\n",
    "            self._target_labels = tf.placeholder_with_default(\n",
    "                target_labels,\n",
    "                shape = [None],\n",
    "                name = 'target_labels')\n",
    "            \n",
    "            # positions\n",
    "            self._minibatch_size = tf.shape(self._context_word_ids)[0]\n",
    "            p = tf.range(self._hp.context_size, dtype = tf.int64)\n",
    "            p = tf.tile(p, [self._minibatch_size])\n",
    "            p = tf.reshape(p, [self._minibatch_size, self._hp.context_size])\n",
    "            self._context_positions = p\n",
    "    \n",
    "    def _attention_layer(self, A):\n",
    "        A_T = tf.transpose(A, perm=[0, 2, 1])\n",
    "        scaled_logits = tf.matmul(A, A_T) / tf.sqrt(tf.cast(tf.shape(A)[-1], tf.float32))\n",
    "        result = tf.matmul(tf.nn.softmax(scaled_logits), A)\n",
    "        result = tf.layers.dropout(\n",
    "            result, \n",
    "            rate=self._hp.dropout_rate,\n",
    "            training=self._training)\n",
    "        return result\n",
    "\n",
    "    def _attention_ff_layer(self, A, scope, reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            A = tf.layers.dense(A, self._hp.d_attention_ff, activation=tf.nn.relu, name='fc1')\n",
    "            A = tf.layers.dense(A, self._hp.d_attention, name='fc2')\n",
    "            A = tf.layers.dropout(\n",
    "                A, \n",
    "                rate=self._hp.dropout_rate, \n",
    "                training=self._training)\n",
    "            return A\n",
    "    \n",
    "    def _attention_full_layer(self, A, scope, reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            A = tf.layers.batch_normalization(\n",
    "                A + self._attention_layer(A), \n",
    "                training=self._training,\n",
    "                name='attention_batch_norm',\n",
    "                reuse=reuse)\n",
    "            A = tf.layers.batch_normalization(\n",
    "                A + self._attention_ff_layer(A, 'ff', reuse),\n",
    "                training=self._training,\n",
    "                name='attention_ff_batch_norm',\n",
    "                reuse=reuse)\n",
    "            return A\n",
    "            \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('model'):\n",
    "            # placeholder: training flag\n",
    "            self._training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            # embed context words\n",
    "            word_embeddings = tf.get_variable(\n",
    "                'word_embeddings', \n",
    "                [self._hp.vocab_size, self._hp.d_embedding_word])\n",
    "            context_words_embedded = tf.nn.embedding_lookup(\n",
    "                word_embeddings,\n",
    "                self._context_word_ids)\n",
    "\n",
    "            # embed context positions\n",
    "            position_embeddings = tf.get_variable(\n",
    "                'position_embeddings',\n",
    "                [self._hp.context_size, self._hp.d_embedding_position],\n",
    "                dtype=tf.float32)\n",
    "            context_positions_embedded = tf.nn.embedding_lookup(\n",
    "                position_embeddings,\n",
    "                self._context_positions)\n",
    "\n",
    "            # build full context vector (concat embeddings)\n",
    "            context_full = tf.concat(\n",
    "                [context_words_embedded, context_positions_embedded], \n",
    "                axis=-1)\n",
    "            \n",
    "            # build attention input vector\n",
    "            context_attention = tf.layers.dense(\n",
    "                context_full,\n",
    "                self._hp.d_attention,\n",
    "                activation=tf.nn.relu,\n",
    "                name='context_attention')\n",
    "            context_attention = tf.layers.batch_normalization(\n",
    "                context_attention,\n",
    "                training=self._training,\n",
    "                name='context_attention')\n",
    "            context_attention = tf.layers.dropout(\n",
    "                context_attention,\n",
    "                rate=self._hp.dropout_rate,\n",
    "                training=self._training)\n",
    "            \n",
    "            # build attention layers\n",
    "            with tf.variable_scope('attention'):\n",
    "                layer = context_attention\n",
    "                for i in range(self._hp.attention_num_layers):\n",
    "                    layer = self._attention_full_layer(layer, 'layer_%d' % i)\n",
    "            self._context_attention_embedded = layer[:, self._hp.context_center_index, :]\n",
    "\n",
    "            # build candidate layers\n",
    "            with tf.variable_scope('candidate'):\n",
    "                layer = self._candidate_page_dists\n",
    "                for i, d_layer in enumerate(self._hp.d_candidate_layers):\n",
    "                    layer = tf.layers.dense(\n",
    "                        layer, \n",
    "                        d_layer,\n",
    "                        activation=tf.nn.relu,\n",
    "                        name='layer_%d' % i)\n",
    "                    layer = tf.layers.batch_normalization(\n",
    "                        layer,\n",
    "                        training=self._training,\n",
    "                        name='layer_%d' % i)\n",
    "                    layer = tf.layers.dropout(\n",
    "                        layer,\n",
    "                        rate=self._hp.dropout_rate,\n",
    "                        training=self._training)\n",
    "            self._candidate_page_dists_embedded = layer\n",
    "            \n",
    "            # build final logistic neuron\n",
    "            with tf.variable_scope('logistic'):\n",
    "                layer = tf.concat(\n",
    "                    [self._context_attention_embedded, self._candidate_page_dists_embedded],\n",
    "                    axis=-1)\n",
    "                for i, d_layer in enumerate(self._hp.d_logistic_layers):\n",
    "                    layer = tf.layers.dense(\n",
    "                        layer, \n",
    "                        d_layer,\n",
    "                        activation=tf.nn.relu,\n",
    "                        name='layer_%d' % i)\n",
    "                    layer = tf.layers.dropout(\n",
    "                        layer,\n",
    "                        rate=self._hp.dropout_rate,\n",
    "                        training=self._training)\n",
    "                self._output_logits = tf.squeeze(\n",
    "                    tf.layers.dense(layer, 1, name='layer_%d' % len(self._hp.d_logistic_layers)),\n",
    "                    axis=-1)\n",
    "\n",
    "            # grab tensors so we can do negative sampling\n",
    "            self._logistic_kernels = []\n",
    "            self._logistic_biases = []\n",
    "            for i in range(len(self._hp.d_logistic_layers)+1):\n",
    "                self._logistic_kernels.append(tf.get_default_graph().get_tensor_by_name(\n",
    "                    'model/logistic/layer_%d/kernel:0' % i))\n",
    "                self._logistic_biases.append(tf.get_default_graph().get_tensor_by_name(\n",
    "                    'model/logistic/layer_%d/bias:0' % i))\n",
    "\n",
    "    def _build_training_model(self):\n",
    "        with tf.variable_scope('train'):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels = tf.cast(self._target_labels, tf.float32),\n",
    "                logits = self._output_logits)\n",
    "            \n",
    "            self._total_loss = tf.reduce_sum(losses)\n",
    "            self._mean_loss = tf.reduce_mean(losses)\n",
    "            \n",
    "            output_labels = tf.greater(tf.sigmoid(self._output_logits), 0.5)\n",
    "            output_labels = tf.cast(output_labels, tf.int64)\n",
    "            self._num_correct_labels = tf.reduce_sum(tf.cast(\n",
    "                tf.equal(output_labels, self._target_labels), \n",
    "                tf.int32))\n",
    "            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self._global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self._optimizer = tf.train.AdamOptimizer(learning_rate=self._hp.learning_rate)\n",
    "                self._train_op = self._optimizer.minimize(\n",
    "                    self._mean_loss,\n",
    "                    global_step=self._global_step)\n",
    "    \n",
    "    def build_model(self):\n",
    "        self._build_data_pipeline()\n",
    "        self._build_model()\n",
    "        self._build_training_model()\n",
    "\n",
    "    def dump_statistics(self):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            print('parameters for \"%s\": %d' % (variable.name, variable_parameters))\n",
    "            total_parameters += variable_parameters\n",
    "        print('total parameters: %d' % total_parameters)\n",
    "\n",
    "    def process(self,\n",
    "                dataset_filename,\n",
    "                options = None,\n",
    "                run_metadata = None,\n",
    "                header = 'results',\n",
    "                train = False,\n",
    "                show_progress = True,\n",
    "                log_file = None):\n",
    "        cum_loss = 0\n",
    "        cum_num_examples = 0\n",
    "        cum_correct_examples = 0\n",
    "        \n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        self._session.run(self._dataset_iterator.initializer, feed_dict={\n",
    "            self._dataset_filenames: [dataset_filename]\n",
    "        })\n",
    "\n",
    "        if show_progress:\n",
    "            progress = tqdm_notebook()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                (_,\n",
    "                 curr_total_loss, \n",
    "                 curr_minibatch_size,\n",
    "                 curr_num_correct_labels,\n",
    "                 curr_logistic_kernels,\n",
    "                 curr_logistic_biases) = self._session.run(\n",
    "                    (self._train_op if train else (),\n",
    "                     self._total_loss,\n",
    "                     self._minibatch_size,\n",
    "                     self._num_correct_labels,\n",
    "                     self._logistic_kernels if train else (),\n",
    "                     self._logistic_biases if train else ()),\n",
    "                    feed_dict = { self._training: train },\n",
    "                    options = options,\n",
    "                    run_metadata = run_metadata)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "            if show_progress:\n",
    "                progress.update(curr_minibatch_size)\n",
    "                \n",
    "            if curr_logistic_kernels:\n",
    "                self._logistic_kernels_checkpoint = curr_logistic_kernels\n",
    "                self._logistic_biases_checkpoint = curr_logistic_biases\n",
    "\n",
    "            cum_loss += curr_total_loss\n",
    "            cum_num_examples += curr_minibatch_size\n",
    "            cum_correct_examples += curr_num_correct_labels\n",
    "\n",
    "        if show_progress:\n",
    "            progress.close()\n",
    "            \n",
    "        finish = datetime.datetime.now()\n",
    "        elapsed = (finish - start).total_seconds() * 1000.0\n",
    "\n",
    "        message = '%s (%d) (%g ms): loss=%g, accuracy=%g' % (\n",
    "            header,\n",
    "            tf.train.global_step(sess, self._global_step),\n",
    "            elapsed,\n",
    "            cum_loss/cum_num_examples,\n",
    "            cum_correct_examples/cum_num_examples)\n",
    "        print(message)\n",
    "        if log_file:\n",
    "            print(message, file=log_file)\n",
    "            log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for \"model/word_embeddings:0\": 3840000\n",
      "parameters for \"model/position_embeddings:0\": 1296\n",
      "parameters for \"model/context_attention/kernel:0\": 18432\n",
      "parameters for \"model/context_attention/bias:0\": 128\n",
      "parameters for \"model/context_attention/gamma:0\": 128\n",
      "parameters for \"model/context_attention/beta:0\": 128\n",
      "parameters for \"model/attention/layer_0/attention_batch_norm/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_0/attention_batch_norm/beta:0\": 128\n",
      "parameters for \"model/attention/layer_0/ff/fc1/kernel:0\": 32768\n",
      "parameters for \"model/attention/layer_0/ff/fc1/bias:0\": 256\n",
      "parameters for \"model/attention/layer_0/ff/fc2/kernel:0\": 32768\n",
      "parameters for \"model/attention/layer_0/ff/fc2/bias:0\": 128\n",
      "parameters for \"model/attention/layer_0/attention_ff_batch_norm/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_0/attention_ff_batch_norm/beta:0\": 128\n",
      "parameters for \"model/attention/layer_1/attention_batch_norm/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_1/attention_batch_norm/beta:0\": 128\n",
      "parameters for \"model/attention/layer_1/ff/fc1/kernel:0\": 32768\n",
      "parameters for \"model/attention/layer_1/ff/fc1/bias:0\": 256\n",
      "parameters for \"model/attention/layer_1/ff/fc2/kernel:0\": 32768\n",
      "parameters for \"model/attention/layer_1/ff/fc2/bias:0\": 128\n",
      "parameters for \"model/attention/layer_1/attention_ff_batch_norm/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_1/attention_ff_batch_norm/beta:0\": 128\n",
      "parameters for \"model/attention/layer_2/attention_batch_norm/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_2/attention_batch_norm/beta:0\": 128\n",
      "parameters for \"model/attention/layer_2/ff/fc1/kernel:0\": 32768\n",
      "parameters for \"model/attention/layer_2/ff/fc1/bias:0\": 256\n",
      "parameters for \"model/attention/layer_2/ff/fc2/kernel:0\": 32768\n",
      "parameters for \"model/attention/layer_2/ff/fc2/bias:0\": 128\n",
      "parameters for \"model/attention/layer_2/attention_ff_batch_norm/gamma:0\": 128\n",
      "parameters for \"model/attention/layer_2/attention_ff_batch_norm/beta:0\": 128\n",
      "parameters for \"model/candidate/layer_0/kernel:0\": 7680000\n",
      "parameters for \"model/candidate/layer_0/bias:0\": 256\n",
      "parameters for \"model/candidate/layer_0/gamma:0\": 256\n",
      "parameters for \"model/candidate/layer_0/beta:0\": 256\n",
      "parameters for \"model/candidate/layer_1/kernel:0\": 32768\n",
      "parameters for \"model/candidate/layer_1/bias:0\": 128\n",
      "parameters for \"model/candidate/layer_1/gamma:0\": 128\n",
      "parameters for \"model/candidate/layer_1/beta:0\": 128\n",
      "parameters for \"model/logistic/layer_0/kernel:0\": 16384\n",
      "parameters for \"model/logistic/layer_0/bias:0\": 64\n",
      "parameters for \"model/logistic/layer_1/kernel:0\": 1024\n",
      "parameters for \"model/logistic/layer_1/bias:0\": 16\n",
      "parameters for \"model/logistic/layer_2/kernel:0\": 16\n",
      "parameters for \"model/logistic/layer_2/bias:0\": 1\n",
      "total parameters: 11790833\n"
     ]
    }
   ],
   "source": [
    "sess = reset_tf(sess)\n",
    "\n",
    "model = EntityLinkingModel(sess, HyperParameters())\n",
    "model.build_model()\n",
    "model.dump_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b09c96543b946cbabc2cf66859a2963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 0 (118) (36097.6 ms): loss=0.278505, accuracy=0.896433\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481621a51f21475ba6efda0c2fbabb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 0 (118) (27150.5 ms): loss=2.02677, accuracy=0.49995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd708d98f7bf4980b54ae46be62e3ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 1 (236) (34920.1 ms): loss=0.242297, accuracy=0.896633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef1a36f161e4fa09be23706acc28822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 1 (236) (27390.2 ms): loss=2.27029, accuracy=0.48575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e08f3b48064ee18130fcf9918a9056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 2 (354) (35134.4 ms): loss=0.160987, accuracy=0.950233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7feeb3dfa44994967bb1fb2dd3b68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 2 (354) (28071.9 ms): loss=3.06801, accuracy=0.486783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45f84abecea48c182ceb10ab7831c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 3 (472) (36136.1 ms): loss=0.0985317, accuracy=0.973117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc97f4e0cd1741109aae3e53dcc984e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 3 (472) (29130.3 ms): loss=3.51425, accuracy=0.492517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb45ed87d774a3f8fb9d6b216b31b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev 4 (590) (37093.6 ms): loss=0.0809694, accuracy=0.98155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85394b6768cb4524a15067c9cabd8bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test 4 (590) (29644.2 ms): loss=3.48509, accuracy=0.488467\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.process(\n",
    "        '../data/simplewiki/simplewiki-20171103.entity_linking.dev.tfrecords.gz', \n",
    "        train=True,\n",
    "        header='dev %d' % i)\n",
    "    model.process(\n",
    "        '../data/simplewiki/simplewiki-20171103.entity_linking.test.tfrecords.gz', \n",
    "        train=False,\n",
    "        header='test %d' % i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
